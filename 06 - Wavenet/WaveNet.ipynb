{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4a104d-170b-491c-8fc5-dcced08774d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# üèÑ‚Äç‚ôÇÔ∏è You will feel surfing!\n",
    "Here, we will pickup where we left the things off in the last notebook.\n",
    "\n",
    "**There:**\n",
    "- We started out with the original modulized code\n",
    "- Then **cleaned** up the code futher by introducing the `Embedding`, `Flatten` and finally the `Sequential` classes, that made our code more \"pytorchized\".\n",
    "- Now, we will use them in here, to **build** the wavenet. \n",
    "\n",
    "But before that, we will need to **understand** the structure of the wavenet, to finally be able to surf over it! üèÑ‚Äç‚ôÇÔ∏è <br>\n",
    "Excited enough? Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982319d2-5a6f-4bca-8546-ed27f36a767d",
   "metadata": {},
   "source": [
    "# üñº Visualizing the difference\n",
    "I know, it may be *(very)* confusing from the paper... and also from the Andrej's lecture and you probably thinking... **okay... but how does it look like? What is going on inside?**.\n",
    "\n",
    "<img src=\"./images/inside.gif\" wdith=200px height=200px>\n",
    "\n",
    "I have tried to brainstorm over it, and let's have a look at this thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09905b1-b1ea-40ad-89fc-d55885919a67",
   "metadata": {},
   "source": [
    "# üõ∑ Till now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c87ba-33a9-4f40-8db7-4050644142cc",
   "metadata": {},
   "source": [
    "<img src=\"./images/simple-net.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf85798-e009-4d4a-a757-3a5f37788f8c",
   "metadata": {},
   "source": [
    "# üß† And, this will be the story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b27dcb-ff31-4513-b807-b867e0786368",
   "metadata": {},
   "source": [
    "<img src=\"./images/wavenet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e68f0-fb6a-4729-bdd5-3544b8a45380",
   "metadata": {},
   "source": [
    "# Woo...\n",
    "Yeah, just a dimension added and a bunch of axis concatenation, except that everything is just the same.\n",
    "\n",
    "- We will be calculating the stuff in the same way, just the matrix multiplication\n",
    "- That means, **instead of combining embeddings** of **all** characters at once in the old method, **now** we will **combine the embeddings of only even-odd** charaters. Which will be always `embedding_size * 2` in all cases.\n",
    "- Passing them seperate will automatically enable the network to learn the \"wavenet\" relationship.\n",
    "- Of course, you can give it some more time to come up with some \"philosophy\" but this is how the architecture looks like in a nutshell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa664d9b-c130-4438-bd07-a2c0dc3eac8c",
   "metadata": {},
   "source": [
    "# üë®‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626bb6c-8928-4e81-b11d-36cd1970c460",
   "metadata": {},
   "source": [
    "> **NOTE**: The code is kind of a *boilerplate*, so it is just copied from the previous notebook with updated `embedding` and related clasees üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2107a64-2f29-4f59-9746-4f6a47abde0c",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Loading & creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f33c5c5-7c69-4a16-a765-c6092e3a5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# loading the dataset\n",
    "with open(\"./names.txt\", \"r\") as file:\n",
    "    names = file.read().splitlines()\n",
    "\n",
    "# total unique characters\n",
    "characters = sorted(list(set(''.join(names))))\n",
    "\n",
    "# Builind index-to-char and char-to-index\n",
    "number_to_chr = {k:v for k, v in enumerate([\".\"] + characters)}\n",
    "chr_to_number = {v:k for k, v in enumerate([\".\"] + characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6948e3a-a9d9-4733-87a6-3e2bebb633a2",
   "metadata": {},
   "source": [
    "üëâ Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f8ed3d-909f-4d27-94f6-68de0f1fa082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will build the dataset and return the X, Y\n",
    "# Used when we have multiple splits :)\n",
    "block_size = 8\n",
    "def build_dataset(shuffled_names):\n",
    "    sot = chr_to_number[\".\"]\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for name in shuffled_names: #FOR ALL NAMES\n",
    "        window_chars = [sot] * block_size\n",
    "        name = name + \".\"\n",
    "\n",
    "        for ch in name:\n",
    "            _3chars = ''.join(\n",
    "                list(\n",
    "                    map(lambda x:number_to_chr[x], window_chars)\n",
    "                )) \n",
    "            ch_index = chr_to_number[ch]\n",
    "\n",
    "            X.append(window_chars)\n",
    "            y.append(ch_index)\n",
    "            window_chars = window_chars[1:] + [ch_index]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36424e5d-8401-46b2-a708-4e32e85b73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Xtrain.shape = torch.Size([182625, 8])\n",
      "* Xval.shape = torch.Size([22655, 8])\n",
      "* Xtest.shape = torch.Size([22866, 8])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names) # In-place shuffling. No longer first word will be \"emma\"\n",
    "\n",
    "\n",
    "train_idx = int(0.8 * len(names)) # 80%\n",
    "val_idx = int(0.9 * len(names)) # 90% - 80% = 10%\n",
    "\n",
    "Xtrain, ytrain = build_dataset(names[:train_idx])\n",
    "Xval, yval = build_dataset(names[train_idx:val_idx])\n",
    "Xtest, ytest = build_dataset(names[val_idx:])\n",
    "\n",
    "print(f\"* {Xtrain.shape = }\\n* {Xval.shape = }\\n* {Xtest.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd83392-7aac-432f-b48b-3d35fdf23819",
   "metadata": {},
   "source": [
    "üëâ What is in the training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5a426c-b3ad-4b72-b0c4-149787f736f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ ‚Üí y\n",
      ".......y ‚Üí u\n",
      "......yu ‚Üí h\n",
      ".....yuh ‚Üí e\n",
      "....yuhe ‚Üí n\n",
      "...yuhen ‚Üí g\n",
      "..yuheng ‚Üí .\n",
      "\n",
      "........ ‚Üí d\n",
      ".......d ‚Üí i\n",
      "......di ‚Üí o\n",
      ".....dio ‚Üí n\n",
      "....dion ‚Üí d\n",
      "...diond ‚Üí r\n",
      "..diondr ‚Üí e\n",
      ".diondre ‚Üí .\n",
      "\n",
      "........ ‚Üí x\n",
      ".......x ‚Üí a\n",
      "......xa ‚Üí v\n",
      ".....xav ‚Üí i\n",
      "....xavi ‚Üí e\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(Xtrain[:20], ytrain[:20]):\n",
    "    print(''.join(number_to_chr[ix.item()] for ix in x), \"‚Üí\", number_to_chr[y.item()])\n",
    "    if y.item() == 0: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c5d88-5485-41b5-a632-bbf6f40d705c",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ We \"pytorchified\" the code, from the last book.\n",
    "*Including the `Flatten`, `Embedding` and `Sequential`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a998b-3980-4c26-acfd-1ecb5b8cb435",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a `Linear` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a8c06c-f837-45ad-9ab5-d37d58b54075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    This will be used to create a Linear Layer of `n_ins` and `n_outs`\n",
    "    and also performs the matrix multiplication\n",
    "    \n",
    "    - Possible to enable/disable the bias\n",
    "    - Automatically set the weights and initialize them with Kaiming\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_ins, n_outs, bias=True):\n",
    "        self.weight = torch.randn(n_ins, n_outs) / n_ins**0.5\n",
    "        self.bias = torch.zeros(n_outs) if bias else None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9acf9-21ec-49c3-8d89-83fd60414736",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a `BatchNorm` class \n",
    "*This is the buggy implementation, with the mean being calculated on the `0` th axis, we will later fix this once we perform the basic test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20bf0926-2d02-466b-9aca-2de548449648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    \"\"\"\n",
    "    This will implement the whole batchnorm stuff that can later be added \n",
    "    with the linear layer.\n",
    "    \n",
    "    - Perform normalization\n",
    "    - Keep track of the statistics of the batch \"while training\" and \"while evaluation\".\n",
    "    - Distinction between training and evaluation/inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        \"\"\"\n",
    "        `eps`: Adds a small number in the denomenator while standardizing to\n",
    "            avoid division by zero error\n",
    "            \n",
    "        `momentum`: Used in the calculation of the statistics while training\n",
    "            to set the effect of how much of the std and mean to keep from the\n",
    "            current batch. High momentum means learn more and visaversa.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True # Will be explained later in a bit below.\n",
    "        \n",
    "        ### For scaling & shifting\n",
    "        # Sacler will be called `gamma`\n",
    "        # Shifter will be called `beta`\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "        ### Keep track of running mean and variance for the inference!\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):   \n",
    "        ### If `training` then calculate the mean and var \n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdims=True)\n",
    "            xvar = x.var(0, keepdims=True)\n",
    "        ### If `not training` then use the running mean and var\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "            \n",
    "        ### Normalize!\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        ### Calculate the running mean and variance\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afb974-84cf-4d9b-b6d2-d0b7ea9eaac6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a `Tanh` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93626ca7-a220-4c0a-a9e8-a2a68c4aef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Just to calculate the `tanh`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530f68c-203e-4c70-9a24-a25266338fcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a `Embedding` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e5dea9d-a77d-4ad5-9bde-891028582425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    \"\"\"\n",
    "    1. It will initialize the weights\n",
    "    2. It is able to call the weights based on their index\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        self.weights = torch.randn(vocab_size, n_embd)\n",
    "        \n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weights[IX]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca14a7-3b59-48bc-a8d4-6954abee195c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a `Flatten` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767dc52-3d1c-4f98-a4fc-45fa4da4dd8b",
   "metadata": {},
   "source": [
    "> As Andrej demonstrated [in this clip](https://youtube.com/clip/UgkxwVvcaO-5voBsSDQhVH0qvo0wqJeUjEEc) how the `view` operation gives exactly the same result as the \"explicit\" even odd concatenation that we demonstrated just above, thus we will use that üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60984128-2d29-4733-8e55-a9036a380390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \"\"\"\n",
    "    This will be the concatenator, BUT with the updated Wavenet\n",
    "    based settings.\n",
    "\n",
    "    So instead of flattening things out, of all embeddings together,\n",
    "    here we will do the `.view()` operation.\n",
    "\n",
    "    ### NOTE ###\n",
    "    Here I am taking the constant `2` which is the only number\n",
    "    to let us continue with the EVEN / ODD example explained above.\n",
    "\n",
    "    What Andrej has done is taking `n` as an initial input which would\n",
    "    replace the constant `2`. But we will continue our WaveNet example\n",
    "    with this constant `2` without making things complicated.\n",
    "\n",
    "    For now, just understand that `2` is the number which will get the \n",
    "    math correct in the `.view()` operation and result in the perfect\n",
    "    concatenation operation.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        n_samples, block_size, emb_size = x.shape\n",
    "        self.out = x.view(n_samples, block_size//2, emb_size*2)\n",
    "        if self.out.shape[1] == 1:\n",
    "            self.out = torch.squeeze(self.out, dim=1)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23a71c-b3b9-45c7-a530-e708614a3c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a `Sequential` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6820c154-42f4-4096-a947-bbecf3ec38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"\n",
    "    We will simply replace the explicit LIST keeping\n",
    "    and FOR LOOPING for the forward pass, in this\n",
    "    single class.\n",
    "    \n",
    "    This is very very neat thing to be done.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        parameters = []\n",
    "        for layer in self.layers:\n",
    "            for p in layer.parameters():\n",
    "                parameters.append(p)\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32791b10-a1c3-4f0c-beb1-adb530d9b585",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß† Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76d6517-a2bb-4043-a60f-2e7eacaf69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5005a51e-37b4-41f3-8f0c-dbd184cd65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_neurons = 200\n",
    "vocab_size = len(number_to_chr) # 27\n",
    "\n",
    "layers = [\n",
    "    Embedding(vocab_size, n_embd), \n",
    "    Flatten(), Linear(n_embd * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Flatten(), Linear(n_neurons * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Flatten(), Linear(n_neurons * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Linear(n_neurons, vocab_size), \n",
    "]\n",
    "\n",
    "model = Sequential(layers) ### WE WILL CALL `MODEL` ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36fe687a-6bd0-433f-bc54-5216dd2f77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171497\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bdc486f-1eb4-4f50-8f9e-5d44f836ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  10000: 3.3014\n"
     ]
    }
   ],
   "source": [
    "epochs = 10_000 # we will break it don't worry\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample_idx = torch.randint(0, Xtrain.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtrain[sample_idx], ytrain[sample_idx]\n",
    "    \n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb) \n",
    "    \n",
    "    # 2Ô∏è‚É£ Backward\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # 3Ô∏è‚É£ Update - with decay\n",
    "    learning_rate = 0.1 if i < 10_000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # for better visualization \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0d7fa9-e46e-46fe-9c6c-62352fc47ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : torch.Size([32, 8, 10])\n",
      "Flatten : torch.Size([32, 4, 20])\n",
      "Linear : torch.Size([32, 4, 200])\n",
      "BatchNorm1d : torch.Size([32, 4, 200])\n",
      "Tanh : torch.Size([32, 4, 200])\n",
      "\n",
      "Flatten : torch.Size([32, 2, 400])\n",
      "Linear : torch.Size([32, 2, 200])\n",
      "BatchNorm1d : torch.Size([32, 2, 200])\n",
      "Tanh : torch.Size([32, 2, 200])\n",
      "\n",
      "Flatten : torch.Size([32, 400])\n",
      "Linear : torch.Size([32, 200])\n",
      "BatchNorm1d : torch.Size([32, 200])\n",
      "Tanh : torch.Size([32, 200])\n",
      "\n",
      "Linear : torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "x = Xb\n",
    "for idx, layer in enumerate(model.layers):\n",
    "    x = layer(x)\n",
    "    layer_name = layer.__class__.__name__\n",
    "    print(layer_name, \":\",layer.out.shape, end=\"\\n\" if layer_name != \"Tanh\" else \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e120b9-34c9-464b-8152-babb8ed9705d",
   "metadata": {},
   "source": [
    "## ü§î Which is currently...\n",
    "\n",
    "<img src=\"./images/wavenet-wiz.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a21c5-d8ea-4c21-92a1-784eb9ce4573",
   "metadata": {},
   "source": [
    "Everything looks nice! üéá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4951e23-853a-44ef-9d60-b3daf13c43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_neurons = 48 ### instead of 68, as Andrej used, I will use 48 to match my previous model=12297\n",
    "vocab_size = len(number_to_chr)\n",
    "\n",
    "layers = [\n",
    "    Embedding(vocab_size, n_embd), \n",
    "    Flatten(), Linear(n_embd * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Flatten(), Linear(n_neurons * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Flatten(), Linear(n_neurons * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Linear(n_neurons, vocab_size), \n",
    "]\n",
    "\n",
    "model = Sequential(layers) ### WE WILL CALL `MODEL` ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517d0784-2cb3-4998-bff1-2611b2b5f24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12201\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7a8844-4042-46af-8b03-af3bbfead7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 100000: 3.3048\n",
      "  10000/ 100000: 2.1995\n",
      "  20000/ 100000: 2.0995\n",
      "  30000/ 100000: 2.0591\n",
      "  40000/ 100000: 2.5509\n",
      "  50000/ 100000: 2.1282\n",
      "  60000/ 100000: 1.6855\n",
      "  70000/ 100000: 2.5461\n",
      "  80000/ 100000: 2.3684\n",
      "  90000/ 100000: 2.7769\n"
     ]
    }
   ],
   "source": [
    "epochs = 1_00_000\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample_idx = torch.randint(0, Xtrain.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtrain[sample_idx], ytrain[sample_idx]\n",
    "    \n",
    "    # 1Ô∏è‚É£ Forward\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb) \n",
    "    \n",
    "    # 2Ô∏è‚É£ Backward\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # 3Ô∏è‚É£ Update - with decay\n",
    "    learning_rate = 0.1 if i < 10_000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # for better visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6b88ad-219c-4d19-9cdb-613b5115ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # NEW - Will disable the gradient tracking temproarily - for performance sake\n",
    "def split_loss(split: str):\n",
    "    x, y = {\n",
    "        'train': (Xtrain, ytrain),\n",
    "        'test': (Xtest, ytest),\n",
    "        'val': (Xval, yval)\n",
    "    }[split]\n",
    "    \n",
    "    logits = model(x)\n",
    "    final_loss = F.cross_entropy(logits, y)\n",
    "    print(split.title(), \":\\t\", round(final_loss.item(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53af5e46-0934-412e-9f02-430ef9206fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74b74b65-a9bb-4f64-9659-0a5a2612c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :\t 2.0576\n",
      "Val :\t 2.09363\n",
      "Test :\t 2.08772\n"
     ]
    }
   ],
   "source": [
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa2728-c30c-4b35-bbcf-09129e5acfff",
   "metadata": {},
   "source": [
    "‚åõ Before, in **Simple ANN**: Test Loss = `2.15053` <br>\n",
    "‚åö Now, in **WaveNet *(with BatchNorm Bug)***: Test Loss = `2.0865`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081a09a-eb0e-4665-8a3b-91bf0855a826",
   "metadata": {},
   "source": [
    "## üêû That BatchNorm Bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a26d4b-e193-4e41-b03d-834ed6043a07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a *bug free* `BatchNorm` class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff693f0-4f8d-4660-a927-a38d85207837",
   "metadata": {},
   "source": [
    "<img src=\"./images/batch_norm_mean_bug.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57f42c-0be3-49fd-b308-97e168514db8",
   "metadata": {},
   "source": [
    "### So, the updated `BatchNorm` code is written below üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f02701e-8bbe-407f-a341-103b05be3765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    \"\"\"\n",
    "    The updated BatchNorm class, where the running mean and variance will \n",
    "    take care of the 3D input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        \n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):   \n",
    "        ## UPDATED CODE ##\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            if x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(dim, keepdims=True)\n",
    "            xvar = x.var(dim, keepdims=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "            \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6990d-0c90-4dd9-8e92-11cce9bb4a07",
   "metadata": {},
   "source": [
    "üëâ Creating a model *(again)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e89246ff-1145-41b2-b5e7-327fcdde52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_neurons = 48 ### instead of 68, as Andrej used, I will use 48 to match my previous model=12297\n",
    "vocab_size = len(number_to_chr)\n",
    "\n",
    "layers = [\n",
    "    Embedding(vocab_size, n_embd), \n",
    "    Flatten(), Linear(n_embd * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Flatten(), Linear(n_neurons * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Flatten(), Linear(n_neurons * 2, n_neurons), BatchNorm1d(n_neurons), Tanh(),\n",
    "    Linear(n_neurons, vocab_size), \n",
    "]\n",
    "\n",
    "model = Sequential(layers) ### WE WILL CALL `MODEL` ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39a5b3ae-c397-49aa-bd52-d446872ad00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12201\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29110270-c4b3-4fbf-bea7-3ad55b1e94d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 100000: 3.3010\n",
      "  10000/ 100000: 2.2331\n",
      "  20000/ 100000: 2.2508\n",
      "  30000/ 100000: 2.0198\n",
      "  40000/ 100000: 1.9547\n",
      "  50000/ 100000: 1.7769\n",
      "  60000/ 100000: 2.0611\n",
      "  70000/ 100000: 1.7319\n",
      "  80000/ 100000: 2.4099\n",
      "  90000/ 100000: 2.1044\n"
     ]
    }
   ],
   "source": [
    "epochs = 1_00_000\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample_idx = torch.randint(0, Xtrain.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtrain[sample_idx], ytrain[sample_idx]\n",
    "\n",
    "    # 1Ô∏è‚É£ Forward\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb) \n",
    "    \n",
    "    # 2Ô∏è‚É£ Backward\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # 3Ô∏è‚É£ Update - with decay\n",
    "    learning_rate = 0.1 if i < 10_000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # for better visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94782d05-505c-425f-aefc-6c9840f9e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e48f858-4cc6-4e6e-8c00-0d9de3b905dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :\t 2.05237\n",
      "Val :\t 2.09428\n",
      "Test :\t 2.08826\n"
     ]
    }
   ],
   "source": [
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5a458-3b00-4c11-a72f-f0d67005ac36",
   "metadata": {},
   "source": [
    "‚åõ Before's Before, in **Simple ANN**: Test Loss = `2.15053` <br>\n",
    "‚è≤ Before, in **WaveNet *(with BatchNorm Bug)***: Test Loss = `2.0865` <br>\n",
    "‚åö Now, in **WaveNet *(with-out BatchNorm Bug)***: Test Loss = `2.08826`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdb52d-1057-4fb2-adc8-b503e5cc41e4",
   "metadata": {},
   "source": [
    "### Inference!! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4665d6e0-332e-4421-b2ad-5d895bfa3132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thama.\n",
      "nezianna.\n",
      "darrett.\n",
      "vihahorree.\n",
      "brocllyn.\n",
      "ejanie.\n",
      "yaden.\n",
      "yabeolade.\n",
      "kalys.\n",
      "yannett.\n",
      "azeana.\n",
      "jahariof.\n",
      "tynna.\n",
      "alygin.\n",
      "caadian.\n",
      "aiv.\n",
      "sheri.\n",
      "joha.\n",
      "myael.\n",
      "kiadsle.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    \n",
    "    while True:\n",
    "        x = torch.tensor([context])\n",
    "        logits = model(x)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        \n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "    print(''.join(number_to_chr[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcb122-6023-4515-b3e6-89dda30a0909",
   "metadata": {},
   "source": [
    "# üéÅ Bonus \n",
    "Let's try, **if the model can complete your name** or not üòÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3504659-4c36-4f3a-a39d-4becfbcce3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 25, 21]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"aayu\" # Please half name\n",
    "initials = list(map(lambda c: chr_to_number[c], name))\n",
    "initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ee271611-40f6-4798-919e-c5de626ab4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 25, 21]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appending \"dots\" to fulfull the block size\n",
    "context = [0] * (block_size - len(initials)) + initials\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6e3bd985-77d5-4ce6-9174-e31226d700e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aayua.\n",
      "aayue.\n",
      "aayus.\n",
      "aayun.\n",
      "aayufei.\n",
      "aayu.\n",
      "aayu.\n",
      "aayu.\n",
      "aayuawsen.\n",
      "aayua.\n"
     ]
    }
   ],
   "source": [
    "for trial in range(10):\n",
    "    out = initials.copy()\n",
    "    while True:\n",
    "        x = torch.tensor([context])\n",
    "        logits = model(x)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        \n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "    print(''.join(number_to_chr[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ec9ba-824b-4043-8c86-6a0010317333",
   "metadata": {},
   "source": [
    "> Oops, it didn't complete my name for a single time üòÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6878f3c-47ae-4aa6-b47d-fff4573cf172",
   "metadata": {},
   "source": [
    "## üòí But,\n",
    "There seems to be some confusion, the model:\n",
    "1. Seems to allow \"minimum\" number of layers, which is dependent on the `block_size`.\n",
    "    - Which means, in this case, we have the **blocksize=8** for which we have to implement the **minimum** number of hidden layers `3`\n",
    "    - We can't lower the number of hidden layers otherwise the **second dimension** won't end up being `1`.\n",
    "2. Following to the **first** point... if we have block size, say `12`, or even `3`, it won't work.\n",
    "    - Because the WaveNet seems to be created only when you have the block size `2`, `4`, `8`, `16`, `32` and so on.\n",
    "    - If you choose other block size, the numbers won't match up and will give the error.\n",
    "  \n",
    "> But anyways... we have learnt something new here ü§ò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf4108-7957-4e85-a3ae-8421af2796d0",
   "metadata": {},
   "source": [
    "# With that said,\n",
    "Let's meet in the master lecture next, where we will build the GPT üòà"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
