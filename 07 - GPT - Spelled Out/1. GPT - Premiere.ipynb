{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6f073a-5ead-49c9-806c-3e075854326e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üîÑ GPT\n",
    "> *Yo, listen up, folks! It's like GPT, man, it's the hot talk of the town right now, no joke! Everyone and their grandma is all gaga about this crazy thing. It's like this super brainy AI wizardry that can talk, write, and do all sorts of mind-bending stuff with words. I mean, it's like having a genius ghost in your computer, cooking up sentences that'll make Shakespeare blush. People are yapping about it at coffee shops, on social media, even at those fancy-schmancy tech conferences. It's like the ultimate word-wrangling rockstar, and ain't nobody keeping quiet about it, b**ch!*\n",
    ">\n",
    "> \\- ChatGPT *(jesse's style)*\n",
    "\n",
    "Yo!!! üî• It has fired up things pretty well!\n",
    "\n",
    "___\n",
    "\n",
    "Let's get back to the point, and collect ourselves from all things, here in this very notebook...\n",
    "- We will start building the GPT *(yes!)*\n",
    "- We will take a small data - tiny shakespere and make the model to generate more of it.\n",
    "- Explore what is the **transformer** architecture and so on...\n",
    "\n",
    "This is going to be a **real** fire üî•üî•üî• <br>\n",
    "Let's do it! *(without talking too much)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4d000-2398-47ca-b7ca-3892a49dfc80",
   "metadata": {},
   "source": [
    "### ‚¨áÔ∏è Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbc9e99-fd3d-4476-a108-cd9c4b899c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f0ef27-e0bc-4500-812c-02b0259c29c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading the file\n",
    "with open('./input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b91220e-c8b5-4ffb-bdba-96500daa5c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c61ce9-f9da-46eb-9320-5d51ec05895b",
   "metadata": {},
   "source": [
    "‚Äî‚Äî Alright ‚Äî‚Äî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45996c2c-d5d3-422a-bdbe-5ecf78d57d0f",
   "metadata": {},
   "source": [
    "### üîç Just a little inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59817562-b60f-4538-b34a-3173418aabc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: 65\n",
      "\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Total unique characters:\", vocab_size, end=\"\\n\\n\")\n",
    "print(chars)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a36a5d-e8ad-4b7e-92e2-9814fb85c2b1",
   "metadata": {},
   "source": [
    "Alright, we just have a **single number** `3` in whole data... what's going on shakespere!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2863c5-eaaf-4b98-8d41-1baa410ab636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not a big deal.\n",
    "text.count(\"3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f233fe-21e6-4336-b9c9-d0b7ec4e4a11",
   "metadata": {},
   "source": [
    "### ü§π‚Äç‚ôÄÔ∏è Playing around with the encoder/decoder = tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad96a91-94b8-46e2-9dbb-9f0b2f6c667f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 46, 47, 57, 1, 47, 57, 1, 19, 28, 32, 8]\n",
      "This is GPT.\n"
     ]
    }
   ],
   "source": [
    "char_to_number = {ch:i for i,ch in enumerate(chars)}\n",
    "number_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# encoder: take a string, output a list of integers\n",
    "encode = lambda s: [char_to_number[c] for c in s]\n",
    "\n",
    "# decoder: take a list of integers, output a string\n",
    "decode = lambda l: ''.join([number_to_char[i] for i in l]) \n",
    "\n",
    "encoded_tokens = encode(\"This is GPT.\")\n",
    "print(encoded_tokens)\n",
    "\n",
    "decoded_tokens = decode(encoded_tokens)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2e6f1-53c6-4d2c-a799-9b101f6c8f3f",
   "metadata": {},
   "source": [
    "> **NOTE**: This is going to be the \"character level\" language model, we won't be making the \"word or sub-word\" level model here. But still it will give the pretty good results ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8e48a-948c-4b77-9aa1-34cae059a519",
   "metadata": {},
   "source": [
    "# üìÇ Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587458ca-b18c-42c5-acf9-593fe7ba6a54",
   "metadata": {},
   "source": [
    "We will keep:\n",
    "1. `90%` data for training\n",
    "2. `10%` for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928cb60b-6737-46d0-afbe-b584d67b64fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fb4d3a-3125-43c6-b435-f9ccd731cf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# datatype long will allow the larger numbers to be stored and avoid the \n",
    "# numeric overflow in the normal integer (int32)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e76de00-601b-40e6-bcee-e1b99ca46854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13dc28-cd4a-46d5-991c-f981e117f0c7",
   "metadata": {},
   "source": [
    "Which looks something like... üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef481f03-1e14-40b5-9be8-33695c08cd12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[:100].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12322ea3-f33f-4262-ac7f-f32db91be825",
   "metadata": {},
   "source": [
    "Nothing fancy, we will just be feeding in the **individual characters** in and will have the `y` as the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b9e58-4b06-4909-b207-b42eea9279f8",
   "metadata": {},
   "source": [
    "### üìè The context length `2048`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ccd33-e175-455c-9724-a7aa76e1a8de",
   "metadata": {},
   "source": [
    "So the chatGPT has some max length, till which at max it can accept the input and then can generate the output which is `4,096 tokens` in ChatGPT case.\n",
    "\n",
    "#### ü•ú Context window in the nutshell\n",
    "\n",
    "ü§î **What does that mean actually**?\n",
    "- Say, you are a such a person who can give prediction the next year of your friend's life **by looking at their** last `10` years at max.\n",
    "- So, you take last `10` years of history and estimate how their next year will be.\n",
    "- You keep going for say till `100` years.\n",
    "- You only have **the context window** of 10 years because you can't analyze more than that at once.\n",
    "- So, when you predict their next year, you take ***that predicted year*** and forget the very first year; keeping the context window of `10`.\n",
    "\n",
    "But, this is not it! <br>\n",
    "People **may not have 10 years of data** so you should also be able to predict their next year even if they give just say `6` years of data and predict the 7th!\n",
    "\n",
    "> ü™∂ *That's where we will train the network which will predict the next token based on past tokens till the maximum context length is reached.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f997d7ec-b68e-455f-aa34-70cbc1d5268c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) = First Cit\n"
     ]
    }
   ],
   "source": [
    "context_window = 8 #for now\n",
    "sample_data = train_data[:context_window+1]\n",
    "print(sample_data, decode(sample_data.tolist()), sep=\" = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "759d25cc-3247-47e7-a216-75a4941f62dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [18] the target: 47\n",
      "When input is [18, 47] the target: 56\n",
      "When input is [18, 47, 56] the target: 57\n",
      "When input is [18, 47, 56, 57] the target: 58\n",
      "When input is [18, 47, 56, 57, 58] the target: 1\n",
      "When input is [18, 47, 56, 57, 58, 1] the target: 15\n",
      "When input is [18, 47, 56, 57, 58, 1, 15] the target: 47\n",
      "When input is [18, 47, 56, 57, 58, 1, 15, 47] the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_window] # except the `+1`th character\n",
    "y = train_data[1:context_window+1] # except the very first `0`th character\n",
    "for t in range(context_window):\n",
    "    context = x[:t+1].tolist()\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97e0ca-6a71-4da2-b5cf-ba297ffa5668",
   "metadata": {},
   "source": [
    "Which also is üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d832d09f-c844-4489-9fb8-638088affe42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F ‚Üí i\n",
      "Fi ‚Üí r\n",
      "Fir ‚Üí s\n",
      "Firs ‚Üí t\n",
      "First ‚Üí  \n",
      "First  ‚Üí C\n",
      "First C ‚Üí i\n",
      "First Ci ‚Üí t\n"
     ]
    }
   ],
   "source": [
    "for t in range(context_window):\n",
    "    context = x[:t+1].tolist()\n",
    "    target = y[t].tolist()\n",
    "    print(f\"{decode(context)} ‚Üí {decode([target])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c9365-914e-4c0d-8f1d-3ee29cc00931",
   "metadata": {},
   "source": [
    "> üóí <br><br>**NOTE**: This is like our `makemore` version, but the difference is... in makemore, we had a **fixed** input size. All names were `8` long. If not, we used to *pad* the `.` character to make them `8` long... here, instead we append the characters gradually so that the **model could learn the combinations** ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d691c43-a27e-40f1-a4bd-a0a02bdcaff7",
   "metadata": {},
   "source": [
    "‚è™ **Before** *(makemore)*:\n",
    "\n",
    "    ... ‚Üí d\n",
    "    ..d ‚Üí i\n",
    "    .di ‚Üí o\n",
    "    dio ‚Üí n\n",
    "    ion ‚Üí d\n",
    "    ond ‚Üí r\n",
    "    ndr ‚Üí e\n",
    "    dre ‚Üí .\n",
    "\n",
    "‚è© **Now** *(GPT)*:\n",
    "\n",
    "    d ‚Üí i\n",
    "    di ‚Üí o\n",
    "    dio ‚Üí n\n",
    "    dion ‚Üí d\n",
    "    diond ‚Üí r\n",
    "    diondr ‚Üí e\n",
    "    diondre ‚Üí ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f7a54-a904-4339-87ba-4b834482a8c6",
   "metadata": {},
   "source": [
    "> üí¨ <br>*We are doing this because we want the transformer \"to be used to\" seeing the characters as low as a single character and as long as `context_window`.* <br><br> ‚Äî Andrej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b78eaa63-9296-473f-a56c-ec0e1b96623f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1003854])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e663b-adea-4b7b-b2eb-b519d0c12c8a",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è Dataset with multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d493c71-8199-4d91-9b91-82f6a6627303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 4      #batch_size\n",
    "context_window = 8 #block_size\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_window, (n_samples,))\n",
    "    x = torch.stack([data[i:i+context_window] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_window+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c268cc1-a89b-412f-8536-821e778cfcff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "üì§ Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('üì• Inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('\\nüì§ Targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07455b8c-9141-4242-9e34-ad163597fd4e",
   "metadata": {},
   "source": [
    "I hope it is clear what is going on here... \n",
    "- Just selects any integer between `0` and `1115386` *(lengthData - 8 | to avoid indexerror)*\n",
    "- And then from ***those*** `4` random indices, it will create the single sample/batch of `8` numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7326b3a-eb7b-45a0-9013-78facf68dddb",
   "metadata": {},
   "source": [
    "ü§® Wanna see in action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1524b278-a592-42d1-b269-d8e6d2f4df2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ\n",
      "\n",
      "[24] ‚Üí 43 \n",
      "L ‚Üí e\n",
      "\n",
      "[24, 43] ‚Üí 58 \n",
      "Le ‚Üí t\n",
      "\n",
      "[24, 43, 58] ‚Üí 5 \n",
      "Let ‚Üí '\n",
      "\n",
      "[24, 43, 58, 5] ‚Üí 57 \n",
      "Let' ‚Üí s\n",
      "\n",
      "[24, 43, 58, 5, 57] ‚Üí 1 \n",
      "Let's ‚Üí  \n",
      "\n",
      "[24, 43, 58, 5, 57, 1] ‚Üí 46 \n",
      "Let's  ‚Üí h\n",
      "\n",
      "[24, 43, 58, 5, 57, 1, 46] ‚Üí 43 \n",
      "Let's h ‚Üí e\n",
      "\n",
      "[24, 43, 58, 5, 57, 1, 46, 43] ‚Üí 39 \n",
      "Let's he ‚Üí a\n",
      "\n",
      "üîµ\n",
      "\n",
      "[44] ‚Üí 53 \n",
      "f ‚Üí o\n",
      "\n",
      "[44, 53] ‚Üí 56 \n",
      "fo ‚Üí r\n",
      "\n",
      "[44, 53, 56] ‚Üí 1 \n",
      "for ‚Üí  \n",
      "\n",
      "[44, 53, 56, 1] ‚Üí 58 \n",
      "for  ‚Üí t\n",
      "\n",
      "[44, 53, 56, 1, 58] ‚Üí 46 \n",
      "for t ‚Üí h\n",
      "\n",
      "[44, 53, 56, 1, 58, 46] ‚Üí 39 \n",
      "for th ‚Üí a\n",
      "\n",
      "[44, 53, 56, 1, 58, 46, 39] ‚Üí 58 \n",
      "for tha ‚Üí t\n",
      "\n",
      "[44, 53, 56, 1, 58, 46, 39, 58] ‚Üí 1 \n",
      "for that ‚Üí  \n",
      "\n",
      "üîµ\n",
      "\n",
      "[52] ‚Üí 58 \n",
      "n ‚Üí t\n",
      "\n",
      "[52, 58] ‚Üí 1 \n",
      "nt ‚Üí  \n",
      "\n",
      "[52, 58, 1] ‚Üí 58 \n",
      "nt  ‚Üí t\n",
      "\n",
      "[52, 58, 1, 58] ‚Üí 46 \n",
      "nt t ‚Üí h\n",
      "\n",
      "[52, 58, 1, 58, 46] ‚Üí 39 \n",
      "nt th ‚Üí a\n",
      "\n",
      "[52, 58, 1, 58, 46, 39] ‚Üí 58 \n",
      "nt tha ‚Üí t\n",
      "\n",
      "[52, 58, 1, 58, 46, 39, 58] ‚Üí 1 \n",
      "nt that ‚Üí  \n",
      "\n",
      "[52, 58, 1, 58, 46, 39, 58, 1] ‚Üí 46 \n",
      "nt that  ‚Üí h\n",
      "\n",
      "üîµ\n",
      "\n",
      "[25] ‚Üí 17 \n",
      "M ‚Üí E\n",
      "\n",
      "[25, 17] ‚Üí 27 \n",
      "ME ‚Üí O\n",
      "\n",
      "[25, 17, 27] ‚Üí 10 \n",
      "MEO ‚Üí :\n",
      "\n",
      "[25, 17, 27, 10] ‚Üí 0 \n",
      "MEO: ‚Üí \n",
      "\n",
      "\n",
      "[25, 17, 27, 10, 0] ‚Üí 21 \n",
      "MEO:\n",
      " ‚Üí I\n",
      "\n",
      "[25, 17, 27, 10, 0, 21] ‚Üí 1 \n",
      "MEO:\n",
      "I ‚Üí  \n",
      "\n",
      "[25, 17, 27, 10, 0, 21, 1] ‚Üí 54 \n",
      "MEO:\n",
      "I  ‚Üí p\n",
      "\n",
      "[25, 17, 27, 10, 0, 21, 1, 54] ‚Üí 39 \n",
      "MEO:\n",
      "I p ‚Üí a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in range(n_samples): # batch dimension\n",
    "    print(\"üîµ\\n\")\n",
    "    for t in range(context_window): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"{context.tolist()} ‚Üí {target} \\n{decode(context.tolist())} ‚Üí {decode([target.tolist()])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87aa10-41e2-47bb-b06f-cebc760aaeb8",
   "metadata": {},
   "source": [
    "More clearly... üíñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ed72ab5-b192-4884-a171-2f6e94adc550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Inputs:\n",
      "[Let's he]\n",
      "[for that]\n",
      "[nt that ]\n",
      "[MEO:\\nI p]\n",
      "\n",
      "üì§ Targets:\n",
      "[et's hea]\n",
      "[or that ]\n",
      "[t that h]\n",
      "[EO:\\nI pa]\n"
     ]
    }
   ],
   "source": [
    "print('üì• Inputs:')\n",
    "for decoded_xb in map(lambda t: decode(t.tolist()), xb):\n",
    "    print(\"[\", decoded_xb.replace(\"\\n\",\"\\\\n\"), \"]\", sep='')\n",
    "    \n",
    "print('\\nüì§ Targets:')\n",
    "for decoded_yb in map(lambda t: decode(t.tolist()), yb):\n",
    "    print(\"[\", decoded_yb.replace(\"\\n\",\"\\\\n\"), \"]\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d575bd-9954-4423-be05-50a54e4df29b",
   "metadata": {},
   "source": [
    "Yeah, so this is how it goes üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31f7da-8991-4bd6-bf25-80a0ffec5a57",
   "metadata": {},
   "source": [
    "Which means... \n",
    "- If a single sample consists `8` characters then there are `8` different combinations for the models to learn from that single sample.\n",
    "- Then *if* we have `4` samples, we will have total `32` training, forward passes to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f987d4-ebce-46da-bb26-0aea48143f9b",
   "metadata": {},
   "source": [
    "# üê´ Let's create the Bigram LM\n",
    "Oh, some DejaVu? Let's recap quickly:\n",
    "- The model only takes care of the **previous** token\n",
    "- Based on that, it will select the **row** *(in the manual case)* and will pick out the next most probable character.\n",
    "\n",
    "That's it! Let's see how that can work here.\n",
    "\n",
    "> **NOTE**: Since I am running this model on the cloud, I am able to use the GPU. So, the following code may use the \"device\" to leverage the faster training üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a7e844-4714-49c3-a909-3f9ec0d2d572",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x268edd86bd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn # for layers and stuff\n",
    "from torch.nn import functional as F # for the loss function and softmax\n",
    "torch.manual_seed(1337) # same as in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0039bb58-cf85-4f25-b635-cdb25c66694c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module): # because, we will use its functions like forward.\n",
    "    \"\"\"\n",
    "    Just the starter skeleton of the Class.\n",
    "    In the very next cell we will implement full BigramLM.\n",
    "    \n",
    "    With Explanation üòâ\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The simple lookup table...\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Just take the index of the \"current\" character \n",
    "        and then use that to get the probability dist \n",
    "        for the next one using the `embedding_table`.\n",
    "        '''\n",
    "        \n",
    "        # Note we are NOT indexing `[]` but CALLING `()` the table\n",
    "        # because that is the Layer (kind of)\n",
    "        logits = self.embedding_table(idx)\n",
    "        \n",
    "        return logits # Form of (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5287a310-ecb2-4c7a-a88a-b26167e9f2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BigramLM(vocab_size)\n",
    "logits = model(xb, yb)\n",
    "logits.shape # B, T, C form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc27c02-c41b-4b5d-9661-78a5a557fbbf",
   "metadata": {},
   "source": [
    "- `4`: Number of samples\n",
    "- `8`: Context window *(each tokens)*\n",
    "- `65`: The vocab size *(distributions of next token for each tokens in context window)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e8ea9-1071-4b59-828b-610b8e9effbc",
   "metadata": {},
   "source": [
    "### üí≠ Think through this...\n",
    "Here for each sample... for each `8` characters we are pulling out the next character's distribution.\n",
    "\n",
    "**Which means**, till now **NO CONNECTION** between the tokens have been made. We have just pulled out the next tokens distribution... and that's solely based on the current token and nothing linking the previous tokens. \n",
    "\n",
    "*That's what Andrej said in [this clip](https://youtube.com/clip/Ugkxcwwtx2tSFQEwgPHmXgylUpKFSXHBM_gn).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fde30-2d71-4784-9043-986c4a7a612f",
   "metadata": {},
   "source": [
    "## üèó Building Up the Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e6f418e-fe07-4568-abdd-9d10f868f639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    This class takes the `vocab_size` as a single input because its being\n",
    "    the \"simplest\" model, we won't need anything else.\n",
    "    \n",
    "    It will create `vocab_size` * `vocab_size` table and then we will be\n",
    "    able to access it.\n",
    "    \n",
    "    The Forward function will take the `x` input and based on the shape\n",
    "    it will perform the forward pass on it. The nuances are explained in \n",
    "    the following markdown cells.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The simple lookup table...\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Just take the index of the \"current\" character \n",
    "        and then use that to get the probability dist \n",
    "        for the next one using the `embedding_table`.\n",
    "        '''\n",
    "\n",
    "        logits = self.embedding_table(idx)\n",
    "        \n",
    "        if targets is None: # means we are inferencing and not training\n",
    "            loss=None\n",
    "        else:               # means we are training\n",
    "            # Refer: Change [1]\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Refer: Change [1]\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # For the given logits and *correct* targets, pick the pre-\n",
    "            # dicted logits for the given target to calculate the \n",
    "            # negative log likelihood loss.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Take the index of the token and guess the next\n",
    "        token based on the embeddings!\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # call the `forward` method \n",
    "            logits, loss = self(idx) # we can do this because we have inherited `nn.Module` :)\n",
    "            \n",
    "            # Take the very last token (8th in the context window)\n",
    "            # and use its distribution to get the next token\n",
    "            logits = logits[:, -1, :] ## refer: Change [2]\n",
    "\n",
    "            # Convert the logits into the probabilities\n",
    "            probs = F.softmax(logits, dim=-1) ## dim=-1: along the last dimension ~ here `1`\n",
    "\n",
    "            # Take the next idx!\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Here we will ALWAYS append, there is NO shrinking!\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eab79a-d387-4508-973e-a9f1e7dc71bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üìñ Change Explainer\n",
    "\n",
    "### `Change [1]`\n",
    "\n",
    "The current `logits` shape is `B, T, C` = `4, 8, 65`. \n",
    "\n",
    "<img src=\"./images/BTC.png\">\n",
    "\n",
    "### `Change [2]`\n",
    "\n",
    "<img src=\"./images/last_token.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77624a-78f0-45d0-8c04-5761d235d7fb",
   "metadata": {},
   "source": [
    "A simple generation üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4c2c72f-7255-4594-b923-e29b57fda481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hbH\n",
      "\n",
      ":CLP.A!fq'3ggt!O!T?X!!SA?W&TrpvYybSE3w&S BXUhmiKYyTmWMPhhmnHKj!!btgnwNNULuEzRuYyiWEQxPX!$3C'MBj\n"
     ]
    }
   ],
   "source": [
    "model = BigramLM(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), dtype=torch.long), \n",
    "        max_new_tokens=100)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e3ac0-bdc0-45a9-ab1c-2e57c2cb3cc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "> That's just an **un-trained** random model üòÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e39499-7584-44b4-82f9-9e97211456fe",
   "metadata": {},
   "source": [
    "# üìà Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7e68e-d29f-4f2e-8ac2-7199ea03dc40",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39725295-ab0d-42a3-918f-411e43f06abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45011d-cec0-4bf3-93df-aeb5408a1a3a",
   "metadata": {},
   "source": [
    "### Set the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58327232-ba0a-4551-af6c-ebe4930af936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84213137-5fef-448f-8a13-62a6bfc5c1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "# The parameters...\n",
    "param = model.parameters()\n",
    "for p in param:\n",
    "    print(len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811597d6-753f-4da2-880c-38190d3dbf5c",
   "metadata": {},
   "source": [
    "Since we only have a simple, single layer... it returns the parameters of that layer, which is `65` in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5801222-7a35-4345-aef0-3e5d7e0e39f6",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fec3e8ac-98b0-4b2b-b6f5-e41f81eec74a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.420738697052002\n"
     ]
    }
   ],
   "source": [
    "n_samples = 32 ### BATCH SIZE!\n",
    "\n",
    "for steps in range(20_000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    ### Aha!!\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    ### Update the weights' value.\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a42311-4d01-4e42-84da-0cc55ee5bc87",
   "metadata": {},
   "source": [
    "Trained generation üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75767163-21ed-4c61-904c-136a6b3e1356",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLTre!\n",
      "\n",
      "\n",
      "APrs whes ge d f.\n",
      "\n",
      "WI beconcitnol ng!\n",
      "Ass'stheave\n",
      "\n",
      "CEEThrofthant thengord te I whamett pis by cer nathopyeareenop:\n",
      "Ly m tome piand.\n",
      "\n",
      "CAr ICENGRTant:\n",
      "Thon ator:\n",
      "Hepat br ble tid nonengelle the ne e stos,\n",
      "THAnd boune And s ooreth sulf ane hy y woford'shoulellon, medeifents,\n",
      "t mutose ca?\n",
      "\n",
      "INGes th my bl t ghe hyot IAn fe w, as oro, blit acin hodof d, nace:\n",
      "Tase sayor f wenheris gr wl ho f 'T:\n",
      "INGous: bofo ges se CK:\n",
      "Thomarst sbieathind y ar,\n",
      "CINGl wero t cothisefolllomy 'sthavewhot t mere\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), dtype=torch.long), \n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d301c-9421-4b68-a567-29321e2da109",
   "metadata": {},
   "source": [
    "Yo! üî• <br>\n",
    "It is just looking at the **previous single token!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893086b-f371-47c9-b69b-dc8bd382c25c",
   "metadata": {},
   "source": [
    "# The code with CUDA üôÖ\n",
    "Let's start stepping into the real world. Let's use the **GPU**!\n",
    "\n",
    "> üòÉ \n",
    ">\n",
    ">*The following code is the **re-written** version of the code above just to demonstrate <br> and introduce the new code as Andrej shows in the [clip](https://youtube.com/clip/Ugkx7YMJZGAh8ybgDkRS4rDpIvV5I4xTDK9d).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831b7d0-f9bf-4c8d-9b96-6cf90cd9254a",
   "metadata": {},
   "source": [
    "üóí **A little note:** From our previous exercises, we are already been familiar with what happens inside. We have used the variable names in *our language* such as calling `batch_size` as `n_samples`. So, from now, I will use the same variable names as are used in the industry so that we can get grip on those industry jargons üòâ\n",
    "\n",
    "> ‚û° In many places I have written *`### üóΩ Transfer to device üóΩ ###`* which means there is the small change in code to transfer the weights on the available device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92382d-a727-4053-acd0-61c00c19098b",
   "metadata": {},
   "source": [
    "### `1.` Setting the hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b889f8ca-3b0b-49d6-820f-5d27919271c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32       # n_samples\n",
    "block_size = 8        # context_window\n",
    "max_iters = 20_000    # total steps\n",
    "eval_interval = 1_000 # interval at we will print the loss\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfd1c1-93b7-422a-bd16-38a538c4234e",
   "metadata": {},
   "source": [
    "For me, the device now is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dea2d2e-4139-442c-b986-1e680a445ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46b189-cb71-471c-a8fc-44deb049101d",
   "metadata": {},
   "source": [
    "### `2.` Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "194847e0-6abd-4f23-9645-756c2e08cf26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM(vocab_size)\n",
    "\n",
    "### üóΩ Transfer to device üóΩ ###\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f8242-32bb-403f-85e1-4dccc71d454b",
   "metadata": {},
   "source": [
    "### `3.` To Get Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90f3f7b9-75c0-4732-8553-7e7ececd3f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    ### üóΩ Transfer to device üóΩ ###\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfd8f5-26ab-41cd-8aa4-dd2e15b1efd1",
   "metadata": {},
   "source": [
    "### `4.` Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "111d3781-cf5e-4563-a13f-4552b50ae887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb37c4c-5df2-4cf6-9a8e-b44293be3765",
   "metadata": {},
   "source": [
    "### `5.` Estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58d083a8-7b7a-4fb2-89e2-78b905efc835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    '''\n",
    "    This function takes the random samples from the dataset (based on the batch size)\n",
    "    for `eval_iter` times. Records loss and takes the mean loss. And reports back.\n",
    "    \n",
    "    Which means, if we have the `eval_iter = 10` and `batch_size=32` then it will take \n",
    "    32 random samples from training data and then validation data for 10 times and takes\n",
    "    the means of these 10 losses.\n",
    "    '''\n",
    "    out = {}\n",
    "    \n",
    "    # üî• sets on evaluation mode... üî•\n",
    "    # which does something like `training_mode = False` \n",
    "    # in the layers like `BatchNorm`.\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    # üî• sets the model to training mode back!!! üî•\n",
    "    # which does something like `training_mode = True` \n",
    "    # in the layers like `BatchNorm`.\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851bbb8-f723-4a4c-ae06-a1742066fe40",
   "metadata": {},
   "source": [
    "### `6.` Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74d327-4e73-4d91-8dbc-c570ff8797fc",
   "metadata": {},
   "source": [
    "This loop is amazing. Let's break the steps down:\n",
    "- Train for `20,000` loops\n",
    "- Print when you hit every `1,000`th step\n",
    "- When you are asked to print the loss, then go and calculate the loss for a random batch of `32` for `eval_iter` times. \n",
    "- Take the mean of those and then print.\n",
    "\n",
    "> ‚û° Doing this will let us see **how the model has learnt** the relationship in the subset of data instead of printing the loss as is for that *last* batch. This way is better üòÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea1b1356-d5a4-4da4-a866-896b3364554f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~4.5146, Val Loss~4.5203\n",
      "[Step 1000]: Train Loss~3.5914, Val Loss~3.5906\n",
      "[Step 2000]: Train Loss~3.0389, Val Loss~3.0614\n",
      "[Step 3000]: Train Loss~2.7728, Val Loss~2.7734\n",
      "[Step 4000]: Train Loss~2.6314, Val Loss~2.6322\n",
      "[Step 5000]: Train Loss~2.5467, Val Loss~2.5608\n",
      "[Step 6000]: Train Loss~2.5171, Val Loss~2.5345\n",
      "[Step 7000]: Train Loss~2.5034, Val Loss~2.5080\n",
      "[Step 8000]: Train Loss~2.4858, Val Loss~2.4993\n",
      "[Step 9000]: Train Loss~2.4760, Val Loss~2.4813\n",
      "[Step 10000]: Train Loss~2.4606, Val Loss~2.4827\n",
      "[Step 11000]: Train Loss~2.4572, Val Loss~2.4773\n",
      "[Step 12000]: Train Loss~2.4620, Val Loss~2.4832\n",
      "[Step 13000]: Train Loss~2.4539, Val Loss~2.4792\n",
      "[Step 14000]: Train Loss~2.4525, Val Loss~2.4763\n",
      "[Step 15000]: Train Loss~2.4566, Val Loss~2.4839\n",
      "[Step 16000]: Train Loss~2.4529, Val Loss~2.4854\n",
      "[Step 17000]: Train Loss~2.4528, Val Loss~2.4847\n",
      "[Step 18000]: Train Loss~2.4479, Val Loss~2.4776\n",
      "[Step 19000]: Train Loss~2.4528, Val Loss~2.4900\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512fa4d-5f16-4902-bed2-7c80e22064a9",
   "metadata": {},
   "source": [
    "### `7.` Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79d06cf5-68e9-4d0e-984d-ef3831a2b13d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CExthy brid owindakis by bth\n",
      "\n",
      "HAPet bobe d e.\n",
      "S:\n",
      "O:\n",
      "IS:\n",
      "Falatanss:\n",
      "Wanthar u qur, vet?\n",
      "F dilasoate awice my.\n",
      "\n",
      "Hastarom oroup\n",
      "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghirileranousel lind me l.\n",
      "HAshe ce hiry:\n",
      "Supr aisspllw y.\n",
      "Hentofu noroopetelaves\n",
      "MP:\n",
      "\n",
      "Pl, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar iris! m:\n",
      "\n",
      "Thiny aleronth,\n",
      "Mad\n",
      "RD:\n",
      "\n",
      "WISo myr f-bube!\n",
      "KENoby ak\n",
      "Sadsal thes ghesthidin cour ay aney Iry ts I fr t ce.\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),  ### üóΩ Transfer to device üóΩ ###\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a809958-4fda-4e5c-a930-a4285564b29b",
   "metadata": {},
   "source": [
    "# üîçüìö Taking the context into an account\n",
    "\n",
    "- The GPT model is **\"autoregressive\"**. Which means, it sees the \"previous\" context and then generates the next token. \n",
    "- That **also** means, the model **doesn't** know the future tokens to generate *this* token, because they are to be generated and aren't yet generated *(in the contrast of text-to-text models or masked learning done with the BERT models where we train the model to see the past and future tokens to estimate the `<MASK>` token)*.\n",
    "\n",
    "<img src=\"./images/autoregerssive.png\">\n",
    "\n",
    "... and to do that we will use the **mean** for now. As a starter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05912796-8d6e-47cd-870f-df5f6f29aff2",
   "metadata": {},
   "source": [
    "## üòÖ The for-loop way\n",
    "Suppose we have the `context-window=8` so, we will start with the `1st` token's embeddings, then go take and incrementlly add the tokens till `8th` taking mean of them in the way.\n",
    "\n",
    "Which looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c3b3933-d8ce-4f2a-b042-45b62ba49c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A sample data\n",
    "B, T, C = 4, 8, 3\n",
    "sample_xb = torch.arange(B*T*C, dtype=torch.float).view(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f884a-72f1-4133-a202-7eca73e477c4",
   "metadata": {},
   "source": [
    "üëâ The embedding size is `3`, alright?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "842ff48c-143f-474b-88c7-686aee3fa8db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.],\n",
       "         [12., 13., 14.],\n",
       "         [15., 16., 17.],\n",
       "         [18., 19., 20.],\n",
       "         [21., 22., 23.]],\n",
       "\n",
       "        [[24., 25., 26.],\n",
       "         [27., 28., 29.],\n",
       "         [30., 31., 32.],\n",
       "         [33., 34., 35.],\n",
       "         [36., 37., 38.],\n",
       "         [39., 40., 41.],\n",
       "         [42., 43., 44.],\n",
       "         [45., 46., 47.]],\n",
       "\n",
       "        [[48., 49., 50.],\n",
       "         [51., 52., 53.],\n",
       "         [54., 55., 56.],\n",
       "         [57., 58., 59.],\n",
       "         [60., 61., 62.],\n",
       "         [63., 64., 65.],\n",
       "         [66., 67., 68.],\n",
       "         [69., 70., 71.]],\n",
       "\n",
       "        [[72., 73., 74.],\n",
       "         [75., 76., 77.],\n",
       "         [78., 79., 80.],\n",
       "         [81., 82., 83.],\n",
       "         [84., 85., 86.],\n",
       "         [87., 88., 89.],\n",
       "         [90., 91., 92.],\n",
       "         [93., 94., 95.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c98c178f-8e2c-4247-ab46-c456ed4e7e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the array full of zeros to fill the means in\n",
    "x_bow = torch.zeros_like(sample_xb)\n",
    "\n",
    "for b in range(B): # for all samples (here 4)\n",
    "    for t in range(T): # for all tokens (here 8)\n",
    "        x_prev = sample_xb[b, :t+1] # shape T, C\n",
    "        x_bow[b, t] = x_prev.mean(dim=0) # shape C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "faa3d863-109f-4698-8e7e-d16ff088e044",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.],\n",
       "        [12., 13., 14.],\n",
       "        [15., 16., 17.],\n",
       "        [18., 19., 20.],\n",
       "        [21., 22., 23.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# say just a first example B[0]\n",
    "sample_xb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76fb89-1bfa-4b75-bf1d-f22070c1210a",
   "metadata": {},
   "source": [
    "üîº We can compare them üîΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "729971de-f389-41fd-8672-8e2086059139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  2.0000],\n",
       "        [ 1.5000,  2.5000,  3.5000],\n",
       "        [ 3.0000,  4.0000,  5.0000],\n",
       "        [ 4.5000,  5.5000,  6.5000],\n",
       "        [ 6.0000,  7.0000,  8.0000],\n",
       "        [ 7.5000,  8.5000,  9.5000],\n",
       "        [ 9.0000, 10.0000, 11.0000],\n",
       "        [10.5000, 11.5000, 12.5000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for that first example, we have the incremental means\n",
    "x_bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cfb97f-d5dd-4c9a-8240-45e7fc2a726a",
   "metadata": {},
   "source": [
    "Note how it performs the mean incrementally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2dc7d-b187-4e43-9a82-daff49f97404",
   "metadata": {},
   "source": [
    "## üòé The Math Trick!\n",
    "The loops are slow, this was just the example with `4` samples and `8` long context... but when these numbers get big, this way we would spend the most of the time in calculating this stuff than the actual training!\n",
    "\n",
    "So, we will use some maths to give the same result **in a single shot!**.\n",
    "\n",
    "> üî≥üî≤ <br>**The matrix multiplication at rescue!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785cdd74-52f5-482c-b714-dead8f903238",
   "metadata": {},
   "source": [
    "___\n",
    "üîó Here is the [math trick link](https://youtube.com/clip/UgkxoQF1PSblzO0ChHycaNpmq8c29MOmdYDk) where Andrej explains this neatly, I would encourage you to check that out there and then let's continue here.\n",
    "___\n",
    "\n",
    "üìë **The concept summary**:\n",
    "1. The **matrix multiplication** allows us to perform the `sum`.\n",
    "2. We can leverage that sum feature from the matmul.\n",
    "3. But to do that in **incremental fashion** we will have to use the `torch.tril` which gives the ones in the incremental fashion and the rest are zeros, which will let us ignore the other numbers.\n",
    "4. The half part is done, the summation *(which is just the cumsum)*. The rest is to divide to get the mean.\n",
    "5. Which we will get by performing the `.sum(dim=1)` which gives the **count** of elements in that row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f0aa9-299e-4f34-9424-360e319eb3ca",
   "metadata": {},
   "source": [
    "## üìî Just to note... `cumsum` performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663fafd-6946-4013-a1d9-c37e44949533",
   "metadata": {},
   "source": [
    "I am not sure ***why Andrej didn't*** mention this in the lecture, but here is the thing.\n",
    "\n",
    "1. To get the **incremental** mean, we use the `torch.tril`\n",
    "2. Then we divide that with the count\n",
    "3. If we observe, the same thing can be done with the **cumsum** and then we can divide accordingly.\n",
    "\n",
    "üë®‚Äçüíª Here is the code for it:\n",
    "```python\n",
    "data = torch.randint(0, 10, (8, 3), dtype=torch.float)\n",
    "cum_sum = data.cumsum(0) / torch.arange(1, data.shape[0]+1).view(data.shape[0], 1)\n",
    "```\n",
    "\n",
    "When I observed the time between **both** ways *(matmul and cumsum)* I observed a large difference. Here's the code you can check too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e1e94-828d-4fc0-ac83-ec1de9bab506",
   "metadata": {},
   "source": [
    "#### `1.` MatMul "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4004384-44a5-4be5-8c8b-c150d43c3a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A big matrix ;)\n",
    "data = torch.randint(0, 10, (10_000, 10_000), dtype=torch.float)\n",
    "the_a = torch.tril(torch.ones(data.shape[0], data.shape[0]))\n",
    "the_a = the_a / the_a.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2374ed9f-4358-4e07-84f9-c95e2d726506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.57 s ¬± 224 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "the_a @ data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a76dc-a821-4c9b-bf7e-af57d86a50a4",
   "metadata": {},
   "source": [
    "#### `2.` CumSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e72e49c4-4b04-4ee5-88b7-afa51c871897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.62 s ¬± 2.38 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "data.cumsum(0) / torch.arange(1, data.shape[0]+1).view(data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819080f8-b865-4e57-80b2-ff4dc68a2469",
   "metadata": {},
   "source": [
    "Check the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ffa79f4-f181-48a9-944d-01dae0178127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MatMul Way\n",
    "res = the_a @ data\n",
    "# CumSum Way\n",
    "cum_sum = data.cumsum(0) / torch.arange(1, data.shape[0]+1).view(data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e116195a-843c-4d9c-835e-b156b2774560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(res, cum_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3ff1d-d6d3-43b4-91ec-18a507694a3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Both are same! Yo!? üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ac16a-ae3e-438c-9db2-8843a15fda92",
   "metadata": {},
   "source": [
    "# üß∏ With Softmax?\n",
    "Turns out that there is the **third** way of doing this, which is the **real way**. <br>\n",
    "Let's explore why.\n",
    "\n",
    "**What softmax does?** <br>\n",
    "On a high level, it converts the numbers into the probability, right? Means, it **normalizes** the numbers. Or in the other language it **gives the mean** of the numbers!\n",
    "\n",
    "> That's what we do here, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4d7accb-e770-4a87-9d4b-d8573d7ff2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For that we simply create the masked 2D tensor tril\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "\n",
    "# then the \"weights\" which will act as the \"count\" for the average\n",
    "wei = torch.zeros((T, T))\n",
    "\n",
    "# we will \"filter\" and make the rest \"-inf\" to exclude them from the \n",
    "# softmax calculation, otherwise they will affect the softmax normalization\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8b804aa-384c-40f0-b32a-07093c5e9747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000,  2.0000],\n",
       "         [ 1.5000,  2.5000,  3.5000],\n",
       "         [ 3.0000,  4.0000,  5.0000],\n",
       "         [ 4.5000,  5.5000,  6.5000],\n",
       "         [ 6.0000,  7.0000,  8.0000],\n",
       "         [ 7.5000,  8.5000,  9.5000],\n",
       "         [ 9.0000, 10.0000, 11.0000],\n",
       "         [10.5000, 11.5000, 12.5000]],\n",
       "\n",
       "        [[24.0000, 25.0000, 26.0000],\n",
       "         [25.5000, 26.5000, 27.5000],\n",
       "         [27.0000, 28.0000, 29.0000],\n",
       "         [28.5000, 29.5000, 30.5000],\n",
       "         [30.0000, 31.0000, 32.0000],\n",
       "         [31.5000, 32.5000, 33.5000],\n",
       "         [33.0000, 34.0000, 35.0000],\n",
       "         [34.5000, 35.5000, 36.5000]],\n",
       "\n",
       "        [[48.0000, 49.0000, 50.0000],\n",
       "         [49.5000, 50.5000, 51.5000],\n",
       "         [51.0000, 52.0000, 53.0000],\n",
       "         [52.5000, 53.5000, 54.5000],\n",
       "         [54.0000, 55.0000, 56.0000],\n",
       "         [55.5000, 56.5000, 57.5000],\n",
       "         [57.0000, 58.0000, 59.0000],\n",
       "         [58.5000, 59.5000, 60.5000]],\n",
       "\n",
       "        [[72.0000, 73.0000, 74.0000],\n",
       "         [73.5000, 74.5000, 75.5000],\n",
       "         [75.0000, 76.0000, 77.0000],\n",
       "         [76.5000, 77.5000, 78.5000],\n",
       "         [78.0000, 79.0000, 80.0000],\n",
       "         [79.5000, 80.5000, 81.5000],\n",
       "         [81.0000, 82.0000, 83.0000],\n",
       "         [82.5000, 83.5000, 84.5000]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(wei, dim=-1) @ sample_xb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f6cc3-53db-4362-92cc-32da7da864d0",
   "metadata": {},
   "source": [
    "### üñº Which looks like...\n",
    "\n",
    "<img src=\"./images/method_comparision.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d4f45-951b-478e-8d7d-1a38c2060912",
   "metadata": {},
   "source": [
    "Well, it may look like \"softmax\" method has a little bit more involved, but later it will help us, because we will be getting the values of `wei` not from the \"torch.zeros\" but from the data itself as the embeddings, and that will be masked later.\n",
    "\n",
    "So, the conclusion is that... we will be using the Softmax way where:\n",
    "1. We create a `tril` which helps to create a mask\n",
    "2. Using that *triled* array to create an actual mask on the `wei` or `weights` so that the future tokens don't communicate with the past.\n",
    "3. Then run through the softmax to get the normalized values and finally attatch (matmul) them with the data `x` to get the incremental mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c64fe3-5adb-4713-8e39-50d0ba7f6184",
   "metadata": {},
   "source": [
    "# üìÖüå± Driving our way towards maturity\n",
    "The old days of *playing* with the bigram toy model are over, let's build ourself up for the GPT. So, from now we will change the code a little-by-little and make it GPTable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b74d3ccc-4bb5-426b-aa7e-48a328b63d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32       # n_samples\n",
    "block_size = 8        # context_window\n",
    "max_iters = 20_000    # total steps\n",
    "eval_interval = 1_000 # interval at we will print the loss\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32   ### Added new - for the extra layer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8678d491-7690-4ee1-b9ae-90cd0e1f62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes list for this run:\n",
    "    1. Removed the `vocab_size` as it was kind of redundant\n",
    "    2. Added a new linear layer of shape `n_embd` which then will\n",
    "        return the logits of `vocab_size`\n",
    "    3. In the `forward` function to get the logits, added a matmul operation.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # added a layer\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Just take the index of the \"current\" character \n",
    "        and then use that to get the probability dist \n",
    "        for the next one using the `embedding_table`.\n",
    "        '''\n",
    "\n",
    "        tok_emb = self.embedding_table(idx) # B, T, n_emb\n",
    "        logits = self.lm_head(tok_emb) # B, T, vocab_size\n",
    "        \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Take the index of the token and guess the next\n",
    "        token based on the embeddings!\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # call the `forward` method \n",
    "            logits, loss = self(idx) # we can do this because we have inherited `nn.Module` :)\n",
    "            \n",
    "            # Take the very last token (8th in the context window)\n",
    "            # and use its distribution to get the next token\n",
    "            logits = logits[:, -1, :] ## refer: Change [2]\n",
    "\n",
    "            # Convert the logits into the probabilities\n",
    "            probs = F.softmax(logits, dim=-1) ## dim=-1: along the last dimension ~ here `1`\n",
    "\n",
    "            # Take the next idx!\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Here we will ALWAYS append, there is NO shrinking!\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2568f8ba-1f1d-4d5b-bcc5-a3ad211d4608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbfa98ae-2e89-4ca5-b4bd-053b22b4c20c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ccb65be-c166-47b6-948e-f677c3eddeaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 32)\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1dfdde8-be74-4e98-a2e6-158bb54cd55d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~4.2778, Val Loss~4.2879\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "        break\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e016e85-2f1a-41d0-8634-0852ccce3b32",
   "metadata": {},
   "source": [
    "Okay, works just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69774d0-3f3c-440c-865c-f8ae80a3f5ef",
   "metadata": {},
   "source": [
    "# üèõÔ∏è The architecture\n",
    "I have been resisting an urge to put the \"famous\" transformer image, but I can't stop myself now. Let's keep referring the diagram to have some clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc591a-babe-4b9b-bc02-9a09b17e2da5",
   "metadata": {},
   "source": [
    "<img src=\"./images/whole-trans.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb9d29-fa9a-498a-ad31-ab4f3b854265",
   "metadata": {},
   "source": [
    "## üßπ Let's simplify it a bit.\n",
    "So, we won't be using the `encoder - decoder` architecture here, which is generatlly used in the **\"seq2seq\"** models like BERT. What GPT is `decoder only` model, which has the **right** part only and is called the **causal language model**. It doesn't care about the input it only looks at the past tokens and keeps creating the new tokens.\n",
    "\n",
    "Let's update the architecture step by step.\n",
    "\n",
    "<img src=\"./images/decoder-only.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d84f2e-c791-42e7-9b0d-0ed8dfabb582",
   "metadata": {},
   "source": [
    "## üòå Alright, what a relief!\n",
    "Let me show the clean image..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633e44f-8072-4794-9000-815a7e57fa3a",
   "metadata": {},
   "source": [
    "<img src=\"./images/decoder-only-cleaned.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c6b37-a310-4070-b5ab-683fa7fad6f9",
   "metadata": {},
   "source": [
    "üòÖ <br> Which turns out to be **just same as the encoder** except it has the \"masked\" part - where the future tokens don't talk to the past. We will clean, and further annotate this architecture... but be sure that what you are looking at is the backbone of the GPT.\n",
    "\n",
    "*(have changed a little bit of terms compared to the original architecture for simplicity)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8bfd8-41e1-425d-bd93-87660b1d444d",
   "metadata": {},
   "source": [
    "# ü´Ä Positions: The heart of LLMs\n",
    "There is a famous pair of sentences: **\"Aayush ate Pizza\"** and **\"Pizza ate Aayush\"**. Do they mean the same thing?\n",
    "\n",
    "Of course, not. \n",
    "\n",
    "- Just by changing the positions of the words, the meaning will change.\n",
    "- Also positions take care of **how the words are connected** which in turn, takes care of the grammar.\n",
    "- So, where we have the case of **generating** the text, we need to take care of the positions!\n",
    "\n",
    "> *That means, we need to keep track of the positions where the word/token occur.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a660b-2912-4a19-81b5-5ffdbf584113",
   "metadata": {},
   "source": [
    "### üßê Let's understand it...\n",
    "\n",
    "```python\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, n_embd)        ### Added this ###\n",
    "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "def forward(self, idx, targets=None):\n",
    "    B, T = idx.shape\n",
    "\n",
    "    tok_emb = self.embedding_table(idx) # B, T, n_emb\n",
    "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) ### Added this ###\n",
    "    x = tok_emb + pos_emb                                                   ### Added this ###\n",
    "    logits = self.lm_head(x) # B, T, vocab_size \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e6a2f-6878-4e88-a830-e150c23a61c3",
   "metadata": {},
   "source": [
    "<img src=\"./images/old-network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d466941-61ac-4080-b432-d9d0b311790a",
   "metadata": {},
   "source": [
    "üìç And so now, we will need to keep track of the positions... **so later** when we want to ***flow*** the information from the past, we can leverage the positions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52238d11-101a-4eeb-8b41-0ca7243ca98e",
   "metadata": {},
   "source": [
    "## üî• And now... \n",
    "\n",
    "<img src=\"./images/with-positions-network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b69f5c-7abf-49c7-a985-0ace0c0c5cc8",
   "metadata": {},
   "source": [
    "> ‚úÖ <br>Hope it is clear now... and we have now **successfully completed** this part of the architecture!\n",
    "\n",
    "<img src=\"./images/positions-done.png\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc75c6b-d4f0-4f24-866e-e74b1252edb9",
   "metadata": {},
   "source": [
    "# üòå Great, done with the positions...\n",
    "We have just understood the underlying structure... and **haven't** yet implemented that one in the model... we will, **but let's join another piece** of the puzzle ***self attention*** ü§≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78427546-f4f8-4b34-81e7-f123e4ea2a21",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3badb-e373-4580-a1eb-bf0c67f5b6a8",
   "metadata": {},
   "source": [
    "# üôá‚Äç‚ôÇÔ∏èü§ùüëÅÔ∏è Self Attention: The Affinity\n",
    "Alright have this sentence in mind:\n",
    "\n",
    "    \"When they finally reached the scene of the place crash, they were dead.\"\n",
    "    \n",
    "Are **they** dead who reached there? Or are **they** dead who were there already? <br>See how the pronoun **they** is used. It is confusing to us and hence to get the meaning of the sentence fully, the model needs to **keep track** of the inter-relations between the tokens what means what.\n",
    "\n",
    "> üôá‚Äç‚ôÇÔ∏è <br>And it is solved by the **Self Attention**. <br> It lets the model to focus on the specific and most important parts of the text letting the model to keep context between the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26623210-50b5-4ccc-9d84-7f2de31e6be9",
   "metadata": {},
   "source": [
    "## ü™ùQuery ‚Äî üîëKey ‚Äî üí∞Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90ec44-1843-4efe-b1bd-f264620161b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Remember our softmax way?\n",
    "\n",
    "```python\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "F.softmax(wei, dim=-1) @ sample_xb\n",
    "```\n",
    "\n",
    "There we were defining the `wei` of the **weights** to be uniform *all zeros*. But that was just for the concept. That represents **\"how each token finds the previous token more or less interesting\"** and **that's what we will find using ü™ùüîëüí∞**!\n",
    "\n",
    "So whole thing is to ***find*** the `wei`. We will need to calculate it. So, let's start the journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285e45b-ac97-43f3-b8d6-86b4b9288c03",
   "metadata": {},
   "source": [
    "# ü™ùQuery\n",
    "\n",
    "> The \"Hook\". \n",
    "\n",
    "For each token, for each individual token we will create some values which will be used to **match** the keys of the rest of the tokens ***including itself***.\n",
    "\n",
    "> **Query vector**: *\"What am I looking for?\"*\n",
    "> <br>‚Äî Andrej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cae0e-ee38-4dfb-aa16-7284dc18efb5",
   "metadata": {},
   "source": [
    "# üîë Key\n",
    "\n",
    "> The \"Key\". \n",
    "\n",
    "We will match the **query** of each token to the keys of each token. That will simply be the matrix multiplication *(dot product)* between them.\n",
    "\n",
    "> **Key vector**: *\"What do I contain?\"*\n",
    "> <br>‚Äî Andrej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c35b09-1cc6-4496-913b-a5781bf0db6f",
   "metadata": {},
   "source": [
    "# ü™ù @ üîë Dot Product\n",
    "Here all tokens will interact with each other which will give us the `wei`... but wait, before using that `wei` we will **mask** the future tokens!\n",
    "\n",
    "*(don't worry, everything will be illustrated in a bit üé®)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f34e18-5019-4d23-9045-915174b88e2b",
   "metadata": {},
   "source": [
    "# üí∞ Value\n",
    "> The \"Information\".\n",
    "\n",
    "Why aren't we done when we performed the dot product between the query and the key? I mean we got the `wei` and that's it! We just found all the affinities between the tokens... so **we should be able to move forward with this extra information** and perform the last step to **use this information in the context of the actual data / actual tokens**.\n",
    "\n",
    "But not. We produce one more vector \"value\" - to represent the actual tokens; and that will finally multiply the `wei` üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95631536-5b1e-4a37-852b-d59babbd05a8",
   "metadata": {},
   "source": [
    "### ü§î Side note, what is a need of \"Value\" vector?\n",
    "\n",
    "ChatGPT has a good and satisfactory answer:\n",
    "\n",
    "1. **Separation of Concerns**: The Transformer architecture <u>employs multi-head attention</u>, where multiple sets of key, query, and value matrices are learned. <u>Each of these sets focuses on different aspects or patterns</u> in the input sequence. By using separate \"value\" vectors, the <u>model can flexibly adjust</u> the way it combines information from the input sequence **without changing the original token embeddings**. This separation of concerns allows the model to learn richer and more diverse representations.\n",
    "\n",
    "> üçµ <br>*My thought, so can we introduce another vector? <br>Query, Key, Value and **MasterValues**? That will give more flexibility in multiple attention heads!*\n",
    "\n",
    "2. **Weighted Sum**: After computing the attention scores, <u>the \"value\" vectors are used to compute a weighted su</u>m. The attention scores determine how much weight each \"value\" vector contributes to the output. This weighted sum operation effectively combines information from different parts of the input sequence to produce the final output. <u>If you used the original token embeddings directly, you wouldn't have this flexibility</u> to compute different weighted sums for different output tokens.\n",
    "\n",
    "3. **Parameterization**: The \"value\" vectors are <u>learnable parameters</u>, which means the model can adaptively adjust them during training to better capture relevant information from the input sequence. In contrast, if you used fixed token embeddings, the model would be limited to a static representation of the input.\n",
    "\n",
    "___\n",
    "\n",
    "**TL;DR**: It is all about the learning flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2651b1-9c5d-4dfb-a8c0-1b066c2d3360",
   "metadata": {},
   "source": [
    "### üßê Let's understand it...\n",
    "\n",
    "```python\n",
    "B, T, n_embd = 4, 3, 5\n",
    "after_position_calculation = torch.rand(B, T, n_embd)\n",
    "\n",
    "head_size = 8\n",
    "query = nn.Linear(n_embd, head_size, bias=False)\n",
    "key = nn.Linear(n_embd, head_size, bias=False)\n",
    "value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "q = query(after_position_calculation)\n",
    "k = key(after_position_calculation)\n",
    "v = value(after_position_calculation)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed466d-bd3a-43d3-86bf-95160d65ae80",
   "metadata": {},
   "source": [
    "# Wnat some visuals? Seriously? Okay ü§®\n",
    "\n",
    "<img src=\"./images/okay-sherlock.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0ef4c-3b71-40d1-9402-142bf6434c2f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204cb03-3c25-46de-b6ef-3c98be7991ee",
   "metadata": {},
   "source": [
    "<img src=\"./images/QKV.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea881c0-9043-41ef-8bea-0cd1978cd36c",
   "metadata": {},
   "source": [
    "# üèÜüåüüëè Honorable Mention\n",
    "Many visuals and simple explanations *(including the funny styles of drawing arrows)* are heavily inspired from our well known **BAM!!**, Josh Starmer. His incredibly easy to follow video for [decoders](https://www.youtube.com/watch?v=bQ5BoolX9Ag&t) is the most recommended üôå."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad629a-5ea8-416e-81be-061b168c26cd",
   "metadata": {},
   "source": [
    "# üë®‚Äçüíª Let's quickly implement it!\n",
    "I can't wait to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2398d95-af36-4c4e-92cd-fbacf8f6593d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' The old stuff, just keeping here to refer :) '''\n",
    "\n",
    "batch_size = 32       \n",
    "block_size = 8        \n",
    "n_embd = 32\n",
    "\n",
    "max_iters = 20_000    \n",
    "eval_interval = 1_000 \n",
    "eval_iters = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---\n",
    "\n",
    "head_size = n_embd ## For now, to avoid the dimensions mismatch below; but that will be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bf21b87-967e-4774-8514-192f9a6c6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes list for this run:\n",
    "    1. Added the code for the positions embeddings\n",
    "    2. Added `SA` head, self-attention head. That `Head` class is implemented\n",
    "    below.\n",
    "    3. Added a safe truncate in the `generate` method to avoid having text\n",
    "    longer than the expected `block_size`\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        #üîªADDED\n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        #üîªADDED | üñä REMAINING IMPLEMENTATION\n",
    "        self.sa_head = Head(head_size)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Changes:\n",
    "        1. Added the forward for positions\n",
    "        2. Code to \"add\" the positions embeddings with the token embeddings\n",
    "        3. Passing that in the `sm_head` which will give the final vector to pass in the `lm_head`\n",
    "        '''\n",
    "        #üîªADDED\n",
    "        B, T = idx.shape \n",
    "        \n",
    "\n",
    "        tok_emb = self.embedding_table(idx) # B, T, n_emb\n",
    "        #üîªADDED\n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device)) # T, n_emb\n",
    "        #üîªADDED\n",
    "        x = tok_emb + positions_emb # B, T, n_emb\n",
    "        #üîªADDED | üñä REMAINING IMPLEMENTATION\n",
    "        x = self.sa_head(x) # B, T, head_size\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "        \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Take the index of the token and guess the next\n",
    "        token based on the embeddings!\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            #üîªADDED\n",
    "            idx_cond = idx[:, -block_size:] # make sure we only have size of `T`\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2db8c26-dafd-41be-9b77-f6ff6a15ce98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class will simply create the Q, K, V vectors\n",
    "    and also the reguster_buffer to create the mask.\n",
    "    \n",
    "    Then on the `forward` it will pass the vectors in the \n",
    "    Q, K, V and give the `out`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # What is this? Explained below.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Take the `x` input which will be the positions.\n",
    "        The shape will be B, T, C meaning:\n",
    "        \"For each batch, there will be T tokens which will have positions encoded in C\n",
    "        space\"\n",
    "        \n",
    "        We will use that and work oursalves forward.\n",
    "        '''\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # the C**-0.5 is used to control the variance\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # the mask\n",
    "        wei = F.softmax(wei, dim=-1) # the final wei\n",
    "\n",
    "        out = wei @ v # this is what we will use further\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dae0e-100f-47ec-a0aa-9962d7068958",
   "metadata": {},
   "source": [
    "### The \"buffers\"?\n",
    "\n",
    "> *register_buffer is a method in PyTorch that allows you to add a tensor to a module's state. This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state.*\n",
    "> <br>‚Äî Bard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323c7d3-ff4f-48e4-9504-cefd78572f14",
   "metadata": {},
   "source": [
    "## üöÜ Training *(excited)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f0cdc8e-c9a5-40d0-85cc-e5d58ccc5168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aef48fd9-d264-41b0-b0a2-f6170c64c6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "faf3db3d-b54c-4bd2-860b-069f967641aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 32)\n",
      "  (positions_embeddings): Embedding(8, 32)\n",
      "  (sa_head): Head(\n",
      "    (query): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (key): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (value): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5b8913c-8867-476c-868e-873bb44c6cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~2.3763, Val Loss~2.3915\n",
      "[Step 1000]: Train Loss~2.3596, Val Loss~2.3879\n",
      "[Step 2000]: Train Loss~2.3590, Val Loss~2.3772\n",
      "[Step 3000]: Train Loss~2.3570, Val Loss~2.3756\n",
      "[Step 4000]: Train Loss~2.3498, Val Loss~2.3761\n",
      "[Step 5000]: Train Loss~2.3601, Val Loss~2.3789\n",
      "[Step 6000]: Train Loss~2.3372, Val Loss~2.3651\n",
      "[Step 7000]: Train Loss~2.3490, Val Loss~2.3687\n",
      "[Step 8000]: Train Loss~2.3315, Val Loss~2.3698\n",
      "[Step 9000]: Train Loss~2.3318, Val Loss~2.3681\n",
      "[Step 10000]: Train Loss~2.3274, Val Loss~2.3652\n",
      "[Step 11000]: Train Loss~2.3277, Val Loss~2.3557\n",
      "[Step 12000]: Train Loss~2.3407, Val Loss~2.3744\n",
      "[Step 13000]: Train Loss~2.3410, Val Loss~2.3667\n",
      "[Step 14000]: Train Loss~2.3162, Val Loss~2.3588\n",
      "[Step 15000]: Train Loss~2.3249, Val Loss~2.3655\n",
      "[Step 16000]: Train Loss~2.3252, Val Loss~2.3684\n",
      "[Step 17000]: Train Loss~2.3237, Val Loss~2.3779\n",
      "[Step 18000]: Train Loss~2.3324, Val Loss~2.3586\n",
      "[Step 19000]: Train Loss~2.3358, Val Loss~2.3594\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd8a40-c8ac-46c2-84d4-bddee0003a3e",
   "metadata": {},
   "source": [
    "## üéâ Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20f337fd-e1ae-4f48-ab04-b33faef634b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thins; sis kes shuk ble-mar, sates spaith but olk virescece:\n",
      "O thers\n",
      "At fishe,\n",
      "Sed we aris il\n",
      "\n",
      "ENCELONUS:\n",
      "Beirak der, hry.\n",
      "\n",
      "\n",
      "NG LARUDKII wongd ses amawshe. \n",
      "I thirear I IS:\n",
      "Binond courm hir, byoras o'd;\n",
      "\n",
      "Thathese haw Hel brno en:\n",
      "Big wiflouth ff ma than:\n",
      "Hraces chet, ofart, l's odl, ta he uat re. Ga Bon' inol s' I orrt burs blitherower lifl he puns foour out arsceard; wat ngoo.\n",
      "\n",
      "Cur woury poour,\n",
      "The,\n",
      "I LIse thonoan dto.\n",
      "\n",
      "POUS:\n",
      "\n",
      "Mar stha pat ing: brain shown'ed.\n",
      "\n",
      "MARKENORY :\n",
      "Tour hehe won ou\n",
      "'lll\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624a51a-dcd6-4c70-8ae5-ece257e4cb0d",
   "metadata": {},
   "source": [
    "> Okay, it is **being human**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770860a2-0d12-4daf-bf73-c811e687db86",
   "metadata": {},
   "source": [
    "# üß†üë•üëÅ Multi-head attention\n",
    "I like this name so much that it's the name of my router ü§ó\n",
    "\n",
    "<img src=\"./images/wifi.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa37abe-7c68-4611-80c2-d32b500dd336",
   "metadata": {},
   "source": [
    "Let's expand the model so that it can learn more relationships between the tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b22fdca3-aec2-4d9f-b57c-d36a5157930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is simpler that you think!\n",
    "    It will simply create `n` heads (each of them with seperate Q, K, V settings)\n",
    "    and will pass the `x` individually (seperate) in each heads and then finally \n",
    "    will combine each `out` on the last dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e3f24-ba6c-4c89-b65a-cdee7ece1812",
   "metadata": {},
   "source": [
    "<img src=\"./images/multihead-concat.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574c304-ff00-4b42-9b06-652ca8daa6e8",
   "metadata": {},
   "source": [
    "And so now the Bigram Model will be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5140f660-2c63-4f1f-87ed-7a37a1a95e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes list for this run:\n",
    "    1. Added multi-head attention!\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # to make shape workout with the next layer...\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Changes:\n",
    "        Nothing to change here.\n",
    "        '''\n",
    "        B, T = idx.shape \n",
    "        \n",
    "\n",
    "        tok_emb = self.embedding_table(idx) # B, T, n_emb\n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device)) # T, n_emb\n",
    "        x = tok_emb + positions_emb # B, T, n_emb\n",
    "        x = self.sa_heads(x) # B, T, head_size\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "        \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8359f8-2923-4d08-a447-204e95cdef0e",
   "metadata": {},
   "source": [
    "## üöÜ Training *(excitedx2)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8bb97d84-dc93-43d0-b65b-c1a3d15bb457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f16eff05-9ea7-4539-888b-272ba37ff114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc35534-8499-47d5-99de-91f10706b508",
   "metadata": {},
   "source": [
    "> üìî **Note** the same number of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "769f4384-38e3-4538-aeb9-05b11aaef444",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 32)\n",
      "  (positions_embeddings): Embedding(8, 32)\n",
      "  (sa_heads): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "05c88666-8f86-4e35-be91-45f9126a7927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~4.2530, Val Loss~4.2557\n",
      "[Step 1000]: Train Loss~2.5166, Val Loss~2.5187\n",
      "[Step 2000]: Train Loss~2.3898, Val Loss~2.4064\n",
      "[Step 3000]: Train Loss~2.3508, Val Loss~2.3434\n",
      "[Step 4000]: Train Loss~2.2879, Val Loss~2.3015\n",
      "[Step 5000]: Train Loss~2.2716, Val Loss~2.2902\n",
      "[Step 6000]: Train Loss~2.2474, Val Loss~2.2635\n",
      "[Step 7000]: Train Loss~2.2294, Val Loss~2.2553\n",
      "[Step 8000]: Train Loss~2.2151, Val Loss~2.2483\n",
      "[Step 9000]: Train Loss~2.1988, Val Loss~2.2501\n",
      "[Step 10000]: Train Loss~2.1957, Val Loss~2.2268\n",
      "[Step 11000]: Train Loss~2.1828, Val Loss~2.2167\n",
      "[Step 12000]: Train Loss~2.1684, Val Loss~2.2066\n",
      "[Step 13000]: Train Loss~2.1616, Val Loss~2.2336\n",
      "[Step 14000]: Train Loss~2.1607, Val Loss~2.2092\n",
      "[Step 15000]: Train Loss~2.1560, Val Loss~2.2192\n",
      "[Step 16000]: Train Loss~2.1593, Val Loss~2.2219\n",
      "[Step 17000]: Train Loss~2.1564, Val Loss~2.2150\n",
      "[Step 18000]: Train Loss~2.1386, Val Loss~2.2128\n",
      "[Step 19000]: Train Loss~2.1401, Val Loss~2.2023\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b66ee5-6030-408f-9fea-499931db6e83",
   "metadata": {},
   "source": [
    "## üéâ Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2612a60-eebf-4132-b517-90c87a554f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MEONTERL:\n",
      "Het.\n",
      "You has to acust, Cis beight guard mesto way dand have drae\n",
      "vire, nern ane wind set!\n",
      "\n",
      "Gow'd the shatortu: porwer my as beree more.\n",
      "\n",
      "\n",
      "Frod no aness! prours bee padty me we weirg onecloik is neat;\n",
      "Butre of ith,\n",
      "hind me you rone kings with at if, ther-dunickin shis bougr,\n",
      "Youmy at's hinto hatene and alighavery a wirgl inhe bread ish gun in Donot nempto ne, be you.\n",
      "\n",
      "Sony corthad din woo.\n",
      "\n",
      "KINCE:\n",
      "You in! the tham be crings you'l thour so rad;\n",
      "Whhave he jred bre'stres.\n",
      "\n",
      "S\n",
      "\n",
      "MIRGOLORD:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ae370-c322-4333-b73a-b36323e93b1e",
   "metadata": {},
   "source": [
    "> **Hell yeah! It is improved!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313ccea-d88f-42fb-ad5b-89c99c0c40ca",
   "metadata": {},
   "source": [
    "# üßº‚ú® The Feed Forward Simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31465a23-4862-43ee-9ad8-6464af2fa850",
   "metadata": {},
   "source": [
    "> ‚úÖ <br>Hope it is clear now... and we have now **successfully completed** this part of the architecture!\n",
    "\n",
    "<img src=\"./images/multihead-maskdone.png\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fa666-162e-4c80-8b6f-839f39b5dcdc",
   "metadata": {},
   "source": [
    "The next part that we will be implementing is the ***\"Feed Forward Layer\"***.\n",
    "\n",
    "#### üöÄ Motivation of this layer\n",
    "> *When we calculated the QKV and solved that multi head part, **we went way too fast** for calculating the probability for the next token. The tokens looked at each other but didn't have the time to **think on** the information that they have found from the other tokens.* <br> ‚Äî Andrej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24308bb3-154e-4730-85a9-fd00d640d260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Just a single layer!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8995b39a-faf3-4eaa-89d9-05f494450f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes list for this run:\n",
    "    1. Added Feed forward\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4) \n",
    "        self.ffwd = FeedForward(n_embd) ### JUST ADDED THIS ###\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Changes:\n",
    "        Nothing to change here.\n",
    "        '''\n",
    "        B, T = idx.shape \n",
    "        \n",
    "\n",
    "        tok_emb = self.embedding_table(idx) # B, T, n_emb\n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device)) # T, n_emb\n",
    "        x = tok_emb + positions_emb # B, T, n_emb\n",
    "        x = self.sa_heads(x) # B, T, head_size\n",
    "        x = self.ffwd(x) ### AND THIS ###\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "        \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # make sure we only have size of `T`\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab4941-1a13-49d8-a8e8-f4bb95cb6fc4",
   "metadata": {},
   "source": [
    "## üöÜ Training *(excitedx3)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04bbb50b-ac0b-4061-b53a-688bed108fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b01be1a5-f055-454e-9fa5-52e4d13e8d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e120aecb-8fd5-4849-8387-4d2761a4d187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 32)\n",
      "  (positions_embeddings): Embedding(8, 32)\n",
      "  (sa_heads): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ffwd): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e721be9-8517-4adc-b1a2-b4e5e7a6056c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~4.2171, Val Loss~4.2125\n",
      "[Step 1000]: Train Loss~2.4522, Val Loss~2.4433\n",
      "[Step 2000]: Train Loss~2.3255, Val Loss~2.3314\n",
      "[Step 3000]: Train Loss~2.2723, Val Loss~2.2850\n",
      "[Step 4000]: Train Loss~2.2333, Val Loss~2.2622\n",
      "[Step 5000]: Train Loss~2.2139, Val Loss~2.2419\n",
      "[Step 6000]: Train Loss~2.1865, Val Loss~2.2280\n",
      "[Step 7000]: Train Loss~2.1735, Val Loss~2.2081\n",
      "[Step 8000]: Train Loss~2.1510, Val Loss~2.1879\n",
      "[Step 9000]: Train Loss~2.1450, Val Loss~2.1956\n",
      "[Step 10000]: Train Loss~2.1317, Val Loss~2.1705\n",
      "[Step 11000]: Train Loss~2.1270, Val Loss~2.1847\n",
      "[Step 12000]: Train Loss~2.1116, Val Loss~2.1748\n",
      "[Step 13000]: Train Loss~2.1069, Val Loss~2.1679\n",
      "[Step 14000]: Train Loss~2.0946, Val Loss~2.1736\n",
      "[Step 15000]: Train Loss~2.1029, Val Loss~2.1679\n",
      "[Step 16000]: Train Loss~2.0843, Val Loss~2.1668\n",
      "[Step 17000]: Train Loss~2.0794, Val Loss~2.1808\n",
      "[Step 18000]: Train Loss~2.0897, Val Loss~2.1582\n",
      "[Step 19000]: Train Loss~2.0780, Val Loss~2.1467\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2264d8-93d6-4ff9-ade1-6e5fea4c3741",
   "metadata": {},
   "source": [
    "## üéâ Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "62b3fbe2-e4fd-48d5-8b52-524b3756fdc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shan con; but his frad,\n",
      "Buch fim the's\n",
      "tack knounk Is he to ince in king and thoulds:\n",
      "Whice shat a:\n",
      "Seare willafgut villsed ort?- RIZABENVINCESSTENCEYCUCKING Roful ahat to is ties be man orm leme!\n",
      "\n",
      "I is and That mell to cmarwich Tho you shave give trer cach by hup ance to,\n",
      "Tharss nour felf;\n",
      "Bit tazen\n",
      "Of to sprond wing me ver it wores.\n",
      "\n",
      "HEOLARD:\n",
      "In, and\n",
      "Iron ther, shat what to grere cibrut Pagctyour tworseatud, no planesen that fiurathou aps, good the carce a noplet.\n",
      "\n",
      "MHENCIO:\n",
      "Sarn shond tate thu\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e20f9-46f1-4f1d-8593-69588b8f24d0",
   "metadata": {},
   "source": [
    "> The model's still dumb üòÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3094b-0bb2-496a-839e-13839689efc1",
   "metadata": {},
   "source": [
    "> ‚úÖ <br>One more lego piece is done now.\n",
    "\n",
    "<img src=\"./images/feedfwd-done.png\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8d615-168f-4f9e-b62f-6898b687e056",
   "metadata": {},
   "source": [
    "# üß© The \"Add & Norm\" with Residual Connections\n",
    "Okay, now it is the time to add the mysterious **Add & Norm** layers and will use the **residual connection** which is just to carry the original tokens' information to the transformed information.\n",
    "\n",
    "**Plan of attack**:\n",
    "- First we will implement our decoder as given in the architecture, which **first** has these masked Multi-Head attention **and then** the Add & Norm layer.\n",
    "- After checking the accuracy with that, we will create a second version which is shown in the Andrej's lecture, in which **first** we will use the Add & Norm later **and then** use the masked multi-head and so on.\n",
    "\n",
    "So, let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf87c1-95ac-4f80-95c4-a5a8208bde2c",
   "metadata": {},
   "source": [
    "## ‚ûïüìä What is \"Add & Norm\"?\n",
    "If you remember the *BatchNorm* layer, which we used to **standardize** the weights of the layer **using the mean and variance** across multiple samples in the batch, here we will do the same, but this will use the mean and variance of the **given sample**. So, there is no need to keep running mean and variance, what a relief üòå"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b38b8-7fa2-4e65-92af-7cbcb5707068",
   "metadata": {},
   "source": [
    "## üîó Now, Residual connection?\n",
    "When the netwrok gets very deep, the issue of **vanishing gradient** occurs. Which is, while backpropogation the gradients become very small that even if they are carrying information but because of so many layers in between, little to no information reaches to the initial-input layers. The gradient update becomes so small there.\n",
    "\n",
    "To prevent this, these **residual connection** or **skip connections** is one of the remedies. Let me paste the ChatGPT response for a clearer understanding:\n",
    "\n",
    "> ### üóí \n",
    "> *The problem arises when the gradients become **very small** as they are propagated backward through many layers. In some cases, they can become so close to zero that they **effectively carry little to no information** about how to update the parameters in the early layers of the network. <br> <br>When gradients vanish, it **becomes extremely challenging** to train the network effectively. Layers earlier in the network do not receive meaningful updates, and as a result, these layers learn very slowly or not at all. This is a significant obstacle in training deep neural networks because the **network's depth** is one of the key factors that can enable it to learn complex and hierarchical representations. <br> <br> The vanishing gradient problem is especially pronounced in networks that use certain activation functions (e.g., sigmoid or tanh) because these functions squash their input into a limited range, making it easier for gradients to vanish when their absolute values are small.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea536a5-c9da-42bb-8e76-0afc81e487ab",
   "metadata": {},
   "source": [
    "## ü§î And... projection layer?\n",
    "Nobody told you about this? üòÖ <br>\n",
    "Alright, it is just to **match** the input and output shapes. Since the shapes matter the most in the NN, we will need to use it. \n",
    "\n",
    "**But when?**<br>\n",
    "Actually the code below shows you the `residual connection` done by the following lines:\n",
    "```python\n",
    "x = x + self.sa_heads(x)\n",
    "x = x + self.ffwd(x) \n",
    "```\n",
    "\n",
    "As you can see the `x` goes through a couple of computation, inside the `sa_heads` and `ffwd` layers, and when it comes out, we are using the **addition** with the `x`. That is the residual connection and there **we need to makesure** that the shape matches!\n",
    "\n",
    "And for that we will implement the `proj` or **projection** layers in the `MultiHead` and `FFWD` :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39926c-d037-400e-a1e1-e88157b05838",
   "metadata": {},
   "source": [
    "# üíª Let's implement this (these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0d86047-8781-4027-8ba2-271af4638484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes:\n",
    "    1. Added a `projection` layer. Since we are using the residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out) # Called here...\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2a7fc-2e5b-4318-8263-813d7148640b",
   "metadata": {},
   "source": [
    "### Wondering something? Anything?\n",
    "If not, then should. You should be a pretty confused at this point that ***if torch.cat() concatenates all the dim=-1, and the `proj` layer accepts `n_embd` shape, then how do the shape work out?***.\n",
    "\n",
    "Well, it wouldn't but since we are passing the shapes `self.sa_heads = MultiHeadAttention(4, n_embd//4)`, it will automatically match because we are using `n_embd//4` and when it will get concatenated `4` times, it will become `n_embd` üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9211eb61-f628-4889-bf3f-997f06278a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes:\n",
    "    1. Added the `proj` layer here as well.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd, n_embd) ## The proj layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e580ce2-5965-4140-97fe-7252e6be8684",
   "metadata": {},
   "source": [
    "### But according to the paper...\n",
    "The **Feed Forward** layer should be the **multiplied by `4`**... so just a small change below üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1b36549-d78d-449a-b5af-f280b98e2315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes:\n",
    "    1. Added the `proj` layer here as well.\n",
    "    2. Added the multiplied by `4`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd) ## The proj layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "508ca633-e14b-4584-b4f5-95dd15c4a07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes:\n",
    "    1. Added the residual connections\n",
    "    2. Added 2 layers for the layernorm.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4) \n",
    "        self.add_norm_1 = nn.LayerNorm(n_embd) ## ADDED THE LN 1 ##\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.add_norm_2 = nn.LayerNorm(n_embd) ## ADDED THE LN 2 ##\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Changes:\n",
    "        1. Applied layernorm after the self-attention and ffwd.\n",
    "        '''\n",
    "        B, T = idx.shape \n",
    "        \n",
    "\n",
    "        tok_emb = self.embedding_table(idx) # B, T, n_emb\n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device)) # T, n_emb\n",
    "        x = tok_emb + positions_emb         # B, T, n_emb\n",
    "        \n",
    "        ### RESIDUAL CONNECTIONS & LAYER NORM ###\n",
    "        x = x + self.add_norm_1(self.sa_heads(x))  # B, T, head_size\n",
    "        x = x + self.add_norm_2(self.ffwd(x)) \n",
    "        logits = self.lm_head(x)             # B, T, vocab_size\n",
    "        \n",
    "        ### NOTE: The layernorm is applied AFTER the self-attention and ffwd.\n",
    "        ### in the later code we will use as used in  Andrej's lecture.\n",
    "        \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # make sure we only have size of `T`\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafba430-8c45-4225-9301-9620e8b909eb",
   "metadata": {},
   "source": [
    "## üöÜ Training *(excitedx4)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0a4e8aa1-080d-497f-ad81-d587c0224988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d282e271-9d6a-4e7e-b0e7-5c9c3a5b093d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a9ae1475-d36f-4f8b-9cc4-c99ebb946870",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 32)\n",
      "  (positions_embeddings): Embedding(8, 32)\n",
      "  (sa_heads): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  )\n",
      "  (add_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffwd): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (add_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0cbbd90a-2b95-4431-9e7e-2e962157eaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~4.8423, Val Loss~4.8601\n",
      "[Step 1000]: Train Loss~2.3000, Val Loss~2.2975\n",
      "[Step 2000]: Train Loss~2.1771, Val Loss~2.2149\n",
      "[Step 3000]: Train Loss~2.1228, Val Loss~2.1739\n",
      "[Step 4000]: Train Loss~2.0763, Val Loss~2.1493\n",
      "[Step 5000]: Train Loss~2.0567, Val Loss~2.1216\n",
      "[Step 6000]: Train Loss~2.0303, Val Loss~2.1010\n",
      "[Step 7000]: Train Loss~2.0041, Val Loss~2.0857\n",
      "[Step 8000]: Train Loss~1.9928, Val Loss~2.0740\n",
      "[Step 9000]: Train Loss~1.9830, Val Loss~2.0999\n",
      "[Step 10000]: Train Loss~1.9806, Val Loss~2.0829\n",
      "[Step 11000]: Train Loss~1.9561, Val Loss~2.0674\n",
      "[Step 12000]: Train Loss~1.9727, Val Loss~2.0826\n",
      "[Step 13000]: Train Loss~1.9700, Val Loss~2.0588\n",
      "[Step 14000]: Train Loss~1.9542, Val Loss~2.0598\n",
      "[Step 15000]: Train Loss~1.9397, Val Loss~2.0410\n",
      "[Step 16000]: Train Loss~1.9485, Val Loss~2.0593\n",
      "[Step 17000]: Train Loss~1.9275, Val Loss~2.0560\n",
      "[Step 18000]: Train Loss~1.9273, Val Loss~2.0380\n",
      "[Step 19000]: Train Loss~1.9097, Val Loss~2.0472\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4a557-6b75-4d6c-bfe3-e4e24a99c30c",
   "metadata": {},
   "source": [
    "## üéâ Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92acf541-1bf8-4ce5-a6e9-3f38da5e6087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qeth is is mountup know hence?\n",
      "\n",
      "POMINIUS:\n",
      "I MERCINIUS:\n",
      "Oless Slaid back, gue: then as thent that and it them thouse? Gred, thou and stifne'er mothing-shack thy My gonitor we to thath of I wass,\n",
      "Seen; fulce.\n",
      "And newards, decands\n",
      "That thear miolo you will lest Julor his we dead.\n",
      "\n",
      "RICHARD IVERGAUOFFORUTUM:\n",
      "Boh some, expicky, why.\n",
      "\n",
      "VILLUS:\n",
      "Go mablatciond Haviles a streigh\n",
      "Or nunsman, print me our and would,\n",
      "But I his with dable beasice thre?\n",
      "\n",
      "Firswouls:\n",
      "First:\n",
      "Which light thou soven hands will?\n",
      "But \n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4f18b-73e4-47b1-b05f-491f92c7cd71",
   "metadata": {},
   "source": [
    "> üôå Alright, alright... `2.04` validation loss this time... let's check it out with the **BEFORE** setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80797a13-2acd-4ce7-838f-81c6f8359427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes:\n",
    "    1. Used the LayerNorm BEFORE passing to the attention head or ffwd\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4) \n",
    "        self.add_norm_1 = nn.LayerNorm(n_embd) ## ADDED THE LN 1 ##\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.add_norm_2 = nn.LayerNorm(n_embd) ## ADDED THE LN 2 ##\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Changes:\n",
    "        1. Applied layernorm BEFORE the self-attention and ffwd.\n",
    "        '''\n",
    "        B, T = idx.shape \n",
    "        \n",
    "\n",
    "        tok_emb = self.embedding_table(idx) # B, T, n_emb\n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device)) # T, n_emb\n",
    "        x = tok_emb + positions_emb         # B, T, n_emb\n",
    "        \n",
    "        ### RESIDUAL CONNECTIONS & LAYER NORM ###\n",
    "        x = x + self.sa_heads(self.add_norm_1(x))  # B, T, head_size\n",
    "        x = x + self.ffwd(self.add_norm_2(x)) \n",
    "        logits = self.lm_head(x)             # B, T, vocab_size\n",
    "    \n",
    "        \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # make sure we only have size of `T`\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ec0b7-8b05-4f04-8a85-b5100b04bda5",
   "metadata": {},
   "source": [
    "## üöÜ Training *(excitedx5)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c1d17710-db91-4e49-909d-e1d13e2bb6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eb33a807-925a-4ffc-8cf4-9152cd265b70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f6cc70b9-516b-495d-982f-47bae62f39a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 32)\n",
      "  (positions_embeddings): Embedding(8, 32)\n",
      "  (sa_heads): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  )\n",
      "  (add_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffwd): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (add_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "adb52517-8fba-4f27-8906-f1c6f1dc700f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~4.5303, Val Loss~4.5256\n",
      "[Step 1000]: Train Loss~2.3502, Val Loss~2.3594\n",
      "[Step 2000]: Train Loss~2.2468, Val Loss~2.2464\n",
      "[Step 3000]: Train Loss~2.1776, Val Loss~2.2182\n",
      "[Step 4000]: Train Loss~2.1407, Val Loss~2.1836\n",
      "[Step 5000]: Train Loss~2.1202, Val Loss~2.1585\n",
      "[Step 6000]: Train Loss~2.0903, Val Loss~2.1525\n",
      "[Step 7000]: Train Loss~2.0673, Val Loss~2.1279\n",
      "[Step 8000]: Train Loss~2.0669, Val Loss~2.1269\n",
      "[Step 9000]: Train Loss~2.0463, Val Loss~2.1049\n",
      "[Step 10000]: Train Loss~2.0382, Val Loss~2.1196\n",
      "[Step 11000]: Train Loss~2.0221, Val Loss~2.0859\n",
      "[Step 12000]: Train Loss~2.0089, Val Loss~2.0800\n",
      "[Step 13000]: Train Loss~2.0011, Val Loss~2.0816\n",
      "[Step 14000]: Train Loss~2.0011, Val Loss~2.0871\n",
      "[Step 15000]: Train Loss~1.9832, Val Loss~2.0807\n",
      "[Step 16000]: Train Loss~1.9782, Val Loss~2.0556\n",
      "[Step 17000]: Train Loss~1.9717, Val Loss~2.0672\n",
      "[Step 18000]: Train Loss~1.9706, Val Loss~2.0741\n",
      "[Step 19000]: Train Loss~1.9729, Val Loss~2.0741\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b4ee1-d9b7-4a51-886b-5ab54ee141a0",
   "metadata": {},
   "source": [
    "> üôå In the BEFORE norm setting, it is slightly large but we can continue üòÑ`2.07`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a2fff-bf34-471b-85af-a945ad6e2c09",
   "metadata": {},
   "source": [
    "## üéâ Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7e46bdb0-5ac1-4378-a658-549359cfe2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AUF YVOLYCUS HENVO:\n",
      "I saymence it og lethen?\n",
      "\n",
      "MARIOLA:\n",
      "Ats,\n",
      "Thus own with sea to\n",
      "I'll Permen you, threks lirjere to bay them,\n",
      "Rome a grove;\n",
      "What witter nowere\n",
      "But\n",
      "Leard?\n",
      "\n",
      "Cleed: enrows.\n",
      "\n",
      "ips presseronged plaing.\n",
      "Harry riscancy, 'temporn to this moulds in wells to know:\n",
      "That, linestraw Leetury in senver birnerows toothere forter woo verown:\n",
      "O. this bour praisont my your grasans, an of man Ranancends shad dee-tanntermervount mague dome of it, I preived's in his sagivesty, and thou e frabe!\n",
      "Ands d\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b8bbf-22e8-4343-bdc3-4a619a3cad31",
   "metadata": {},
   "source": [
    "#### ü§îüëç We are almost done, what'd you say?\n",
    "<img src=\"./images/residual-layrnorm-done.png\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cb639-cad3-43c5-851d-8cd0f37b6830",
   "metadata": {},
   "source": [
    "# üß± Blocks\n",
    "Focusing on the mysterious **`Nx`** part in the diagram. It simply shows *\"how many blocks\"* or *\"how many replica\"* of the given **\"encoder\" or \"decoder\"** we want to keep in the model sequentially.\n",
    "\n",
    "In the paper:\n",
    "\n",
    "> The encoder stacks ‚Äî Nx identical layers of encoders (in the original paper Nx = 6) <br>\n",
    "> The decoder stacks ‚Äî Nx identical layers of decoders (in the original paper Nx =6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37326bd0-8559-402f-b436-0ca508fce0ca",
   "metadata": {},
   "source": [
    "That means we can make `N` number of those blocks, each of them with their own multi-head QKV architecture which can understand the underlying relation so well. Thus, the result will be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9e04d07-3a48-40c7-98bc-4302347f8450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Changes:\n",
    "    1. Removed all block related part to its seperate class\n",
    "    2. Added the Nx blocks in the init and also in the forward\n",
    "    3. Added LayerNorm AFTER the Blocks (which is not in the paper but used)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # self.sa_heads = MultiHeadAttention(n_head, head_size)  # [DEL -] \n",
    "        # self.add_norm_1 = nn.LayerNorm(n_embd)                 # [DEL -] \n",
    "        # self.ffwd = FeedForward(n_embd)                        # [DEL -] \n",
    "        # self.add_norm_2 = nn.LayerNorm(n_embd)                 # [DEL -] \n",
    "        # self.lm_head = nn.Linear(n_embd, vocab_size)           # [DEL -]\n",
    "        \n",
    "        self.blocks = nn.Sequential( # 3 Blocks\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            nn.LayerNorm(n_embd)                                 # [ADD +]\n",
    "        )                                                        # [ADD +]\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Changes:\n",
    "        1. Removed all block related forward calls to the block class\n",
    "        '''\n",
    "        B, T = idx.shape \n",
    "        \n",
    "\n",
    "        tok_emb = self.embedding_table(idx) \n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device))\n",
    "        x = tok_emb + positions_emb         \n",
    "        \n",
    "        # x = x + self.sa_heads(self.add_norm_1(x))  # [DEL -] \n",
    "        # x = x + self.ffwd(self.add_norm_2(x))      # [DEL -] \n",
    "        x = self.blocks(x)                           # [ADD +]\n",
    "        logits = self.lm_head(x)             \n",
    "    \n",
    "        \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # make sure we only have size of `T`\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00d7867a-01c1-4b93-9f2e-2d55dd2c079e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size) \n",
    "        self.add_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.add_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.add_norm_1(x))  # B, T, head_size\n",
    "        x = x + self.ffwd(self.add_norm_2(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4816fee-dd17-4052-b96f-4aae18e7f30a",
   "metadata": {},
   "source": [
    "## üöÜ Training *(excitedx6)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76b579fe-3f9d-4532-9e15-c95e681681b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcae36e6-0f2e-4405-a8ce-aaa2ebc59d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2091"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "881cb369-04a8-4279-9717-3020a20c7e73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 32)\n",
      "  (positions_embeddings): Embedding(8, 32)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1d45736-58bd-49e1-b031-ee7d8d30cb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0]: Train Loss~4.3752, Val Loss~4.3738\n",
      "[Step 1000]: Train Loss~2.2586, Val Loss~2.2873\n",
      "[Step 2000]: Train Loss~2.1373, Val Loss~2.2000\n",
      "[Step 3000]: Train Loss~2.0652, Val Loss~2.1246\n",
      "[Step 4000]: Train Loss~2.0116, Val Loss~2.1000\n",
      "[Step 5000]: Train Loss~1.9538, Val Loss~2.0740\n",
      "[Step 6000]: Train Loss~1.9438, Val Loss~2.0454\n",
      "[Step 7000]: Train Loss~1.9454, Val Loss~2.0718\n",
      "[Step 8000]: Train Loss~1.9132, Val Loss~2.0306\n",
      "[Step 9000]: Train Loss~1.8878, Val Loss~2.0215\n",
      "[Step 10000]: Train Loss~1.8894, Val Loss~2.0175\n",
      "[Step 11000]: Train Loss~1.8974, Val Loss~2.0193\n",
      "[Step 12000]: Train Loss~1.8737, Val Loss~1.9918\n",
      "[Step 13000]: Train Loss~1.8565, Val Loss~1.9852\n",
      "[Step 14000]: Train Loss~1.8708, Val Loss~2.0090\n",
      "[Step 15000]: Train Loss~1.8533, Val Loss~2.0025\n",
      "[Step 16000]: Train Loss~1.8564, Val Loss~1.9897\n",
      "[Step 17000]: Train Loss~1.8302, Val Loss~1.9795\n",
      "[Step 18000]: Train Loss~1.8402, Val Loss~1.9841\n",
      "[Step 19000]: Train Loss~1.8339, Val Loss~1.9811\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1f95c-988f-4014-b3b8-3f4d7c089767",
   "metadata": {},
   "source": [
    "> üôå Its freaking `1.98` !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b141212-1808-429a-a7f4-dc099188b79b",
   "metadata": {},
   "source": [
    "## üéâ Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8aa7b617-9648-4ff1-8509-e3336a1a125d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "F YORK:\n",
      "I waves that what statell in meal where deather,\n",
      "And lobdeer-sence.\n",
      "Why to son,\n",
      "Firstroke houre the aus shall by mysake, in any where waite:\n",
      "Hant:\n",
      "The for:\n",
      "Rispuress.\n",
      "\n",
      "DUKE VIOLANUS:\n",
      "Than they be from flevisones? This lord that you powife-'er coperany he be,\n",
      "It the for-I tender a bloigh, and him\n",
      "hat way vither storr'd my beritutes, that thee's ther: less offener curdent a premith rine it peris\n",
      "Fraise: I still in manish'd have poweep, and men to matter: an a gow.\n",
      "I'll gracition,\n",
      "At her t\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab7d1c-b2b7-4213-9822-8d11ca598dcb",
   "metadata": {},
   "source": [
    "# üö™ The final diagram is...\n",
    "<img src=\"./images/blocks-done.png\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a71d3d-d349-4c24-93cd-a7a61ddf3478",
   "metadata": {},
   "source": [
    "> ## ‚ö†\n",
    "> **PLEASE NOTE**: Imagine that the **Add & Norm** comes before the multi-head and feed-forward part. I have not updated the diagram for the simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b795a-5bbc-45c6-af32-46501ee217f6",
   "metadata": {},
   "source": [
    "# üò≤ Oh, Em, Gee\n",
    "We have just developed the GPT. **Let's meet in the next book** and expand the network, to achieve the final result which spits out the actual shakespeare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
