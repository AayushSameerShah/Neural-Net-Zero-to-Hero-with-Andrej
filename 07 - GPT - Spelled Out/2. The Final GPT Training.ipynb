{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd433c4-9aba-4c0e-8f9a-68333edb3ce3",
   "metadata": {},
   "source": [
    "# ✍ Shakespeare clone's getting ready\n",
    "So far the journey has been spectacular. We have finally come to the *\"felina\"* of breaking things down to its atomic level. **In** this notebook we will use the knowledge and the code from the previous book to expand the GPT structure and train it so that it can give us better completions.\n",
    "\n",
    "**Let's get it done.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6be0dc-1213-47c4-a433-f05b657dc70d",
   "metadata": {},
   "source": [
    "# 0️⃣ Get the imports done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a511a7d-4301-4c51-a2a3-d51b58b7ee56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f757e321b90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # for layers and stuff\n",
    "from torch.nn import functional as F # for the loss function and softmax\n",
    "torch.manual_seed(1337) # same as in the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489368a-2fbe-468d-84d3-e55ee98fd3bf",
   "metadata": {},
   "source": [
    "# 1️⃣ Read and prepare the dataset\n",
    "Please note that, since we have explored the data and how the preparation of the data behaves internally in the previous book, I would like the code in this book to be straightforward and less verbose, just comments ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f8eff-452a-45c9-9e4f-e6551ac8255d",
   "metadata": {},
   "source": [
    "### `1.1` Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb0350d-882a-47ce-b2c2-eb3467926948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1115394 tokens in total.\n",
      "The total possible tokens are 65: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# reading the file\n",
    "with open('./input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(f\"The dataset has {len(text)} tokens in total.\")\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"The total possible tokens are {vocab_size}: {chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3add03d-8fbe-4d5c-b72c-4b265cbd5a7c",
   "metadata": {},
   "source": [
    "### `1.2` Encoder decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0a0f7b-c602-431d-bfa1-ab70e760a248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the tokenizer\n",
    "char_to_number = {ch:i for i,ch in enumerate(chars)}\n",
    "number_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# encoder: take a string, output a list of integers\n",
    "encode = lambda s: [char_to_number[c] for c in s]\n",
    "\n",
    "# decoder: take a list of integers, output a string\n",
    "decode = lambda l: ''.join([number_to_char[i] for i in l]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb3cbe-addb-45de-ba22-712122b67f6e",
   "metadata": {},
   "source": [
    "### `1.3` Train-Test preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b72093-670e-4339-b9a0-21cb9052857c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde16ff8-01c0-4e12-b5dc-753bbca325fb",
   "metadata": {},
   "source": [
    "# 2️⃣ Build the lego pieces of the GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72558118-5f53-498a-b940-bc5a07dd3638",
   "metadata": {},
   "source": [
    "### `2.1` The main model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc185c6-2f89-4a8f-b3e8-da30e02ebfe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    The Final BigramLM model which had and does the following:\n",
    "    \n",
    "    ## Has:\n",
    "    1. Token embedding layer\n",
    "    2. Position embedding layer\n",
    "    3. Nx Blocks which has multihead attentions and feed-forward\n",
    "    4. Finally the LM-head\n",
    "    5. The shapes written in comments\n",
    "    \n",
    "    ## Does:\n",
    "    1. Takes the input which will be in the B, T format\n",
    "    2. Converts them into B, T, C (starting with the Token embedding layer)\n",
    "    3. The rest is the history... you really want me to talk much!? \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)       \n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential( \n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)          \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape \n",
    "        tok_emb = self.embedding_table(idx) \n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device))\n",
    "        x = tok_emb + positions_emb         \n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)             \n",
    "    \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)        \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28628123-4d4a-4295-8809-05a89e9141d9",
   "metadata": {},
   "source": [
    "### `2.2` Block class (decoder body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fb6403-1f4f-4fa8-8610-012d6bbca166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    The block basically is the collection of self attention layers (multi) and \n",
    "    the feed forward layers with residual connections and the layer norm layers.\n",
    "    \n",
    "    All we want to do is to isolate them so that we can make as many as we want\n",
    "    and get better results!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size) \n",
    "        self.add_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.add_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.add_norm_1(x))  # B, T, head_size\n",
    "        x = x + self.ffwd(self.add_norm_2(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73507f-de31-4925-b94a-bb1e371a1140",
   "metadata": {},
   "source": [
    "### `2.3` Decoder's heart: Multihead-Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a37bd3-9bf4-4f06-ae2a-fe321bb6e470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout) ###\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out)) ###\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e773733-a602-4a0a-b5d7-a6e523094df3",
   "metadata": {},
   "source": [
    "### `2.4` MultiHead requires the `Head` to work with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ed6c1d-38ad-4e03-a05b-dd1480bfe46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class will simply create the Q, K, V vectors\n",
    "    and also the reguster_buffer to create the mask.\n",
    "    \n",
    "    Then on the `forward` it will pass the vectors in the \n",
    "    Q, K, V and give the `out`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Take the `x` input which will be the positions.\n",
    "        The shape will be B, T, C meaning:\n",
    "        \"For each batch, there will be T tokens which will have positions encoded in C\n",
    "        space\"\n",
    "        \n",
    "        We will use that and work oursalves forward.\n",
    "        '''\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # the C**-0.5 is used to control the variance\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # the mask\n",
    "        wei = F.softmax(wei, dim=-1) # the final wei\n",
    "        \n",
    "        wei = self.dropout(wei) ###\n",
    "        out = wei @ v # this is what we will use further\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8250ac-f2fe-4d48-bb67-4dcd324e5548",
   "metadata": {},
   "source": [
    "### `2.5` Decoder some time to pause, and think: FFWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb1f9cc-96b8-43da-8546-59637b32f056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout) ###\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f4a7d-bac2-4217-90bc-d7772b7fb3bd",
   "metadata": {},
   "source": [
    "It looks like we are pretty much done here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e29837-b396-40e4-80c9-4e1bff3b1a0d",
   "metadata": {},
   "source": [
    "# 3️⃣ Training helping hands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c5c88-e404-4d7d-ae09-d5342ecda056",
   "metadata": {},
   "source": [
    "### `3.1` Get batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d2c979-dd79-492b-8ecb-7e481adf3b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821d719-1263-45a4-8a49-4d1c56078230",
   "metadata": {},
   "source": [
    "### `3.2` To estimate loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b126965-2628-4792-82e9-8871a5e77f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    '''\n",
    "    This function takes the random samples from the dataset (based on the batch size)\n",
    "    for `eval_iter` times. Records loss and takes the mean loss. And reports back.\n",
    "    \n",
    "    Which means, if we have the `eval_iter = 10` and `batch_size=32` then it will take \n",
    "    32 random samples from training data and then validation data for 10 times and takes\n",
    "    the means of these 10 losses.\n",
    "    '''\n",
    "    out = {}\n",
    "    \n",
    "    # 🔥 sets on evaluation mode... 🔥\n",
    "    # which does something like `training_mode = False` \n",
    "    # in the layers like `BatchNorm`.\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    # 🔥 sets the model to training mode back!!! 🔥\n",
    "    # which does something like `training_mode = True` \n",
    "    # in the layers like `BatchNorm`.\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526db7bf-9ceb-452f-995b-31ff23544c19",
   "metadata": {},
   "source": [
    "# 4️⃣ Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10eee58-fd78-41ab-ade0-e4a7c760591a",
   "metadata": {},
   "source": [
    "### `4.1` Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459b0705-342f-46c5-b9e7-4ec92c96e6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64      # samples we will use for the single forward pass\n",
    "block_size = 256     # the context window (significantly bigger than our toy examples)\n",
    "max_iters = 5000     # total forward-backward passes\n",
    "\n",
    "eval_interval = 500  # after how many steps we want to print the loss?\n",
    "learning_rate = 3e-4 # learning rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "eval_iters = 200    # when printing the loss, how many samples to consider for validation?\n",
    "n_embd = 384        # embedding size of each token\n",
    "n_head = 6          # `n` multi heads for the self-attention\n",
    "n_layers = 6        # `n` for `Nx` which shows how many blocks to use\n",
    "dropout = 0.2       # randomly drop % percentage of waights from getting trained for that single pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec7968-fbd6-4a12-a3f3-9e92db1383c9",
   "metadata": {},
   "source": [
    "### `4.2` Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cadbd12d-757a-474c-950c-2fd958474bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLM()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcc933ac-d14f-4abb-9ee0-479561535289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44995"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters now\n",
    "sum(len(i) for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c511879c-ba41-4e6a-9f5d-71ac2184b094",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (embedding_table): Embedding(65, 384)\n",
      "  (positions_embeddings): Embedding(256, 384)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (add_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (add_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf409e72-08e1-46e4-928d-aab897e73597",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}]: Train Loss~{losses['train']:.4f}, Val Loss~{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d9eb0-cc74-468d-8154-66f0fe0f45b2",
   "metadata": {},
   "source": [
    "___\n",
    "> 😅 <br>Well, I actually let it ran for about an hour on Tesla T4 and it only completed about 1600 steps (epochs)... So I stopped it and saved that checkpoint and have just loaded it below. The val loss was around `1.6`.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "151a1a0e-2a9d-44ad-bb93-aa2a6ed134a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = BigramLM()\n",
    "model.load_state_dict(torch.load(\"./ShakeGPT.zip\", map_location=torch.device('cpu')))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27aaacf9-aab4-4dde-91b5-5b670297923a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Or didrections: brone the twicers dies thy countrich\n",
      "before king, through on us; unvised a tidity\n",
      "werecren follow surers, stare thou daughteling thy head\n",
      "Is choldly lords: then and I not, forge\n",
      "Clizens: the hasts for first, sindee heir?\n",
      "Volsch:\n",
      "But myself to love, you would say do now? and 'tis no soon?\n",
      "\n",
      "GLOUCES:\n",
      "Have my chorself i' the muntil' to tear?\n",
      "\n",
      "PERCLIFFOLIA:\n",
      "Marry, again!\n",
      "\n",
      "AUTICIO:\n",
      "Not made, we we'gce tonder Jo?\n",
      "\n",
      "AUTIO:\n",
      "Metake has is weepecut this crown words?\n",
      "Nence betted the slays, b\n"
     ]
    }
   ],
   "source": [
    "output = decode(\n",
    "    model.generate(\n",
    "        idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),  ### 🗽 Transfer to device 🗽 ###\n",
    "        max_new_tokens=500)[0].tolist()\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bfdc8-1d4c-4f36-b197-e64523cfe295",
   "metadata": {},
   "source": [
    "Yo! This is really good result. But I would say, it is basically \"writing the right\" words but the context is not known. What it wrote previously is like not known to the model. It just spits out the words, you can't get the proper story.\n",
    "\n",
    "But **dude it's our GPT!** finally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c995db-4494-4797-bc9e-ef2cb0a409b6",
   "metadata": {},
   "source": [
    "# ⏸ The journey takes a pause\n",
    "Since we are finally at the point where have create GPT from scratch, actually spelled out... but there's more to it as it gets addded in the Andrej's course.\n",
    "\n",
    "But let's quickly revise our journey *(memories!)*:\n",
    "1. Started with the **Micrograd**: We saw how internally each expression can be formed and can learn with the backprop\n",
    "2. We built a simple Bigram model with just **2** tokens, and by building the table manually we formed our **base for the embeddings**.\n",
    "3. Build the Bigram with a simple neural net which got us the understanding of how the model learns the relationships which we laied out manually in the previous model.\n",
    "4. Tried to diaognise the model, went through each layers of the model, fixed distributions and plotted many charts for the weights!\n",
    "5. Increased **confidence** with the backpropogating through a whole freaking model, each layer, with hands, manually. That was some serious stuff there.\n",
    "6. Finally made a GPT. Not just made, we developed the ground understanding for that first.\n",
    "\n",
    "That sounds a lot! But if you have followed the course from the start, then **man! pat yourself** it really is a big stepping stone for some thing bigger in the future. I promise.\n",
    "\n",
    "The jorney may well be continued with other types of models in this course as Andrej adds in the series. <br>\n",
    "But till then, keep GPTing!\n",
    "\n",
    "\n",
    "✍ **Aayush Shah**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
