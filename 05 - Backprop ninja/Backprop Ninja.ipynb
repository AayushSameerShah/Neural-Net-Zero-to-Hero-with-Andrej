{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "# ü•∑ Makemore: becoming a backprop ninja\n",
    "\n",
    "Hey there! üôã‚Äç‚ôÇÔ∏è<br>\n",
    "Here again we are. This time with **a serious challange**. We will get inside the backprop and will **learn by doing** it. \n",
    "\n",
    "**Some things to consider**:\n",
    "- Here I will use Karpathy's code, but we will fill the code by our own *(of course)*\n",
    "- There will be other explanations in the book to understand the backprop\n",
    "\n",
    "Let's get started, looks promising ü§û"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Since our makemore was *supposed to* handle total `28` different characters `A-Z`, `<`, `>`; and Karpathy's version supports total `27`, we will not change it and will **keep** it to `27`.  Because the main goal of this notebook is to ***understand*** the internals of backprop and not some tweak in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/hero.png\" height=500 width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `1.` Loading & Creating the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.1` Creating the character mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "### Build the vocabulary of characters and mappings to/from integers\n",
    "\n",
    "# Total posible characters\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# 'a' -> 1\n",
    "string_to_int = {s:i+1 for i,s in enumerate(chars)}\n",
    "\n",
    "# add special character at 0\n",
    "string_to_int['.'] = 0\n",
    "\n",
    "# 1 -> 'a'\n",
    "int_to_string = {i:s for s,i in string_to_int.items()}\n",
    "\n",
    "vocab_size = len(int_to_string)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.2` Creating the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V_zt2QHr8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "### Build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = string_to_int[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.3` Utility function to check the fail or pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MJPU8HT08PPu"
   },
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    '''\n",
    "    s: Name of the process\n",
    "    dt: The grad calculated by us\n",
    "    t: The torch's grad\n",
    "    \n",
    "    This function is responsible to check whether:\n",
    "    1. ALL grad of \"ours\" and torch's EXACTLY the same?\n",
    "    2. Are they \"close\" enough to be called same?\n",
    "    3. Check the maximum difference (simply our - torch)\n",
    "    '''\n",
    "    \n",
    "    ex = torch.all(dt == t.grad).item()        # 1.\n",
    "    app = torch.allclose(dt, t.grad)           # 2.\n",
    "    maxdiff = (dt - t.grad).abs().max().item() # 3.\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `2.` Creating the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ The code is the same from the previous notebooks **but** it will change in the calculation **in the last layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)       # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd), generator=g) # embeddings\n",
    "\n",
    "### Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), \n",
    "                 generator=g) * (5/3)/((n_embd * block_size)**0.5) # recall gain / sqrt(fan_in)\n",
    "b1 = torch.randn(n_hidden, \n",
    "                 generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "\n",
    "\n",
    "### Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), \n",
    "                 generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,\n",
    "                 generator=g) * 0.1\n",
    "\n",
    "### BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† We need to \"warp our head\" around the NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/NN_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ñ∂ Here we **just have** the nodes and \"weights\"... next up we will **add the biases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/NN_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ñ∂ And finally we will do the **scaling** and **shifting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/NN_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ñ∂ Alright, now we know what is happening in the net, *at least visually*... we can have a bit better confidence moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `3.` Prepare training variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `3.1` Prepare training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3.2` Ignite the forward pass\n",
    "\n",
    "This is the üßä **coolest** part of this notebook. \n",
    "Here we will *break* the steps down in the **manageable** chunks so that we can perform the backward pass.\n",
    "\n",
    "I will *introduce* each part slowly to digest it properly. Let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Have a look at the `original` code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "'''\n",
    "The following code is what we have used in the last notebook.\n",
    "Should not make you freak out ;)\n",
    "'''\n",
    "losses = []\n",
    "batch_size = 32 \n",
    "epochs = 20_000\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Data Sampling (FROM XTRAIN ONLY)\n",
    "    sample_idx = torch.randint(0, Xtrain.shape[0], (batch_size,), generator=generator)\n",
    "    \n",
    "    # 1Ô∏è‚É£ Forward pass\n",
    "    emb = embeddings[Xtrain[sample_idx]] \n",
    "    preact = emb.view(-1, block_size * emb_dim) @ W1 + b1 # seperated\n",
    "    preact_mean = preact.mean(0, keepdim=True)   # preact mean\n",
    "    preact_std = preact.std(0, keepdim=True)     # preact std\n",
    "    preact = (preact - preact_mean) / preact_std # standardzation\n",
    "    preact = (scaler * preact) + shifter         # scaling and shifting\n",
    "    \n",
    "    h = torch.tanh(preact)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = cross_entropy(logits, ytrain[sample_idx]) \n",
    "    \n",
    "    # 2Ô∏è‚É£ Backward\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # 3Ô∏è‚É£ Update - with decay\n",
    "    learning_rate = 0.1 if i < 10_000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "    losses.append(loss.item()) # for better visualization\n",
    "    \n",
    "plt.plot(losses, lw=0.1)\n",
    "plt.title(\"Training log-loss\");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ New `expanded` code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2.1` Transform the sampled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus now we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32 samples, each having 30 inputs (x1 to x30... keep the network in mind)\n",
    "embcat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2.2` Perform the matmul\n",
    "> Now we are in the 1Ô∏è‚É£ code block of the \"old\" code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2.3` Perform the batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òë Find the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BatchNorm layer ###\n",
    "\n",
    "# üëâ Find the mean\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# The above is equivalent to: `hprebn.sum(0) / n` ie. simply the mean :)\n",
    "\n",
    "# üëâ Find the difference\n",
    "bndiff = hprebn - bnmeani\n",
    "\n",
    "# üëâ Squaring the difference to find variance below\n",
    "bndiff2 = bndiff**2\n",
    "\n",
    "# üëâ Finding the variance\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# The above is equivalent to: `bndiff2.sum(0, keepdim=0) / (n-1)` ie. simply the variance :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òë Perform the standardization and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëâ Inverse the variance so that it can be \"multiplied\"\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "## Note: 1e-5 is a small constant to avoid division by zero\n",
    "\n",
    "# üëâ Finally standardize the numbers\n",
    "bnraw = bndiff * bnvar_inv\n",
    "\n",
    "# üëâ Give the spice to it by gain and bias\n",
    "hpreact = bngain * bnraw + bnbias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Yo! The batchnorm is done! **How simple that was!** <br>\n",
    "Before we did that by:\n",
    "```python\n",
    "preact_mean = preact.mean(0, keepdim=True)   # preact mean\n",
    "preact_std = preact.std(0, keepdim=True)     # preact std\n",
    "preact = (preact - preact_mean) / preact_std # standardzation\n",
    "preact = (scaler * preact) + shifter         # scaling and shifting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2.4` Apply an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2.5` Matmul for the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2.6` The cross entropy loss!\n",
    "> We are back with the softmax!\n",
    "\n",
    "***(Want some refresher? Go to `03 - Makemore MLP/Make more with MLP.ipynb` and scroll to topic: \"üî• Boom Boom üî•\")***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "\n",
    "# üëâ Find the largest logit for each 32 samples\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "## Now shape is (32, 1)\n",
    "\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above **we could have skipped the step** of finding the max logit and subtracting it from all logits. **We did this** to avoid numerical overflow. I think we have done/discussed this somewhere before but I can't recall where. \n",
    "\n",
    "> üí° For now, just understand that \"*subtracting the max logit from all logits will make the max logit `0` and others negative which will help `.exp()` not to introduce very large numbers*\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have the counts, we now can have the probability\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "\n",
    "# üëâ Dividing the sum of counts for each count\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "\n",
    "# üëâ But not a direct division, we will use inversion to match the grads better\n",
    "probs = counts * counts_sum_inv\n",
    "\n",
    "# üëâ Finally converting the probs back to log \n",
    "logprobs = probs.log()\n",
    "\n",
    "# üëâ Findinf the loss by picking up the logs for each target character\n",
    "loss = -logprobs[range(n), Yb].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3426, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plain loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3.3` Perform the backward pass! üî•\n",
    "> We are now in the 2Ô∏è‚É£ code block in the our training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3426, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "          embcat, emb]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `4.` Backward by us üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section header:\n",
    "\n",
    "    # Exercise 1: backprop through the whole thing manually,\n",
    "    backpropagating through exactly all of the variables\n",
    "    as they are defined in the forward pass above, one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.1` Derivative of `logprobs` wrt `loss`\n",
    "\n",
    "> üí≠ Which *\"each of the values of `logprobs` impacting the `loss`?\"*\n",
    "\n",
    "We can see that the `loss` is simply the **mean** of the \"plucked out\" logs of the next token. Thus here only `32` tokens participate in calculating the loss.\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "```\n",
    "\n",
    "Which goes through the following process: \n",
    "- **Addition** of all logs (0 to 32)\n",
    "- **Division** by 32\n",
    "\n",
    "Thus, total `2` operations are being carried out. If you recall from the *micrograd* we were finding the gradient like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.33333333333551707"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function which resambles the operation\n",
    "def f(a, b, c):\n",
    "    '''here taking only 3 numbers instead of 32'''\n",
    "    return -(a+b+c)/3\n",
    "\n",
    "# inputs\n",
    "a = 2.0\n",
    "b = 3.0\n",
    "c = 10.0\n",
    "# a small change\n",
    "h_ = 0.0001\n",
    "\n",
    "old = f(a, b, c)\n",
    "a += h_\n",
    "new = f(a, b, c)\n",
    "\n",
    "# slope\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is around `-1/3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20000000001019203"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a, b, c, d, e):\n",
    "    '''here taking only 5 numbers instead of 32'''\n",
    "    return -(a+b+c+d+e)/5\n",
    "\n",
    "# inputs\n",
    "a, b, c, d, e = 2.0, 3.0, 10.0, 55.0, 0.44\n",
    "old = f(a, b, c, d, e)\n",
    "a += h_\n",
    "new = f(a, b, c, d, e)\n",
    "\n",
    "# slope\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is around `-1/5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí≠ **Which means** the slope is `-1/n`. And for our case the slope will be `-1/32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö† **REMEMBER**: We are only dealing with `1` logit from each `32` samples. Thus **DON'T MAKE** a mistake to assign the derivative of only selected logits to all other logits. Because currently in this state all other logits don't have ANY impact in the loss calculation and thus their grad will be `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mO-8aqxK8PPw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We will use the convention `d__` for the variables which are calculateb by us.\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs.shape # the structure of probs then will be filled out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the slope / grad calculation\n",
    "d = -1/n\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign in the structure!\n",
    "dlogprobs[range(n), Yb] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check-1 ‚úÖ\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.2` Derivative of `probs` wrt `logprobs`\n",
    "\n",
    "> üí≠ Which is *\"how performing `.log()` on `probs` impacted the `logprobs`?\"*\n",
    "\n",
    "Working for this line: \n",
    "```python\n",
    "logprobs = probs.log()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999500033329731, 0.49998750041746476, 0.3333277779016264)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function which resambles the operation\n",
    "def f(a):\n",
    "    '''here taking only 1 number because it works elementwise'''\n",
    "    return np.log(a)\n",
    "\n",
    "# inputs\n",
    "a = 1.0\n",
    "b = 2.0\n",
    "c = 3.0\n",
    "\n",
    "old = f(a)\n",
    "a += h_\n",
    "new = f(a)\n",
    "d_a = (new - old) / h_\n",
    "\n",
    "old = f(b)\n",
    "b += h_\n",
    "new = f(b)\n",
    "d_b = (new - old) / h_\n",
    "\n",
    "old = f(c)\n",
    "c += h_\n",
    "new = f(c)\n",
    "d_c = (new - old) / h_\n",
    "\n",
    "(d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Oops! **Our this trick** only works for the simple addition, multiplication and subtraction. Here with log() which is non linear, we need to rely on something more to understand the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching on the internet gives $\\frac{1}{x}$ as the derivative of `log`. <br>Thus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999000099990001, 0.49997500124993743, 0.3333222225925802)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/a, 1/b, 1/c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are correct!\n",
    "\n",
    "‚ö† **REMEMBER**: Here  the ***\"chain rule\"*** will be applied! And also *we are safe* to use all elements because the `log` is applied on each `32 * 27` elements unlike only 32 elements in the exercise above üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        the 1/x       chain rule (previous grads)\n",
    "dprobs = (1 / probs) * dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check-2 ‚úÖ\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight üß†\n",
    "Andrej really has shared a beautiful insight at this point. See, `grads` show \"how much impact\" does the current element or node has on the loss *(because of the chain rule)*. And here we can see the calculation is: `(1 / probs) * dlogprobs`.\n",
    "\n",
    "That means, if the character that has been picked up correctly, then **it will have the probability of 1.0** or near. That way the **grads will pass through** . But if the character which is picked is **wrong** the, it will have the probability low and hence (1 / probs) will introduce a bigger number and thus the gradients will be boosted!\n",
    "\n",
    "Thus next time those values will have higher impact on loss and then the subsequent iterations will take care of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.3` Derivative of `counts` wrt `probs`\n",
    "\n",
    "> üí≠ Which is *\"what is effect of `counts` and `counts_sum_inv` on  `probs`?\"*\n",
    "\n",
    "Working for this line: \n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "Of course, here we are finding the `probs` by multiplying **and not** by dividing. And here we will need to find **both of their** gradients individually as they are both combined operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí≠ Okay that means: \"*for each 32 samples having 27 counts, we are multiplying with inverse sum of all 32 samples to get prob of each*.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the `*` sign!! \n",
    "\n",
    "Yo? üòä  <br>\n",
    "-- or -- <br>\n",
    "No Yo? üôÅ \n",
    "\n",
    "**I think Yo!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.000000000010772"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 2.0, 3.0\n",
    "old = f(a, b)\n",
    "a += h_\n",
    "new = f(a, b)\n",
    "\n",
    "# slope\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0000000000042206"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 2.0, 3.0\n",
    "old = f(a, b)\n",
    "b += h_\n",
    "new = f(a, b)\n",
    "\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same way `probs = counts * counts_sum_inv` are related. Thus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_sum_inv   | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "### the line below uses .sum() function. Please view the \"explainer\" below\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\n",
    "\n",
    "# Check-3 ‚úÖ\n",
    "cmp('count_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üîé Explainer of what just happened.\n",
    "Well, Andrej explains much clearly at this point, worth checking that one out [by this clip](https://youtube.com/clip/Ugkx_b4RbW-o9WG7uBN-MsMQK1v0G2pn2gfI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/broadcast.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üôÑ But, \n",
    "when we perform the `counts * dprobs` to get the derivative of the `count_sum_inv` we basically are multiplying:\n",
    "\n",
    "- counts (32 x 27)\n",
    "- dprobs (32 x 27)\n",
    "\n",
    "Thus the result will be `32 x 27` and not the `32, 1` which is the shape of `count_sum_inv`.\n",
    "\n",
    "### ü§î What to do now?\n",
    "If you recall... in the `01 - Micrograd/Micrograd Foundations.ipynb` book, we had one section **\"üêû Bug - 001\"**. When having a single element which gets gradients from multiple other calculations... we used `+=` sign to solve that issue.\n",
    "\n",
    "> ***Here we will do the same***. Just sum things up. And hence you saw \"`(counts * dprobs).sum(1, keepdims=True)`\". \n",
    "\n",
    "Hope that's clear ü§û"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.4` Derivative of `counts` wrt `probs`\n",
    "\n",
    "Still, working for this line:\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "But now, it is the turn for the `counts` derivatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: False | approximate: False | maxdiff: 0.006148857995867729\n"
     ]
    }
   ],
   "source": [
    "# Check-4 ‚ùå\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§Ø **Oops!** <br> Looks like a \"small\" max diff... but still it is problematic. But what is this?\n",
    "\n",
    "**Have a look at the `lifeline of counts`...**\n",
    "```python\n",
    "counts = norm_logits.exp()\n",
    "\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "\n",
    "# USAGE - 1\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "\n",
    "# USAGE - 2\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "Unlike `count_sum_inv` which is used in only single place in the calculating `probs`, `count` is used in **2** places. Thus, it's derivative has to `2` branches. <br>\n",
    "**Which looks like:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/counts_branch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üê∏ We can clearly see that the `counts` is associated with more than 1 operations, thus it gradients should not be solely calculated based on the single operation like we did above.\n",
    "\n",
    "### ü§î What to do then?\n",
    "Here is the **exact** problem that we have to refer in **üêû Bug - 001**. Here we will \"sum\" all the effects of count which come from:\n",
    "1. `counts_sum`\n",
    "2.`probs` *(which we just saw above and failed)*\n",
    "\n",
    "So we will calculate for the `counts_sum` first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.5` Derivative of `counts_sum` wrt `count_sum_inv`\n",
    "\n",
    "> üí≠ How `counts_sum_inv` is affected by the `counts_sum` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "counts_sum_inv = counts_sum**-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2499875006256591"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    return a**-1\n",
    "\n",
    "a  = 2.0\n",
    "old = f(a)\n",
    "a += h_\n",
    "new = f(a)\n",
    "\n",
    "# slope\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009999900000973172"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b  = 10.0\n",
    "old = f(b)\n",
    "b += h_\n",
    "new = f(b)\n",
    "\n",
    "# slope\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which translates to $-\\frac{1}{x^2}$ <br>Thus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.24997500187487495, -0.009999800002999961)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This confirms that we are correct\n",
    "-(1/a**2), -(1/b**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum = -(1 / counts_sum**2) * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check-5 ‚úÖ\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now we are ready to find for `count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.4` Derivative of `counts` wrt `probs` (again)\n",
    "\n",
    "Still, working for this line:\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "But now we have enough data to calculate the gradient for the `counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which is simply...\n",
    "\n",
    "# USAGE-1\n",
    "dcounts = 1.0 * dcounts_sum\n",
    "\n",
    "# USAGE - 2 (NOTE: We used += below but explicitely)\n",
    "dcounts = dcounts + (counts_sum_inv * dprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check-4 ‚úÖ (finally)\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.6` Derivative of `norm_logits` wrt `counts`\n",
    "\n",
    "> üí≠ How `norm_logits` will affect the `counts` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "counts = norm_logits.exp()\n",
    "```\n",
    "This again is the elementwise operation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.389425564063856"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    return np.exp(a)\n",
    "\n",
    "a  = 2.0\n",
    "old = f(a)\n",
    "a += h_\n",
    "new = f(a)\n",
    "\n",
    "# slope\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22027.567154727876"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b  = 10.0\n",
    "old = f(b)\n",
    "b += h_\n",
    "new = f(b)\n",
    "\n",
    "# slope\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which translates to $e^x$  itself!<br>\n",
    "\n",
    "> üí≠ **That means** derivative of any element exponentiated, will have the derivative of the same magnitude. \n",
    "\n",
    "And also we will need to use the chain rule here. A small code snippet from **micrograd**:\n",
    "```python\n",
    "def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(np.exp(x), (self, ), 'exp')\n",
    "        \n",
    "    def _backward():\n",
    "        self.grad += out.data * out.grad # chain rule\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "```\n",
    "Here we used `self.grad += out.data * out.grad` which confirms what I said above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "\n",
    "# Check-6 ‚úÖ\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4.7` Derivative of `log_maxes` wrt `norm_logits`\n",
    "\n",
    "> üí≠ How `logit_maxes` will affect the `norm_logits` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "```\n",
    "\n",
    "Here again, we have *2* variables participating in the calculation of the `norm_logits`. Here, **luckily** we are ***experienced*** enough to know beforehand that the second variable `logits` will also be participating in 2 operations like `counts` did. \n",
    "\n",
    "But for now, `logit_maxes` will have an easy time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# `-` has the derivative of -1.0\n",
    "dlogit_maxes = (-1.0 * dnorm_logits).sum(1, keepdims=True)\n",
    "\n",
    "# Check-7 ‚úÖ\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ùì Why `.sum`? Please revisit `4.3` üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `logit`'s life\n",
    "```python\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "# USAGE - 1\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "\n",
    "# USAGE - 2\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is...\n",
    "\n",
    "<img src=\"./images/logits_branch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.8` Derivative of `logits` wrt `logit_maxes`\n",
    "\n",
    "> üí≠ How `logits` will affect the `logit_maxes` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "```\n",
    "\n",
    "Alright, so first of all we will calculate the `logits'` derivative for `logit_maxes` and then we will find another for `norm_logits` and will sum them all up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† An insight\n",
    "Before continue though, we **need to keep in mind** that *in calculation of the `norm_logits` we are **only using** the max values per sample to normalize them*. \n",
    "\n",
    "Thus, the gradient of `logits` in this phase **will only be counted** towards those values which were max and were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üôÑ But... what will be their gradient?\n",
    "See, we are **only** picking out the elements, and **picking out** is **not** an operation so it doesn't have any *fancy* derivation. But still since we are picking and using them, they will get the `1` as the gradient and those which weren't used will get `0` gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 2],\n",
       "        [19],\n",
       "        [15],\n",
       "        [15],\n",
       "        [25],\n",
       "        [16],\n",
       "        [ 3],\n",
       "        [19],\n",
       "        [ 8],\n",
       "        [15],\n",
       "        [ 3],\n",
       "        [22],\n",
       "        [18],\n",
       "        [ 8],\n",
       "        [ 5],\n",
       "        [ 2],\n",
       "        [ 1],\n",
       "        [22],\n",
       "        [ 6],\n",
       "        [10],\n",
       "        [19],\n",
       "        [22],\n",
       "        [22],\n",
       "        [23],\n",
       "        [ 5],\n",
       "        [22],\n",
       "        [20],\n",
       "        [24],\n",
       "        [ 8],\n",
       "        [24],\n",
       "        [13]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall these values had the indices\n",
    "logits.max(1, keepdims=True).indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will be used to locate the elements in the `logits`. Thus, we will do something like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am following the old-school way :)\n",
    "dlogits = torch.zeros_like(logits)\n",
    "dlogits[range(logits.shape[0]), logits.max(1).indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dlogits);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### üö® THIS IS AN IMPORTANT STEP OTHERWISE THERE WILL BE A SLIGHT DIFFERENCE üö® ###\n",
    "\n",
    "# because the gradients have to flow through\n",
    "dlogits = (dlogits * dlogit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.9` Derivative of `logits` wrt `norm_logits`\n",
    "\n",
    "> üí≠ How `logits` will affect the `norm_logits` variable?\n",
    "\n",
    "Again, Working for this line:\n",
    "```python\n",
    "norm_logits = logits - logit_maxes\n",
    "```\n",
    "\n",
    "Here we will find the **second part** of the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is...\n",
    "dlogits = dlogits + (1.0 * dnorm_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check-8 ‚úÖ\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§© The crazy part of the backprop\n",
    "Now we will be backproping through the `matmul` layer! Holy! \n",
    "\n",
    "Let's recap a little bit:\n",
    "- We have been backing ourselves out from `loss`\n",
    "- `loss` ‚Üê `logprobs` ‚Üê `probs` ‚Üê `counts` ‚Üê `norm_logits` ‚Üê `logits`\n",
    "\n",
    "That's our **story till now**, with a little bit of extra things here and there, but now this is the *boss*! Let's tackle it now. <br>\n",
    "Have a look at the following code as a refresher:\n",
    "```python\n",
    "# Layer 1\n",
    "W2 = torch.randn((n_hidden, vocab_size),  generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "### ...omitted code...\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "### Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.10` Derivative of `h` wrt `logits`\n",
    "\n",
    "> üí≠ How  our matrix multiplication `h` will affect the `logits` variable?\n",
    "\n",
    "Again, Working for this line:\n",
    "```python\n",
    "logits = h @ W2 + b2 # output layer\n",
    "```\n",
    "\n",
    "Yo! This is getting crazy at this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/h_layer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See, here `h` is first matmuled and *then* whole calculation is added.\n",
    "\n",
    "And inside..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a simple function to help us out here as well\n",
    "# taking a 2x2, 2x2 metrics and doing the matmul manually\n",
    "\n",
    "def f(a, b):\n",
    "    \"\"\"\n",
    "    Internally the numpy `@` will perform the following:\n",
    "    d1 = (x1*w1 + x2*w3)\n",
    "    d2 = (x1*w2 + x2*w4)\n",
    "    d3 = (x3*w1 + x4*w3)\n",
    "    d4 = (x3*w2 + x4*w4)\n",
    "    \n",
    "    result = np.array([[d1, d2], [d3, d4]])\n",
    "    \"\"\"\n",
    "    result = a @ b\n",
    "    return result\n",
    "    \n",
    "\n",
    "x1, x2, x3, x4  = 1.0, 3.0, 5.0, 7.0\n",
    "w1, w2, w3, w4  = 7.0, 9.0, 1.0, 2.0\n",
    "\n",
    "a = np.array([[x1, x2], [x3, x4]])\n",
    "b = np.array([[w1, w2], [w3, w4]])\n",
    "\n",
    "old = f(a, b)\n",
    "a[1, 0] += h_ # or changing x1 slightly\n",
    "new = f(a, b)\n",
    "\n",
    "# slope\n",
    "((new - old) / h_).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[x1, x2], [x3, x4]])\n",
    "b = np.array([[w1, w2], [w3, w4]])\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means the `X1` has the local gradient of `7 + 9 = 16`. *(as you can see, `X1` is related with `W1` and `W2` which take `7` and `9` values respectively.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but suppose we have the gradient of the output of a@b (as we do with logits)\n",
    "output_grad = np.array([[1., 2.], [3., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then the `a` layer will have the gradient\n",
    "output_grad @ b.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the Andrej's video. But we can verify. \n",
    "\n",
    "Say we are finding the same gradient for `X1` as found about which has the local gradient of 16. **But** because of the chain rule we will need to consider the `d1` and `d2` gradients as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1 gradient (from `output_grad`)\n",
    "d1_grad = output_grad[0, 0]\n",
    "d2_grad = output_grad[0, 1]\n",
    "\n",
    "# The gradient for `x1` since it is what affects d1 and d2\n",
    "d1_grad * 7 + d2_grad * 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ That **confirms!!** the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here in the example I tried to find the slope of `x1`. Which is related with `w1` and `w3` thus it will have the slope `7` and `9` respectively and added up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change and play around with **any** value you like instead of `x1` and see how it affects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all weights are **related with the `xn` values** with the `*` multiplication, we can see that how they are getting the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§® So what? Exactly?\n",
    "I know it sound a little confusing and isn't properly understood yet. Let's take some additional steps to see what happens behind the scenes.\n",
    "\n",
    "But, now. **Please pay attention**. A little miss would be dangerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/the_h_story.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holy! ü§Ø\n",
    "That was a lot for a small stuff, but I had to, to make things **clear-er**. Now we can **confidently** use the *hacky* thing here.\n",
    "\n",
    "> Remember, we still are in the \"*`4.10` Derivative of `h` wrt `logits`*\" section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dlogits @ W2.T\n",
    "\n",
    "# Check-9 ‚úÖ\n",
    "cmp('h', dh, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.11` Derivative of `W1` wrt `logits`\n",
    "\n",
    "> üí≠ How  our matrix multiplication `W1` will affect the `logits` variable?\n",
    "\n",
    "Again, Working for this line:\n",
    "```python\n",
    "logits = h @ W2 + b2 # output layer\n",
    "```\n",
    "\n",
    "Now, its really easy, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = h.T @ dlogits\n",
    "\n",
    "# Check-10 ‚úÖ\n",
    "cmp('W2', dW2, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.12` Derivative of `b1` wrt `logits`\n",
    "\n",
    "> üí≠ How our  addition of `b1` will affect the `logits` variable?\n",
    "\n",
    "Again, Working for this line:\n",
    "```python\n",
    "logits = h @ W2 + b2 # output layer\n",
    "```\n",
    "\n",
    "This is the easy part, we actually could've done this way before `h` and `W2`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since `b2` is related with (h @ W1) with the `+` sign, it will simply\n",
    "# \"pass\" the grads of `dlogits`.\n",
    "\n",
    "db2 = dlogits.sum(0) # need to perform the sum becaise basically `b2` is broadcast\n",
    "\n",
    "# Check-11 ‚úÖ\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú Now, towards the layer-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.13` Derivative of `hpreact` wrt `h`\n",
    "\n",
    "> üí≠ How the `tanh(hpreact)` will affect the `h` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "```\n",
    "\n",
    "This is simply back propogating through the `tanh` activation. Let's refer micrograd.\n",
    "\n",
    "```python\n",
    "def tanh(self):\n",
    "    x = self.data\n",
    "    t = (np.exp(2*x) - 1) / (np.exp(2*x) + 1)\n",
    "    out = Value(t, (self,), \"tanh\")\n",
    "    \n",
    "    def _backward():\n",
    "        self.grad = (1 - t ** 2) * out.grad # chain rule here too!\n",
    "        out._backward = _backward\n",
    "    return out\n",
    "```\n",
    "\n",
    "Let's apply this here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhpreact = (1 - h**2) * dh\n",
    "\n",
    "# Check-12 ‚úÖ\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Now we will be working with this portion of the forward pass\n",
    "\n",
    "```python\n",
    "# üëâ Inverse the variance so that it can be \"multiplied\"\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "## Note: 1e-5 is a small constant to avoid division by zero\n",
    "\n",
    "# üëâ Finally standardize the numbers\n",
    "bnraw = bndiff * bnvar_inv\n",
    "\n",
    "# üëâ Give the spice to it by gain and bias\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.14` Derivative of `bngain` wrt `hpreact`\n",
    "\n",
    "> üí≠ How the `bngain` will affect the `hpreact` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```\n",
    "Once again **a simple step here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
    "\n",
    "# Check-13 ‚úÖ\n",
    "cmp('bngain', dbngain, bngain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.15` Derivative of `bnraw` wrt `hpreact`\n",
    "\n",
    "> üí≠ How the `bnraw` will affect the `hpreact` variable?\n",
    "\n",
    "**Again**, Working for this line:\n",
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```\n",
    "Once, once again **a simple step here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnraw = (bngain * dhpreact)\n",
    "\n",
    "# Check-14 ‚úÖ\n",
    "cmp('bnraw', dbnraw, bnraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.16` Derivative of ` bnbias` wrt `hpreact`\n",
    "\n",
    "> üí≠ How the ` bnbias` will affect the `hpreact` variable?\n",
    "\n",
    "**Again, again**, Working for this line:\n",
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```\n",
    "Once, once, once again **a simple step here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnbias = dhpreact.sum(0, keepdims=True)\n",
    "\n",
    "# Check-15 ‚úÖ\n",
    "cmp('bnbias', dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.17` Derivative of `bnvar_inv` wrt `bnraw`\n",
    "\n",
    "> üí≠ How the `bnvar_inv` will affect the `bnraw` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# üëâ Finally standardize the numbers\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```\n",
    "\n",
    "Here if you remember, we **are standardizing** the values of first layer so that it stays \"well behaved\". And on this `braw` are will be using the `bias` and `gain` to scale and shift it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdims=True)\n",
    "\n",
    "# Check-16 ‚úÖ\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.18` Derivative of `bndiff` wrt `bnraw`\n",
    "\n",
    "> üí≠ How the `bndiff` will affect the `bnraw` variable?\n",
    "\n",
    "**Again**, Working for this line:\n",
    "```python\n",
    "# üëâ Finally standardize the numbers\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ö Hold on!\n",
    "There is some **history** behind `bndiff`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `bndiff`'s life cycle\n",
    "\n",
    "```python\n",
    "### BatchNorm layer ###\n",
    "\n",
    "# üëâ Find the mean\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# The above is equivalent to: `hprebn.sum(0) / n` ie. simply the mean :)\n",
    "\n",
    "# üëâ Find the difference (BIRTH) üî¥\n",
    "bndiff = hprebn - bnmeani\n",
    "\n",
    "# üëâ Squaring the difference to find variance below (USAGE-1) üî¥\n",
    "bndiff2 = bndiff**2\n",
    "\n",
    "# üëâ Finding the variance\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# The above is equivalent to: `bndiff2.sum(0, keepdim=0) / (n-1)` ie. simply the variance :)\n",
    "\n",
    "# üëâ Inverse the variance so that it can be \"multiplied\"\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "## Note: 1e-5 is a small constant to avoid division by zero\n",
    "\n",
    "# üëâ Finally standardize the numbers  (USAGE-2) üî¥\n",
    "bnraw = bndiff * bnvar_inv \n",
    "\n",
    "# üëâ Give the spice to it by gain and bias\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bndiff_life.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A hell lot of chains! ‚õì\n",
    "Let's start with finding the gradients for `bnvar` *(because we are already done with the `bnvar_inv`)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## `4.19` Derivative of `bnvar` wrt `bnvar_inv`\n",
    "\n",
    "> üí≠ How performing `+ 1e-5` and `**-0.5` on `bnvar` affects the `bnvar_inv`?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# üëâ Inverse the variance so that it can be \"multiplied\"\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "```\n",
    "\n",
    "To help you recall this line a little, we are basically **inversing the variance** for the $\\frac{(x - \\mu)^2}{\\sigma^2}$ so that it can be multiplied. \n",
    "\n",
    "Thus, we are in the standardization step.\n",
    "\n",
    "> üìù We are **also** adding a simple, small number `0.00005` in the variance aka `bnvar` so that we don't get the zero division error. *Thus, we need to take that into the account as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample function to guide us through\n",
    "def f(a):\n",
    "    return (a + 1e-5) **-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3.0\n",
    "\n",
    "old = f(a)\n",
    "a += h_\n",
    "new = f(a)\n",
    "\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5.0\n",
    "\n",
    "old = f(a)\n",
    "a += h_\n",
    "new = f(a)\n",
    "\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking out for the derivative** on the internet would give us:\n",
    "\n",
    "$-\\dfrac{1}{2\\left(x+\\frac{1}{100000}\\right)^\\frac{3}{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks\n",
    "\n",
    "### when a=3.0\n",
    "print(\n",
    "    -(1 / (2 * ((3.0 + 1e-5)**1.5)))\n",
    ")\n",
    "\n",
    "\n",
    "### when a=5.0\n",
    "print(\n",
    "    -(1 / (2 * ((5.0 + 1e-5)**1.5)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximate**, but it works. So let's use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ The way we found\n",
    "$-\\dfrac{1}{2\\left(x+\\frac{1}{100000}\\right)^\\frac{3}{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar = -(1 / (2 * ((bnvar + 1e-5)**1.5))) * dbnvar_inv\n",
    "\n",
    "# Check-17 ‚úÖ\n",
    "cmp('bnvar', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ As said, **it is the approximate** and also this is how I found it online. Let's check *what Andrej has used* in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Andrej's way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar = -0.5*(bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "\n",
    "# Check-17 ‚úÖ\n",
    "cmp('bnvar', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ **Whoa!!** It works! Exact.\n",
    "\n",
    "> üòä We will keep this *Andrej's way* because it is giving `exact = True`. Though, we can take *our way*, but in the subsequent checks it will \"retain\" it's approximate effect and will give the \"approximate\" results all times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.20` Derivative of `bndiff2` wrt `bnvar`\n",
    "\n",
    "> üí≠ How the calculation of the variance done via `bndiff2` will affect the `bnvar`?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# üëâ Finding the variance\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# The above is equivalent to: `bndiff2.sum(0, keepdim=0) / (n-1)` ie. simply the variance :)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here, multiple things are going on together, but if we look in the graph once again, we can see that the things are still differenciable. \n",
    "\n",
    "Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ Look that the `.sum()` is simply `a + b + c + d` and then it is multiplied with the constantv `1/(n - 1)`. Same thing that we have been doing till now, will also be applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function at the rescue!\n",
    "def f(a, b, c, d):\n",
    "    sum_ = a + b + c + d\n",
    "    constant = 1 / (4 - 1) # `4` is in place of `n`\n",
    "    return constant * sum_\n",
    "\n",
    "a, b, c, d = 1., 3., 5., 2.\n",
    "\n",
    "old = f(a, b, c, d)\n",
    "a += h_\n",
    "new = f(a, b, c, d)\n",
    "\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which equals to `1/3` as a slope. And having say `6` variables would give use `1/5` as the slope. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 6 variables\n",
    "def f(a, b, c, d, e, f_):\n",
    "    sum_ = a + b + c + d  + e + f_\n",
    "    constant = 1 / (6 - 1) # `6` is in place of `n`\n",
    "    return constant * sum_\n",
    "\n",
    "a, b, c, d, e, f_ = 1., 3., 5., 2., 3., 5.\n",
    "\n",
    "old = f(a, b, c, d, e, f_)\n",
    "a += h_\n",
    "new = f(a, b, c, d, e, f_)\n",
    "\n",
    "(new - old) / h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right? It is `1/5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†  Watchout the shape! Here we are dealing with the layer which had `32 x 64` before and after the operation it became `1 x 64`. <br> <br> In the situations which are the reverse of this, we do **sum** while backpropogating *(many time what we have done above)*, but herer we will do the reverse of it, we will have to \"cast\" the shape from `1 x 64` to `32 x 64` and the easy way to do this, is by using the `torch.ones()` as shown in the lecture üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "propogated_shape = torch.ones_like(bndiff2)\n",
    "dbndiff2 = (1/(n-1)) * propogated_shape * dbnvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff2.shape, bndiff2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check-18 ‚úÖ\n",
    "cmp('bndiff2', dbndiff2, bndiff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.21` Derivative of `bndiff` wrt `bndiff2` (usage-1)\n",
    "\n",
    "> üí≠ How the `bndiff` will affect the `bndiff2` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# üëâ Squaring the difference to find variance below (USAGE-1) üî¥\n",
    "bndiff2 = bndiff**2\n",
    "```\n",
    "\n",
    "This is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The part one calculation\n",
    "dbndiff = (2 * bndiff) * dbndiff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.18` Derivative of `bndiff` wrt `bnraw` (again & finally | Usage - 2)\n",
    "\n",
    "> üí≠ How the `bndiff` will affect the `bnraw` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# üëâ Finally standardize the numbers  (USAGE-2) üî¥\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second half :)\n",
    "dbndiff += bnvar_inv * dbnraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check-19  ‚úÖ\n",
    "cmp('bndiff', dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.22` Derivative of `bnmeani` wrt `bndiff`\n",
    "\n",
    "> üí≠ How the `bnmeani` will affect the `bndiff` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# üëâ Find the difference\n",
    "bndiff = hprebn - bnmeani\n",
    "```\n",
    "\n",
    "Well, that is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnmeani = (-1.0 * dbndiff).sum(0, keepdims=True)\n",
    "\n",
    "# Check-20 ‚úÖ\n",
    "cmp('bnmeani', dbnmeani, bnmeani)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.23` Derivative of `hprebn` wrt `bndiff`\n",
    "\n",
    "> üí≠ How the `hprebn` will affect the `bndiff` variable?\n",
    "\n",
    "**Again**, Working for this line:\n",
    "```python\n",
    "# üëâ Find the difference\n",
    "bndiff = hprebn - bnmeani\n",
    "```\n",
    "\n",
    "‚ñ∂ Well, It has a little history. Let's see it's lifecycle.\n",
    "\n",
    "```python\n",
    "# Linear layer 1 (BIRTH) üî¥\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "\n",
    "# üëâ Find the mean (USAGE - 1) üî¥\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# The above is equivalent to: `hprebn.sum(0) / n` ie. simply the mean :)\n",
    "\n",
    "# üëâ Find the difference (USAGE - 2) üî¥\n",
    "bndiff = hprebn - bnmeani\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/hpredn_life.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why firs of all we will need to find other derivations along the way, which is not crazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.24` Derivative of `hprebn.sum()` wrt `bnmeani`\n",
    "\n",
    "> üí≠ How the `hprebn` will affect the `bndiff` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# üëâ Find the mean (USAGE - 1) üî¥\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# The above is equivalent to: `hprebn.sum(0) / n` ie. simply the mean :)\n",
    "```\n",
    "\n",
    "Here the `hprebn` is summed up and then multiplied with a constant `1/n`.\n",
    "\n",
    "> üí≠ That means, `1/n` is it's slope. After having enough experience with previous exercises, I think we don't need to uise our `f` function üòâ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First half of `dhprebn`\n",
    "dhprebn = torch.ones_like(hprebn) * (1/n) * dbnmeani\n",
    "# again, did `ones_like` to make the shape proper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.23` Derivative of `hprebn` wrt `bndiff` (continued, again)\n",
    "\n",
    "> üí≠ How the `hprebn` will affect the `bndiff` variable?\n",
    "\n",
    "**Again**, Working for this line:\n",
    "```python\n",
    "# üëâ Find the difference (USAGE - 2) üî¥\n",
    "bndiff = hprebn - bnmeani\n",
    "```\n",
    "\n",
    "Which is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second half\n",
    "dhprebn += 1.0 * dbndiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check-21 ‚úÖ\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.25` Derivative of `embcat` wrt `hprebn`\n",
    "\n",
    "> üí≠ How the embeddings `embcat` will affect the `hprebn` variable?\n",
    "\n",
    "Working for this line:\n",
    "```python\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "```\n",
    "\n",
    "This is again the simple case which we **\"demystified\"** in the `4.10` section, do visit it once again ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "# Check-22 ‚úÖ\n",
    "cmp('embcat', dembcat, embcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.26` Derivative of `W1` wrt `hprebn`\n",
    "\n",
    "> üí≠ How the weights `W1` will affect the `hprebn` variable?\n",
    "\n",
    "**Again**,Working for this line:\n",
    "```python\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "# Check-23 ‚úÖ\n",
    "cmp('W1', dW1, W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.27` Derivative of `b1` wrt `hprebn`\n",
    "\n",
    "> üí≠ How the bias `b1` will affect the `hprebn` variable?\n",
    "\n",
    "**Again**,Working for this line:\n",
    "```python\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = (1.0 * dhprebn).sum(0, keepdims=True)\n",
    "\n",
    "# Check-24 ‚úÖ\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.28` Derivative of `emb` wrt `embcat`\n",
    "\n",
    "> üí≠ How changing the shape of `emb` will affect the `embcat`?\n",
    "\n",
    "Working for these lines:\n",
    "```python\n",
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "# Check-25 ‚úÖ\n",
    "cmp('emb', demb, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `4.29` Derivative of `C` wrt `emb`\n",
    "\n",
    "> üí≠ How  selecting only 32 examples in `C` affects the `emb`?\n",
    "\n",
    "**Still**, Working for these lines:\n",
    "```python\n",
    "# forward pass, \"chunked\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/final_flow.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It was simpler than I expected\n",
    "\n",
    "## Create a grad matrix \n",
    "dC = torch.zeros_like(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample that passed in...\n",
    "for th, sample in enumerate(Xb):\n",
    "    for idx, loc in enumerate(sample):\n",
    "        dC[loc] += demb[th, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üôÑ Explainer\n",
    "```python\n",
    "üëâ for th, sample in enumerate(Xb):\n",
    "```\n",
    "\n",
    "    It will take:\n",
    "    Xb = [[1, 24, 5],\n",
    "          [6, 4, 3],\n",
    "          [2, 24, 5],\n",
    "          [1, 4, 7],\n",
    "          [5, 6, 5]]\n",
    "\n",
    "    [1, 24, 5] in the first iteration.\n",
    "    \n",
    "```python\n",
    "üëâ for idx, loc in enumerate(sample):\n",
    "```\n",
    "\n",
    "    It will take from: [1, 24, 5]\n",
    "    One-by-one so that it can be used in `C`\n",
    "    \n",
    "Then it will simply work. Have a look at the code, **it will make sense**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check-26 ‚úÖ\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üî•üî•üî• We have done this!!\n",
    "**Exercise - 1** completed successfully!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yo! That was a LOT!\n",
    "If you're not feeling something, you **should**! The job was incredible dude!\n",
    "\n",
    "Now a simple note! *We won't be going to solve the next exercises*. üò¢ <br>\n",
    "The simple reason is that, I am **not a calculus guy** to be frank and I think whatever we have done above is more than enough for us to understand what is happening under the hood.\n",
    "\n",
    "The main points the exercise #3 and #4 cover are:\n",
    "- Of course all atomic pieces are not going to be a part of internal **loss.backward()** because in practice that will take a **lot** of time to compute.\n",
    "- There has to be a simplified expression which can give the same result.\n",
    "- That will increase the computation speed and is practical.\n",
    "\n",
    "So for now, I am ending this notebook here and will see you at another exciting notebook with **Wavenet**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "PS: And yes we can still be proud and can say we have achieved `#loss.backward()` confidence! üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dog_sunglasses.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
