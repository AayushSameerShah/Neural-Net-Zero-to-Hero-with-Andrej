{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beee0c83-db1b-4d66-9d85-d0caeff3df89",
   "metadata": {},
   "source": [
    "# ‚åö Let's be quick.\n",
    "\n",
    "1. Define the functions\n",
    "2. Roll in!\n",
    "\n",
    "> Why?\n",
    ">\n",
    "> We need to re-define the functions as these are hte pickle files, and they will need to lookup to the definitions. For the mature saving, we would need to do some *official* stuff, but for us, for now the things will work just fine.\n",
    "\n",
    "So below, I will be re-defining the methods. No changes have been made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b7ba6-0874-4948-88a3-069ae584cb6a",
   "metadata": {},
   "source": [
    "# `0.` Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43db2b8f-d7ae-48eb-b39e-e61a8c7e1e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17fe866d8b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, pickle\n",
    "import torch.nn as nn # for layers and stuff\n",
    "from torch.nn import functional as F # for the loss function and softmax\n",
    "torch.manual_seed(1337) # same as in the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ac221-f02a-4a50-bdf2-0cb30ca12c8d",
   "metadata": {},
   "source": [
    "# `1.` Re-defining everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a8b778-ebee-417a-a2ab-40b1a6132c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    The Final BigramLM model does the following:\n",
    "    \n",
    "    ## Has:\n",
    "    1. Token embedding layer\n",
    "    2. Position embedding layer\n",
    "    3. Nx Blocks which has multihead attentions and feed-forward\n",
    "    4. Finally the LM-head\n",
    "    5. The shapes written in comments\n",
    "    \n",
    "    ## Does:\n",
    "    1. Takes the input which will be in the B, T format\n",
    "    2. Converts them into B, T, C (starting with the Token embedding layer)\n",
    "    3. The rest is the history... you really want me to talk much!? \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)       \n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential( \n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)          \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape \n",
    "        tok_emb = self.embedding_table(idx) \n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device))\n",
    "        x = tok_emb + positions_emb         \n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)             \n",
    "    \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)        \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9de115-65e5-4f19-b625-f25173a513bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    The block basically is the collection of self attention layers (multi) and \n",
    "    the feed forward layers with residual connections and the layer norm layers.\n",
    "    \n",
    "    All we want to do is to isolate them so that we can make as many as we want\n",
    "    and get better results!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size) \n",
    "        self.add_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.add_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.add_norm_1(x))  # B, T, head_size\n",
    "        x = x + self.ffwd(self.add_norm_2(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411cd406-246b-4455-bd6a-d2ecbc91ac97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout) ###\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out)) ###\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3d30a2-66d7-4c41-8a23-0b77b5fedb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class will simply create the Q, K, V vectors\n",
    "    and also the reguster_buffer to create the mask.\n",
    "    \n",
    "    Then on the `forward` it will pass the vectors in the \n",
    "    Q, K, V and give the `out`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Take the `x` input which will be the positions.\n",
    "        The shape will be B, T, C meaning:\n",
    "        \"For each batch, there will be T tokens which will have positions encoded in C\n",
    "        space\"\n",
    "        \n",
    "        We will use that and work oursalves forward.\n",
    "        '''\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # the C**-0.5 is used to control the variance\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # the mask\n",
    "        wei = F.softmax(wei, dim=-1) # the final wei\n",
    "        \n",
    "        wei = self.dropout(wei) ###\n",
    "        out = wei @ v # this is what we will use further\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59fe6c0-8276-42a2-b619-b66150c62f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout) ###\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de857319-e381-43ba-8e7c-a57a7a25aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RegexTokenizer:\n",
    "    '''\n",
    "    This is supposed to get a little crazy.\n",
    "    \n",
    "    Step 1: Split the text based on the regex pattern.\n",
    "    Step 2: Now, we have the cleaned words.\n",
    "    Step 3: Get their raw tokens individually.\n",
    "    Step 4: Don't merge them yet, because it will nullify the step 1-3. \n",
    "    Step 4: Find pairs (stats) for each of the words - while keeping \"common\" stats across each.\n",
    "    Step 5: Find the max repetative pair.\n",
    "    Step 6: Replace that pair in each token group.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # initialize the defaut vocab\n",
    "        self.vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "        self.trained=False\n",
    "        self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        self.GPT4_PATTERN_COMPILED = re.compile(self.GPT4_SPLIT_PATTERN)\n",
    "    \n",
    "    def find_most_repeated_pair(self, tokens, counter=None) -> Tuple[Tuple, int, Dict]:\n",
    "        '''\n",
    "        Now, this function is changed slightly as we will calculcate the \n",
    "        max when needed after this function call.\n",
    "        \n",
    "        Also, the `counter` can be passed and updated, and returned.\n",
    "        Doing this will ensure, the global counter.\n",
    "        '''\n",
    "        counter = counter if counter is not None else defaultdict(int)\n",
    "        for pair in zip(tokens, tokens[1:]):\n",
    "            counter[pair] += 1\n",
    "        return counter # will be useful when the counter=None passed.\n",
    "\n",
    "    def replace_pair_with_new_token(self, tokens, pair, new_idx) -> List:\n",
    "        new_tokens = [] # this will hold the copy for the new tokens\n",
    "        idx = 0\n",
    "        while idx < len(tokens):\n",
    "            if idx < len(tokens) - 1 and (tokens[idx] == pair[0]) and (tokens[idx + 1] == pair[1]): # this is a match!\n",
    "                new_tokens.append(new_idx)\n",
    "                idx += 2\n",
    "            else: # this is not a match\n",
    "                new_tokens.append(tokens[idx])\n",
    "                idx += 1\n",
    "        return new_tokens\n",
    "        \n",
    "    def train(self, blob, vocab_size=None) -> None:\n",
    "        '''\n",
    "        This function will train the tokenizer based on the \n",
    "        training data given as text.\n",
    "        \n",
    "        1. blob: The data in text format that will be used as training\n",
    "            of the tokenizer.\n",
    "        \n",
    "        2. vocab_size: This is \"how many new tokens you want to generate\"\n",
    "            - `None` means indefinite; generate all combinations.\n",
    "            - `int` means the number of merges.\n",
    "        '''\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # First split\n",
    "        cleaned_text = self.GPT4_PATTERN_COMPILED.findall(blob)\n",
    "        # Then create the tokens\n",
    "        self.tokens = [list(map(int, word.encode(\"utf-8\"))) for word in cleaned_text]\n",
    "        \n",
    "        \n",
    "        new_idx = 255\n",
    "        merges = {}\n",
    "        for i in tqdm(range(vocab_size)):\n",
    "            stats = defaultdict(int)\n",
    "            for token_group in self.tokens:\n",
    "                # pass the stats, which will be updated in place\n",
    "                self.find_most_repeated_pair(token_group, stats)\n",
    "            \n",
    "            max_pair = max(stats, key=stats.get)\n",
    "            max_count = stats[max_pair]\n",
    "            \n",
    "            if max_count > 1:\n",
    "                new_idx += 1\n",
    "                self.tokens = [self.replace_pair_with_new_token(token_group, max_pair, new_idx) for token_group in self.tokens]\n",
    "                merges[max_pair] = new_idx\n",
    "            else: # every pair is occuring for once only\n",
    "                break\n",
    "        self.total_merges = i+1\n",
    "        \n",
    "        ## The training is done now merge the stuff\n",
    "        for pair, idx in merges.items():\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "        self.merges = merges   \n",
    "        self.trained = True\n",
    "        \n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        The goal of this function is to encode the given text into the \n",
    "        tokens that are acceptable by our `vocab`.\n",
    "        \n",
    "        So, we will need to keep encoding the tokens form the start (top)\n",
    "        to the bottom.\n",
    "        \n",
    "        The `order` of the vocab **is not guerenteed** in the older versions\n",
    "        of python, so we wil need to rely on the `idx`. The lower the idx\n",
    "        is, the older that token is!\n",
    "        '''\n",
    "        \n",
    "        if not self.trained:\n",
    "            raise NotImplementedError(\"Please first train the tokenizer!\")\n",
    "        \n",
    "        # tokens = text.encode(\"utf-8\")\n",
    "        split_words = self.GPT4_PATTERN_COMPILED.findall(text)\n",
    "        split_tokens = [list(word.encode(\"utf-8\")) for word in split_words]\n",
    "        \n",
    "        final_tokens = []\n",
    "        for chunk in split_tokens:\n",
    "            while len(chunk) >= 2:\n",
    "                stats = self.find_most_repeated_pair(chunk)\n",
    "                # now the goal is to get all pairs of the new tokens\n",
    "                # we are not interested in the count, just the pairs\n",
    "                # then check for each pair, if \n",
    "                pair_replace = min(self.merges, key=lambda x: stats.get(x, float(\"inf\")))\n",
    "                if pair_replace in stats:\n",
    "                    chunk = self.replace_pair_with_new_token(chunk, \n",
    "                                                     pair_replace,\n",
    "                                                     self.merges[pair_replace])\n",
    "                else:\n",
    "                    break\n",
    "            final_tokens.extend(chunk)\n",
    "        return final_tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        decoded_stream = [self.vocab[idx] for idx in tokens]\n",
    "        text = b\"\".join(decoded_stream)\n",
    "        return text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a43dd2-192c-410c-82de-a00ef67fb2a2",
   "metadata": {},
   "source": [
    "# `2.` Also the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d939490-f2c2-4dc0-8318-53413ccbeb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64      # samples we will use for the single forward pass\n",
    "block_size = 256     # the context window (significantly bigger than our toy examples)\n",
    "max_iters = 5000     # total forward-backward passes\n",
    "\n",
    "eval_interval = 500  # after how many steps we want to print the loss?\n",
    "learning_rate = 3e-4 # learning rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "eval_iters = 200    # when printing the loss, how many samples to consider for validation?\n",
    "n_embd = 384        # embedding size of each token\n",
    "n_head = 6          # `n` multi heads for the self-attention\n",
    "n_layers = 6        # `n` for `Nx` which shows how many blocks to use\n",
    "dropout = 0.2       # randomly drop % percentage of waights from getting trained for that single pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb30ca1-c1bd-470a-bc71-1d1b4c91b78e",
   "metadata": {},
   "source": [
    "# `3.` Finally, loading the model üéâ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6918ad9d-129e-4334-a31d-8cee31dff168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"./model/ShakeGPT_BPE.pt\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ef6a2-c88d-4c30-9b21-9e9a9571ca02",
   "metadata": {},
   "source": [
    "# `4.` Loading the tokenizer üéâüéâ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb0a78e-be8b-4e83-a5af-a445e6e38f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./model/tokenizer.pkl\", \"rb\") as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcce9f8-d880-4ca0-a8b7-fa488019671a",
   "metadata": {},
   "source": [
    "# `5.` Generating stuff!!! üéâüéâüéâ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776d9829-7da4-4900-9bbd-b5fdc3b0e96c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000ed friend\n",
      "Till health for her by his brother:, who, forgive her\n",
      "Shall rebellion too.\n",
      "\n",
      "GLOUCESTER:\n",
      "There straight have made us, you'll show me wail the city.\n",
      "\n",
      "GLOUCESTER:\n",
      "But I spit, my brother is careless to you,\n",
      "If he stand for being all, that he is no less\n",
      "Abid our kindred title at Bontona.\n",
      "\n",
      "HAMBENBROKE:\n",
      "O, that I uncle Your brother, your humour be spirit of sorrow;\n",
      "Who stands but in the mouth as free,\n",
      "It were well deserved with Mowbray's forefellow.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Pithee of him, Lord Hastings, and you come.\n",
      "What, will you beg of this, that ill, or you are of gone?\n",
      "\n",
      "Boy:\n",
      "Ay, but not up for yourself; for, my good lord.\n",
      "\n",
      "GLOUCESTER:\n",
      "And we will not hold your subject.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Thanks, farewell: O, what news!\n",
      "See that will not despair of that?\n",
      "\n",
      "GLOUCESTER:\n",
      "Now, lords, Edward, after man,\n",
      "Grant us ye not our friends; Sir Wednesee\n",
      "King Edward's love, and not as of pause\n",
      "As if that we should second yield them both;\n",
      "Away to rcourage it rather\n",
      "Than those targed as I have my CARGARET:\n",
      "I was not said King Henry's enemies.\n",
      "\n",
      "GLOUCESTER:\n",
      "Ah, 'tis my brother father,\n",
      "Meantime shall thus rather for such so offend,\n",
      "Bless myself, in plucking great trifless mirr'd me.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "'Tis whether thou art stuff, brothers not a coward,\n",
      "For traitor, being so quick?\n",
      "\n",
      "GLOUCESTER:\n",
      "Ay, to debase your honour, as I be access,\n",
      "Even as to say the place.\n",
      "\n",
      "GLOUCESTER:\n",
      "Under your face hath away.\n",
      "But can tell him that I tell you for your hate\n",
      "My will former show where you are my tongue\n",
      "Say play the England's prety,\n",
      "And foul perfectise in gore of your age,\n",
      "Falsehood to his mistress' son.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Ay, cousin! and so I marry it so?\n",
      "So jest, smile we rule my gracious lord,\n",
      "And honest, attend on your relish blood.\n",
      "Our holy father, when we bid her withal lasts,\n",
      "Send on her sin unto lay with grief,\n",
      "And send the brought of his hand towards York\n",
      "The present in me.\n",
      "\n",
      "GLOUCESTER:\n",
      "He hath not recomplain my cheek\n",
      "His mighty could quicken us whence I'll not be.\n",
      "Comes; I a protector,--\n",
      "\n",
      "GLOUCESTER:\n",
      "O cousin, all, castable sickness,\n",
      "He flowers the bed, answer, grants\n",
      "And wants something trembrace the feasts of my death,\n",
      "Destroying angel draging in his grave.\n",
      "\n",
      "WARWICK:\n",
      "What news abroad?\n",
      "\n",
      "Messenger:\n",
      "Nor truth, which heart have no less confume,\n",
      "That from the hopeful army banish'd moont\n",
      "At some traitor and sole spuring institute\n",
      "To answer him as sack in spleen's torture;\n",
      "And being more minded, can he more\n",
      "Shall titled this tedious sting than my grave.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now tell me, brother of this treason head and my life.\n",
      "Therefore, Warwick, wait it up so:\n",
      "And, Warwick, vulgen do not go.\n",
      "She you grant that I'll to the rancour of death\n",
      "And that I may be fearful mother day:\n",
      "Therefore, if my uncle Gloucester, my brother Clarence,\n",
      "Who, came from Montague and my arm,\n",
      "Where in that might have leave to let them send\n",
      "And see the solely title leave, my brother Margues,\n",
      "That I may be these killeds,\n",
      "Inter the time of Barlarence and Coventry,\n",
      "But, how made the tomb his hand,\n",
      "To yield urave me and the jointment him,\n",
      "Witness of honour brook your grace.\n",
      "\n",
      "WARWICK:\n",
      "Warwick, brother Warwick,' York.\n",
      "Now, in hold my men of blood,\n",
      "Is it not in Edward's chase a monarch.\n",
      "The earld, and thy spothes loyalty,\n",
      "Shall come from the after man's fruit.\n",
      "\n",
      "KING HENRY VI:\n",
      "Away with our lady's free enemies out,\n",
      "'Are I answer than at thy ghost,\n",
      "And hear some satisfied: send it not follow him;\n",
      "A king, like other sepul traitors,\n",
      "For foul dear as his banishment\n",
      "And passing and exhastisperously in his love\n",
      "Unto his breast with thy court\n",
      "And, being such assistory to the fearful man\n",
      "Against the field the field ofORFOLK:\n",
      "What more shall turn? Then hope is depart not seen,\n",
      "But by the gone and our locks;\n",
      "If just, are in the noise, make free\n",
      "Is not ten thousand strong. My neither not thy mother,\n",
      "That I may denied; and I may desire\n",
      "In honour he do well as Culet, who comes this raven\n",
      "In sorest from Angelo.\n",
      "\n",
      "CLAUDIO:\n",
      "Our musical lips of my brother's son,\n",
      "If is due already\n",
      "To mistake his life: thou darest;\n",
      "That when the title bountenants maketer moon\n",
      "More powerful shope confession\n",
      "As that doth bleeds as makes your love and more\n",
      "Than any life and for learn of such question.\n",
      "O, sort, can ever pray you,\n",
      "When I am like one thd make wise thine ears\n",
      "Only play'd the ground with the deept of the unredvow,\n",
      "His desertain temperate surfeities\n",
      "With reasons and most loving gentleeless\n",
      "Than any companion as we work\n",
      "To find the beams of the breath top.\n",
      "\n",
      "First Lord:\n",
      "Tell me this, my lord, the self-shalled trunk in\n",
      "Than Edward's present time, and towards Galliester\n",
      "And that his peral war,\n",
      "And yet his cankindness is a noble to him,\n",
      "And give him whose deserves even withal\n",
      "He doubt not to success welcome your fathers.\n",
      "\n",
      "Lord:\n",
      "How now, what would you do?\n",
      "\n",
      ":\n",
      "What, my lord.\n",
      "\n",
      "Lord:\n",
      "Marry, the Lordshire's habit.\n",
      "\n",
      "CATESBY:\n",
      "I hope of this, my lord,\n",
      "I do beseech you, and leave it,\n",
      "Before you wrong to his cory; or I mean you\n",
      "Their five and your love assistance.\n",
      "\n",
      "LUCIO:\n",
      "Why, this is the message of false\n",
      "To the outward free: you are the Tower.\n",
      "\n",
      "First Gentleman:\n",
      "Was it not; here gone, the lunates; for they come\n",
      "ansomen, his head.\n",
      "\n",
      "LUCIO:\n",
      "Nay, sir, where he is the man that the business in the Vols\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),  ### üóΩ Transfer to device üóΩ ###\n",
    "        max_new_tokens=2048)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58935428-a02f-4658-a32d-97f4d44eb7a1",
   "metadata": {},
   "source": [
    "## Here's the output image, if Github doesn't render above cell üëÜüèª \n",
    "<img src=\"./images/bpe-output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0675e-215b-42ad-a5d4-2b3c353ab025",
   "metadata": {},
   "source": [
    "# Looks great! Isn't it?\n",
    "Let'a take a pause, and meet in the next adventure! ü•¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
