{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beee0c83-db1b-4d66-9d85-d0caeff3df89",
   "metadata": {},
   "source": [
    "# âŒš Let's be quick.\n",
    "\n",
    "1. Define the functions\n",
    "2. Roll in!\n",
    "\n",
    "> Why?\n",
    ">\n",
    "> We need to re-define the functions as these are hte pickle files, and they will need to lookup to the definitions. For the mature saving, we would need to do some *official* stuff, but for us, for now the things will work just fine.\n",
    "\n",
    "So below, I will be re-defining the methods. No changes have been made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b7ba6-0874-4948-88a3-069ae584cb6a",
   "metadata": {},
   "source": [
    "# `0.` Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43db2b8f-d7ae-48eb-b39e-e61a8c7e1e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17fe866d8b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, pickle\n",
    "import torch.nn as nn # for layers and stuff\n",
    "from torch.nn import functional as F # for the loss function and softmax\n",
    "torch.manual_seed(1337) # same as in the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ac221-f02a-4a50-bdf2-0cb30ca12c8d",
   "metadata": {},
   "source": [
    "# `1.` Re-defining everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a8b778-ebee-417a-a2ab-40b1a6132c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    \"\"\"\n",
    "    The Final BigramLM model does the following:\n",
    "    \n",
    "    ## Has:\n",
    "    1. Token embedding layer\n",
    "    2. Position embedding layer\n",
    "    3. Nx Blocks which has multihead attentions and feed-forward\n",
    "    4. Finally the LM-head\n",
    "    5. The shapes written in comments\n",
    "    \n",
    "    ## Does:\n",
    "    1. Takes the input which will be in the B, T format\n",
    "    2. Converts them into B, T, C (starting with the Token embedding layer)\n",
    "    3. The rest is the history... you really want me to talk much!? \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)       \n",
    "        self.positions_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential( \n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)          \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape \n",
    "        tok_emb = self.embedding_table(idx) \n",
    "        positions_emb = self.positions_embeddings(torch.arange(T, device=device))\n",
    "        x = tok_emb + positions_emb         \n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)             \n",
    "    \n",
    "        if targets is None: \n",
    "            loss=None\n",
    "        else:               \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)        \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9de115-65e5-4f19-b625-f25173a513bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    The block basically is the collection of self attention layers (multi) and \n",
    "    the feed forward layers with residual connections and the layer norm layers.\n",
    "    \n",
    "    All we want to do is to isolate them so that we can make as many as we want\n",
    "    and get better results!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size) \n",
    "        self.add_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.add_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.add_norm_1(x))  # B, T, head_size\n",
    "        x = x + self.ffwd(self.add_norm_2(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411cd406-246b-4455-bd6a-d2ecbc91ac97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout) ###\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out)) ###\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3d30a2-66d7-4c41-8a23-0b77b5fedb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class will simply create the Q, K, V vectors\n",
    "    and also the reguster_buffer to create the mask.\n",
    "    \n",
    "    Then on the `forward` it will pass the vectors in the \n",
    "    Q, K, V and give the `out`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Take the `x` input which will be the positions.\n",
    "        The shape will be B, T, C meaning:\n",
    "        \"For each batch, there will be T tokens which will have positions encoded in C\n",
    "        space\"\n",
    "        \n",
    "        We will use that and work oursalves forward.\n",
    "        '''\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # the C**-0.5 is used to control the variance\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # the mask\n",
    "        wei = F.softmax(wei, dim=-1) # the final wei\n",
    "        \n",
    "        wei = self.dropout(wei) ###\n",
    "        out = wei @ v # this is what we will use further\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59fe6c0-8276-42a2-b619-b66150c62f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout) ###\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de857319-e381-43ba-8e7c-a57a7a25aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RegexTokenizer:\n",
    "    '''\n",
    "    This is supposed to get a little crazy.\n",
    "    \n",
    "    Step 1: Split the text based on the regex pattern.\n",
    "    Step 2: Now, we have the cleaned words.\n",
    "    Step 3: Get their raw tokens individually.\n",
    "    Step 4: Don't merge them yet, because it will nullify the step 1-3. \n",
    "    Step 4: Find pairs (stats) for each of the words - while keeping \"common\" stats across each.\n",
    "    Step 5: Find the max repetative pair.\n",
    "    Step 6: Replace that pair in each token group.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # initialize the defaut vocab\n",
    "        self.vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "        self.trained=False\n",
    "        self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        self.GPT4_PATTERN_COMPILED = re.compile(self.GPT4_SPLIT_PATTERN)\n",
    "    \n",
    "    def find_most_repeated_pair(self, tokens, counter=None) -> Tuple[Tuple, int, Dict]:\n",
    "        '''\n",
    "        Now, this function is changed slightly as we will calculcate the \n",
    "        max when needed after this function call.\n",
    "        \n",
    "        Also, the `counter` can be passed and updated, and returned.\n",
    "        Doing this will ensure, the global counter.\n",
    "        '''\n",
    "        counter = counter if counter is not None else defaultdict(int)\n",
    "        for pair in zip(tokens, tokens[1:]):\n",
    "            counter[pair] += 1\n",
    "        return counter # will be useful when the counter=None passed.\n",
    "\n",
    "    def replace_pair_with_new_token(self, tokens, pair, new_idx) -> List:\n",
    "        new_tokens = [] # this will hold the copy for the new tokens\n",
    "        idx = 0\n",
    "        while idx < len(tokens):\n",
    "            if idx < len(tokens) - 1 and (tokens[idx] == pair[0]) and (tokens[idx + 1] == pair[1]): # this is a match!\n",
    "                new_tokens.append(new_idx)\n",
    "                idx += 2\n",
    "            else: # this is not a match\n",
    "                new_tokens.append(tokens[idx])\n",
    "                idx += 1\n",
    "        return new_tokens\n",
    "        \n",
    "    def train(self, blob, vocab_size=None) -> None:\n",
    "        '''\n",
    "        This function will train the tokenizer based on the \n",
    "        training data given as text.\n",
    "        \n",
    "        1. blob: The data in text format that will be used as training\n",
    "            of the tokenizer.\n",
    "        \n",
    "        2. vocab_size: This is \"how many new tokens you want to generate\"\n",
    "            - `None` means indefinite; generate all combinations.\n",
    "            - `int` means the number of merges.\n",
    "        '''\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # First split\n",
    "        cleaned_text = self.GPT4_PATTERN_COMPILED.findall(blob)\n",
    "        # Then create the tokens\n",
    "        self.tokens = [list(map(int, word.encode(\"utf-8\"))) for word in cleaned_text]\n",
    "        \n",
    "        \n",
    "        new_idx = 255\n",
    "        merges = {}\n",
    "        for i in tqdm(range(vocab_size)):\n",
    "            stats = defaultdict(int)\n",
    "            for token_group in self.tokens:\n",
    "                # pass the stats, which will be updated in place\n",
    "                self.find_most_repeated_pair(token_group, stats)\n",
    "            \n",
    "            max_pair = max(stats, key=stats.get)\n",
    "            max_count = stats[max_pair]\n",
    "            \n",
    "            if max_count > 1:\n",
    "                new_idx += 1\n",
    "                self.tokens = [self.replace_pair_with_new_token(token_group, max_pair, new_idx) for token_group in self.tokens]\n",
    "                merges[max_pair] = new_idx\n",
    "            else: # every pair is occuring for once only\n",
    "                break\n",
    "        self.total_merges = i+1\n",
    "        \n",
    "        ## The training is done now merge the stuff\n",
    "        for pair, idx in merges.items():\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "        self.merges = merges   \n",
    "        self.trained = True\n",
    "        \n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        The goal of this function is to encode the given text into the \n",
    "        tokens that are acceptable by our `vocab`.\n",
    "        \n",
    "        So, we will need to keep encoding the tokens form the start (top)\n",
    "        to the bottom.\n",
    "        \n",
    "        The `order` of the vocab **is not guerenteed** in the older versions\n",
    "        of python, so we wil need to rely on the `idx`. The lower the idx\n",
    "        is, the older that token is!\n",
    "        '''\n",
    "        \n",
    "        if not self.trained:\n",
    "            raise NotImplementedError(\"Please first train the tokenizer!\")\n",
    "        \n",
    "        # tokens = text.encode(\"utf-8\")\n",
    "        split_words = self.GPT4_PATTERN_COMPILED.findall(text)\n",
    "        split_tokens = [list(word.encode(\"utf-8\")) for word in split_words]\n",
    "        \n",
    "        final_tokens = []\n",
    "        for chunk in split_tokens:\n",
    "            while len(chunk) >= 2:\n",
    "                stats = self.find_most_repeated_pair(chunk)\n",
    "                # now the goal is to get all pairs of the new tokens\n",
    "                # we are not interested in the count, just the pairs\n",
    "                # then check for each pair, if \n",
    "                pair_replace = min(self.merges, key=lambda x: stats.get(x, float(\"inf\")))\n",
    "                if pair_replace in stats:\n",
    "                    chunk = self.replace_pair_with_new_token(chunk, \n",
    "                                                     pair_replace,\n",
    "                                                     self.merges[pair_replace])\n",
    "                else:\n",
    "                    break\n",
    "            final_tokens.extend(chunk)\n",
    "        return final_tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        decoded_stream = [self.vocab[idx] for idx in tokens]\n",
    "        text = b\"\".join(decoded_stream)\n",
    "        return text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a43dd2-192c-410c-82de-a00ef67fb2a2",
   "metadata": {},
   "source": [
    "# `2.` Also the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d939490-f2c2-4dc0-8318-53413ccbeb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64      # samples we will use for the single forward pass\n",
    "block_size = 256     # the context window (significantly bigger than our toy examples)\n",
    "max_iters = 5000     # total forward-backward passes\n",
    "\n",
    "eval_interval = 500  # after how many steps we want to print the loss?\n",
    "learning_rate = 3e-4 # learning rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "eval_iters = 200    # when printing the loss, how many samples to consider for validation?\n",
    "n_embd = 384        # embedding size of each token\n",
    "n_head = 6          # `n` multi heads for the self-attention\n",
    "n_layers = 6        # `n` for `Nx` which shows how many blocks to use\n",
    "dropout = 0.2       # randomly drop % percentage of waights from getting trained for that single pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb30ca1-c1bd-470a-bc71-1d1b4c91b78e",
   "metadata": {},
   "source": [
    "# `3.` Finally, loading the model ðŸŽ‰  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6918ad9d-129e-4334-a31d-8cee31dff168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"./model/ShakeGPT_BPE.pt\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ef6a2-c88d-4c30-9b21-9e9a9571ca02",
   "metadata": {},
   "source": [
    "# `4.` Loading the tokenizer ðŸŽ‰ðŸŽ‰  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb0a78e-be8b-4e83-a5af-a445e6e38f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./model/tokenizer.pkl\", \"rb\") as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcce9f8-d880-4ca0-a8b7-fa488019671a",
   "metadata": {},
   "source": [
    "# `5.` Generating stuff!!! ðŸŽ‰ðŸŽ‰ðŸŽ‰ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "776d9829-7da4-4900-9bbd-b5fdc3b0e96c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000ar Plancentio, for this time alone,\n",
      "Where I will gladly give it so.\n",
      "\n",
      "Pardine:\n",
      "And so, by my lord, for I am one:\n",
      "I warrant thee, man, I pity in all;\n",
      "Where is mine is no vulalsous. Fortune;\n",
      "I take her goss charged to Lord's back.\n",
      "\n",
      "Provost:\n",
      "We must stay by, and disposition all our delay.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What's sister, the duke hath done well?\n",
      "And here I came to God whatly from his husband.\n",
      "\n",
      "Provost:\n",
      "Provost, fear't, provost, with him and leave yourself.\n",
      "Must let me try your require you in favour.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Well, you have fellow ever rather respect,\n",
      "Into you pardonow with you: come to your wisdom,\n",
      "If our way shall all plain prayers with complaint.\n",
      "\n",
      "ELLA:\n",
      "Provost.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "That way you, find the queen is come alone.\n",
      "\n",
      "Provost:\n",
      "And now, methought me much understanding.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "It is another day other for him\n",
      "she's rest to my mistress: his father will complain\n",
      "say you what's meet again?\n",
      "\n",
      "Provost:\n",
      "I saw her; for the hope there is that offended\n",
      "these hath madeed not in two such out, soonesty in love with\n",
      "shaps; for, 'tis but when I drawd not such a man:--\n",
      "care, then as I come, I prithee,--\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Pray you, ask me,--\n",
      "\n",
      "enatory brother, to the\n",
      "dead, or instruction, who shall you have\n",
      "theyer enough: please your queen and then mine\n",
      "fellows will conceit them, and I thank you'll an oath\n",
      "time unpile more intelligence will as loud to do it.\n",
      "\n",
      "Provost:\n",
      "How now, sir? advise your worship. This man may sends\n",
      "have me anon, thinking upon't.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "It is fond royal matter.\n",
      "\n",
      "Provost:\n",
      "Away, be gone, and let me know the office; here\n",
      "they, and go bawdge and bring the\n",
      "boy; and I shall not come to cross, thou hast\n",
      "up of a good estate two. Pray you, he's\n",
      "will you see it.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "An away.\n",
      "\n",
      "Provost:\n",
      "I know mine own good word, and you'll bear the swift of it\n",
      "peakure.\n",
      "\n",
      "Provost:\n",
      "O mock me to the business. How do I do I know this,\n",
      "and will be many many moody to endure your\n",
      "ofter the father's, but I cannot in the\n",
      "sharacter of the sea or no;\n",
      "and though not I give, but I warrant it, but\n",
      "him.\n",
      "\n",
      "LUCIO:\n",
      "What, fare you well.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Who has there?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "He had rather say you.\n",
      "\n",
      "Provost:\n",
      "I'll know advoid you, my sister. Antigonus, boy company.\n",
      "\n",
      "ESCALUS:\n",
      "If it be true, let us go: to myself, and leave it.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "He's some sad joy: if all so fetten of the\n",
      "air of love, to desire of Provost, Elbow; go, beholemen;\n",
      "and therefore, as you may bring you use him utter.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "My majesty is a foolish executed to his habits: made\n",
      "merry; this is the trust: and it\n",
      "more read him, I warrant you, slipping you shall not Justice.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "You shall hind examp, by him offencemen.\n",
      "\n",
      "LUCIO:\n",
      "I do a brain; and so is come to that either,\n",
      "stander to the business, that you may know owe.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "\n",
      "ANGOLIXENES:\n",
      "But is so with me? you are enough inquired\n",
      "that you mind to that, but have the newsape,\n",
      "When you, I know not how you are dead, I know no.\n",
      "\n",
      "Provost:\n",
      "What, will you do me man? what, good father\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What's yours?\n",
      "\n",
      "Provost:\n",
      "Nay, liberty?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "By nothing done, he we hearray'd, by your great liberty.\n",
      "\n",
      "ESCALUS:\n",
      "What's the matter?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "He hath deserved him, father?\n",
      "\n",
      "Provost:\n",
      "Under him, he hath a like to him. As far way:\n",
      "Asken time is a little joint\n",
      "And that Barnardine. Onardine is a whitness He\n",
      "To bring his son, my brother come to his\n",
      "Provost! Love me to't and my holy sake\n",
      "a command a thing: come again. Then.\n",
      "Hostess sir! I pray you, defy us thanks,\n",
      "after, paplease you, go me some office duke:\n",
      "And tell you cannot tell what you justify\n",
      "As well our capture.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Bensovent?\n",
      "\n",
      "Provost:\n",
      "No; it is not constantly.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Not well, then.\n",
      "What will you regiment for this?\n",
      "\n",
      "Provost:\n",
      "Ange with that way and speed--\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "We are manner? And among when you beholding as\n",
      "advironed stabing: you have report a bark\n",
      "handen limited.\n",
      "\n",
      "ESCALUS:\n",
      "I shall bear her fair mad.\n",
      "\n",
      "POur speedy was journey to conceive\n",
      "Thanicer impures: these place is spent to death.\n",
      "\n",
      "ELBOW:\n",
      "Gentle my way; now here, sir, good father: let life out my\n",
      "change, the king parts for it out of my mistressaline.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "So will you do, sir?\n",
      "\n",
      "Provost:\n",
      "No, sir; and well methoughts, use it not that dread and find\n",
      "to some the gentleman of his brother sobed head.\n",
      "\n",
      "Provost:\n",
      "Harryour blunts?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Provost:\n",
      "O, sir, letters, love.\n",
      "\n",
      "Provost:\n",
      "It is a gentleman and of my tongue.\n",
      "\n",
      "ESCALUS:\n",
      "Has a supherdures, and venture in the\n",
      "best beholding that has, may be so booted, she\n",
      "you take up him. Look on my\n",
      "advour with your head: I hope, let us we'll sack\n",
      "him.\n",
      "\n",
      "Provost:\n",
      "Madam, letters in; I will go you jest.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "See.\n",
      "Well be much no more.\n",
      "\n",
      "LUCIO:\n",
      "I yonder at the office. Pray you to the carry?\n",
      "\n",
      "Justice:\n",
      "Signior Heaven me, and I pray you, hear;\n",
      "One price!\n",
      "\n",
      "ESCALUS:\n",
      "I prithee, speak.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "As cannot say you, sir?\n",
      "\n",
      "FRANCA:\n",
      "And woo my matter.\n",
      "\n",
      "POMPEY:\n",
      "What's the matter?\n",
      "\n",
      "Provost:\n",
      "He cannot tell us by this, sir. My abow.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Nay, fool; and have you not put the\n",
      "dex of heaven, which is, sir, I know my leave.\n",
      "\n",
      "ISABELLA:\n",
      "O, how should he so? says this?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Sir, but he that even such a justous part abitentence that\n",
      "would call you for your brother?\n",
      "\n",
      "ESCALUS:\n",
      "Dostessing that, honesty to 't.\n",
      "\n",
      "FRES\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(idx = torch.zeros((1, 1), \n",
    "                          dtype=torch.long,\n",
    "                          device=device),  ### ðŸ—½ Transfer to device ðŸ—½ ###\n",
    "        max_new_tokens=2048)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0675e-215b-42ad-a5d4-2b3c353ab025",
   "metadata": {},
   "source": [
    "# Looks great! Isn't it?\n",
    "Let'a take a pause, and meet in the next adventure! ðŸ¥¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e1e9b-9618-4a2f-b1a9-99231b85a734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
