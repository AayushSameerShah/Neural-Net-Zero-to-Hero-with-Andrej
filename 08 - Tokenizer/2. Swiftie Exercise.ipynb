{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d894ae46-0038-4372-bba9-4c26dd836e1a",
   "metadata": {},
   "source": [
    "# So! Let's train a tokenizer on the Taylor Swift WikiPage \n",
    "> *To be honest, currently in my mind [Fortnight - from TTPD](https://www.youtube.com/watch?v=q3zqJs7JUCQ) is playing ‚ù§Ô∏è‚Äçüî•*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea66559-a8c1-4655-9026-c36368941b28",
   "metadata": {},
   "source": [
    "üëâüèª Please reach out to this page: [page](https://github.com/karpathy/minbpe/blob/master/exercise.md) to get the instructions that I have followed in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee0d3f-039a-428c-855e-d1c6a0dc06ee",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Write the `BasicTokenizer` class, with the following three core functions:\n",
    "\n",
    "- `def train(self, text, vocab_size, verbose=False)`\n",
    "- `def encode(self, text)`\n",
    "- `def decode(self, ids)`\n",
    "\n",
    "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file tests/taylorswift.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c14596f-c3dd-4c53-b002-9ecb03edc314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "with open(\"./data/ts.txt\", \"r\") as file:\n",
    "    file = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589dfb9c-ff9a-4175-851c-4947b753b1a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üí™üèª Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d8d242-3b35-4b88-b8f4-87a9004c4c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74a33d9f-05bc-4cd4-9fa1-461e2be52bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        # initialize the defaut vocab\n",
    "        self.vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "        self.trained=False\n",
    "        \n",
    "    \n",
    "    def find_most_repeated_pair(self, tokens) -> Tuple[Tuple, int, Dict]:\n",
    "        counter = defaultdict(int)\n",
    "        for pair in zip(tokens, tokens[1:]):\n",
    "            counter[pair] += 1\n",
    "\n",
    "        max_pair = max(counter, key=counter.get)\n",
    "        max_count = counter[max_pair]\n",
    "        return max_pair, max_count, counter\n",
    "\n",
    "    def replace_pair_with_new_token(self, tokens, pair, new_idx) -> List:\n",
    "        new_tokens = [] # this will hold the copy for the new tokens\n",
    "        idx = 0\n",
    "        while idx < len(tokens):\n",
    "            if idx < len(tokens) - 1 and (tokens[idx] == pair[0]) and (tokens[idx + 1] == pair[1]): # this is a match!\n",
    "                new_tokens.append(new_idx)\n",
    "                idx += 2\n",
    "            else: # this is not a match\n",
    "                new_tokens.append(tokens[idx])\n",
    "                idx += 1\n",
    "        return new_tokens\n",
    "        \n",
    "    def train(self, blob, vocab_size=None) -> None:\n",
    "        '''\n",
    "        This function will train the tokenizer based on the \n",
    "        training data given as text.\n",
    "        \n",
    "        1. blob: The data in text format that will be used as training\n",
    "            of the tokenizer.\n",
    "        \n",
    "        2. vocab_size: This is \"how many new tokens you want to generate\"\n",
    "            - `None` means indefinite; generate all combinations.\n",
    "            - `int` means the number of merges.\n",
    "        '''\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokens = list(map(int, blob.encode(\"utf-8\")))\n",
    "        \n",
    "        new_idx = 255\n",
    "        merges = {}\n",
    "        for i in range(vocab_size):\n",
    "            pair, count, stats = self.find_most_repeated_pair(self.tokens)\n",
    "            if count > 1:\n",
    "                new_idx += 1\n",
    "                self.tokens = self.replace_pair_with_new_token(self.tokens, pair, new_idx)\n",
    "                merges[pair] = new_idx\n",
    "            else: # every pair is occuring for once only\n",
    "                break\n",
    "        self.total_merges = i+1\n",
    "        \n",
    "        ## The training is done now merge the stuff\n",
    "        for pair, idx in merges.items():\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "        self.merges = merges   \n",
    "        self.trained = True\n",
    "        \n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        The goal of this function is to encode the given text into the \n",
    "        tokens that are acceptable by our `vocab`.\n",
    "        \n",
    "        So, we will need to keep encoding the tokens form the start (top)\n",
    "        to the bottom.\n",
    "        \n",
    "        The `order` of the vocab **is not guerenteed** in the older versions\n",
    "        of python, so we wil need to rely on the `idx`. The lower the idx\n",
    "        is, the older that token is!\n",
    "        '''\n",
    "        \n",
    "        if not self.trained:\n",
    "            raise NotImplementedError(\"Please first train the tokenizer!\")\n",
    "        \n",
    "        tokens = text.encode(\"utf-8\")    \n",
    "        while len(tokens) >= 2:\n",
    "            _, _, stats = self.find_most_repeated_pair(tokens)\n",
    "            # now the goal is to get all pairs of the new tokens\n",
    "            # we are not interested in the count, just the pairs\n",
    "            # then check for each pair, if \n",
    "            pair_replace = min(self.merges, key=lambda x: stats.get(x, float(\"inf\")))\n",
    "            if pair_replace in stats:\n",
    "                tokens = self.replace_pair_with_new_token(tokens, \n",
    "                                                 pair_replace,\n",
    "                                                 self.merges[pair_replace])\n",
    "            else:\n",
    "                break\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        decoded_stream = [self.vocab[idx] for idx in tokens]\n",
    "        text = b\"\".join(decoded_stream)\n",
    "        return text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0788c812-260a-4f3e-9d43-159940d67f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8def1ccf-b79e-458e-aa8b-bce53d1e2369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.train(\"This is the sentence that should test the current tokenizer.\", vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "50e88228-1346-41c8-b57d-ff0b6f4e319b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is me now'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"This is me now\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "25ed99e1-4591-41a9-bc38-a1980119ffd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.total_merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97043d3-e8a3-4214-8f4a-d82b4f12c074",
   "metadata": {},
   "source": [
    "> The **base** version of the tokenizer is ready! üéâüéâ *(Trust me, it was so easy that I didn't have to look at the actual code once!)*.\n",
    "\n",
    "Let's now train that on the TS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f5596e49-97da-41d4-bcf5-c688207939e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(file, vocab_size=50_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f5ba4-6e7b-4da6-9048-66acc3dd833e",
   "metadata": {},
   "source": [
    "üëÜüèª The above training took me around `2` minutes `30` seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae29c1-0165-4828-8ced-ff07b455e72a",
   "metadata": {},
   "source": [
    "> Note: The `50,000` is the max number of merges, there can be less than that. And the total will be stored in `total_merges` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dd5e8ba9-1aa5-4ffa-a55d-58ad9a91204b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8157"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.total_merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eec0e5-b264-4fda-99cf-852d49922dbc",
   "metadata": {},
   "source": [
    "Total `8K` merges! Cool! üî•\n",
    "\n",
    "### Wait, not cool üòµ\n",
    "We have given it total `50,000` merges to do, and it could do `8K`. That means **we have merged many trash tokens** and that also means, we have long tokens in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b64b514f-2fc3-43a9-bf61-3890b16a64fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Wikimedia Foundation'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab[255 + 8156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e710ca3d-bb85-433e-84d2-57cdcb8d9499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Wikimedia '"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab[255 + 8155]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40385094-497a-40e2-89be-d6507a9e9833",
   "metadata": {},
   "source": [
    "I mean WTF!? It has to happen, right!? Now, let's make it better. Just say, 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "02a0540b-29a6-486a-88de-49bbe69e6ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(file, vocab_size=512)\n",
    "tokenizer.total_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4f0a1504-7e93-4019-88ec-252873784aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'tim'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab[255 + 511]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "deb44423-3b09-472e-9028-e6a2fe8db643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'ay '"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab[255 + 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce70d2-6978-4138-abef-3644e5b3641f",
   "metadata": {},
   "source": [
    "Now, better üëçüèª "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b5c1e3b7-55bb-46e2-92e4-4d250da8c584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'e '\n",
      "b', '\n",
      "b'd '\n",
      "b'. '\n",
      "b'r '\n",
      "b'20'\n",
      "b's '\n",
      "b'in'\n",
      "b'on'\n",
      "b'ri'\n",
      "b't '\n",
      "b'th'\n",
      "b'ed '\n",
      "b', 20'\n",
      "b'an'\n",
      "b'ar'\n",
      "b'er '\n",
      "b'y '\n",
      "b'al'\n",
      "b'the '\n",
      "b'ved '\n",
      "b'wi'\n",
      "b'er'\n",
      "b'on '\n",
      "b'wif'\n",
      "b'Re'\n",
      "b'Swif'\n",
      "b'or '\n",
      "b'ch'\n",
      "b', 201'\n",
      "b'om'\n",
      "b'ber '\n",
      "b' the '\n",
      "b'ay'\n",
      "b'en'\n",
      "b'or'\n",
      "b'al '\n",
      "b'em'\n",
      "b'.\\n'\n",
      "b'rie'\n",
      "b'ing'\n",
      "b', 202'\n",
      "b'ti'\n",
      "b'ayl'\n",
      "b'\". '\n",
      "b'll'\n",
      "b'Tayl'\n",
      "b'trie'\n",
      "b'.\\n '\n",
      "b'to'\n"
     ]
    }
   ],
   "source": [
    "# let's visualize the merges:\n",
    "for pair in list(tokenizer.merges.keys())[:50]:\n",
    "    print(tokenizer.vocab[tokenizer.merges[pair]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe4746-3512-4feb-8996-63f4ba93bdc1",
   "metadata": {},
   "source": [
    "> We can see, how the *model* has started to detect \"Taylor\" a lot as a single token! **And also note** that there are the tokens with *numbers, punctuations, new lines* and so on.\n",
    "\n",
    "So, next up we will solve that by training the `RegexTokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74854121-509d-421f-a040-fcdc8f2767fd",
   "metadata": {},
   "source": [
    "<img src=\"./images/regex-flow.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff45d89b-e00b-455e-b142-31a2e93e9d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9b92ddca-51f3-477f-ab62-d90fe749885d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegexTokenizer:\n",
    "    '''\n",
    "    This is supposed to get a little crazy.\n",
    "    \n",
    "    Step 1: Split the text based on the regex pattern.\n",
    "    Step 2: Now, we have the cleaned words.\n",
    "    Step 3: Get their raw tokens individually.\n",
    "    Step 4: Don't merge them yet, because it will nullify the step 1-3. \n",
    "    Step 4: Find pairs (stats) for each of the words - while keeping \"common\" stats across each.\n",
    "    Step 5: Find the max repetative pair.\n",
    "    Step 6: Replace that pair in each token group.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # initialize the defaut vocab\n",
    "        self.vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "        self.trained=False\n",
    "        self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        self.GPT4_PATTERN_COMPILED = re.compile(self.GPT4_SPLIT_PATTERN)\n",
    "    \n",
    "    def find_most_repeated_pair(self, tokens, counter=None) -> Tuple[Tuple, int, Dict]:\n",
    "        '''\n",
    "        Now, this function is changed slightly as we will calculcate the \n",
    "        max when needed after this function call.\n",
    "        \n",
    "        Also, the `counter` can be passed and updated, and returned.\n",
    "        Doing this will ensure, the global counter.\n",
    "        '''\n",
    "        counter = counter if counter is not None else defaultdict(int)\n",
    "        for pair in zip(tokens, tokens[1:]):\n",
    "            counter[pair] += 1\n",
    "        return counter # will be useful when the counter=None passed.\n",
    "\n",
    "    def replace_pair_with_new_token(self, tokens, pair, new_idx) -> List:\n",
    "        new_tokens = [] # this will hold the copy for the new tokens\n",
    "        idx = 0\n",
    "        while idx < len(tokens):\n",
    "            if idx < len(tokens) - 1 and (tokens[idx] == pair[0]) and (tokens[idx + 1] == pair[1]): # this is a match!\n",
    "                new_tokens.append(new_idx)\n",
    "                idx += 2\n",
    "            else: # this is not a match\n",
    "                new_tokens.append(tokens[idx])\n",
    "                idx += 1\n",
    "        return new_tokens\n",
    "        \n",
    "    def train(self, blob, vocab_size=None) -> None:\n",
    "        '''\n",
    "        This function will train the tokenizer based on the \n",
    "        training data given as text.\n",
    "        \n",
    "        1. blob: The data in text format that will be used as training\n",
    "            of the tokenizer.\n",
    "        \n",
    "        2. vocab_size: This is \"how many new tokens you want to generate\"\n",
    "            - `None` means indefinite; generate all combinations.\n",
    "            - `int` means the number of merges.\n",
    "        '''\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # First split\n",
    "        cleaned_text = self.GPT4_PATTERN_COMPILED.findall(blob)\n",
    "        # Then create the tokens\n",
    "        self.tokens = [list(map(int, word.encode(\"utf-8\"))) for word in cleaned_text]\n",
    "        \n",
    "        \n",
    "        new_idx = 255\n",
    "        merges = {}\n",
    "        for i in range(vocab_size):\n",
    "            stats = defaultdict(int)\n",
    "            for token_group in self.tokens:\n",
    "                # pass the stats, which will be updated in place\n",
    "                self.find_most_repeated_pair(token_group, stats)\n",
    "            \n",
    "            max_pair = max(stats, key=stats.get)\n",
    "            max_count = stats[max_pair]\n",
    "            \n",
    "            if max_count > 1:\n",
    "                new_idx += 1\n",
    "                self.tokens = [self.replace_pair_with_new_token(token_group, max_pair, new_idx) for token_group in self.tokens]\n",
    "                merges[max_pair] = new_idx\n",
    "            else: # every pair is occuring for once only\n",
    "                break\n",
    "        self.total_merges = i+1\n",
    "        \n",
    "        ## The training is done now merge the stuff\n",
    "        for pair, idx in merges.items():\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "        self.merges = merges   \n",
    "        self.trained = True\n",
    "        \n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        The goal of this function is to encode the given text into the \n",
    "        tokens that are acceptable by our `vocab`.\n",
    "        \n",
    "        So, we will need to keep encoding the tokens form the start (top)\n",
    "        to the bottom.\n",
    "        \n",
    "        The `order` of the vocab **is not guerenteed** in the older versions\n",
    "        of python, so we wil need to rely on the `idx`. The lower the idx\n",
    "        is, the older that token is!\n",
    "        '''\n",
    "        \n",
    "        if not self.trained:\n",
    "            raise NotImplementedError(\"Please first train the tokenizer!\")\n",
    "        \n",
    "        # tokens = text.encode(\"utf-8\")\n",
    "        split_words = self.GPT4_PATTERN_COMPILED.findall(text)\n",
    "        split_tokens = [list(word.encode(\"utf-8\")) for word in split_words]\n",
    "        \n",
    "        final_tokens = []\n",
    "        for chunk in split_tokens:\n",
    "            while len(chunk) >= 2:\n",
    "                stats = self.find_most_repeated_pair(chunk)\n",
    "                # now the goal is to get all pairs of the new tokens\n",
    "                # we are not interested in the count, just the pairs\n",
    "                # then check for each pair, if \n",
    "                pair_replace = min(self.merges, key=lambda x: stats.get(x, float(\"inf\")))\n",
    "                if pair_replace in stats:\n",
    "                    chunk = self.replace_pair_with_new_token(chunk, \n",
    "                                                     pair_replace,\n",
    "                                                     self.merges[pair_replace])\n",
    "                else:\n",
    "                    break\n",
    "            final_tokens.extend(chunk)\n",
    "        return final_tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        decoded_stream = [self.vocab[idx] for idx in tokens]\n",
    "        text = b\"\".join(decoded_stream)\n",
    "        return text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5013fab-9768-4ba7-9b17-debdf229f805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"This is the name of aayush shah 123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "751c102b-1029-424b-8f16-1fcad5443777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.train(file, vocab_size=512)\n",
    "tokenizer.total_merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ddaf2-8dd5-489e-8b1f-cf1e9786934c",
   "metadata": {},
   "source": [
    "ü´±üèª‚Äçü´≤üèª Let's now see the quality of merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99f2a955-facc-462b-8fc0-9d62f7c00cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'er'\n",
      "b'20'\n",
      "b'or'\n",
      "b'in'\n",
      "b'ed'\n",
      "b' t'\n",
      "b'on'\n",
      "b'he'\n",
      "b' S'\n",
      "b'ar'\n",
      "b'an'\n",
      "b' A'\n",
      "b' the'\n",
      "b'al'\n",
      "b'ri'\n",
      "b'ved'\n",
      "b'st'\n",
      "b'wi'\n",
      "b' R'\n",
      "b'201'\n",
      "b' f'\n",
      "b'202'\n",
      "b' T'\n",
      "b'ft'\n",
      "b'ay'\n",
      "b' \"'\n",
      "b'wift'\n",
      "b'et'\n",
      "b' Swift'\n",
      "b'ch'\n",
      "b'ber'\n",
      "b'at'\n",
      "b'om'\n",
      "b'es'\n",
      "b'en'\n",
      "b'em'\n",
      "b'\".'\n",
      "b' ('\n",
      "b'.\\n'\n",
      "b'ing'\n",
      "b'lor'\n",
      "b' M'\n",
      "b'ig'\n",
      "b' on'\n",
      "b'aylor'\n",
      "b'll'\n",
      "b'rie'\n",
      "b' Ret'\n",
      "b' Retrie'\n",
      "b' Retrieved'\n"
     ]
    }
   ],
   "source": [
    "# let's visualize the merges:\n",
    "for pair in list(tokenizer.merges.keys())[:50]:\n",
    "    print(tokenizer.vocab[tokenizer.merges[pair]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd13087-4716-4480-a487-a5d930a9859b",
   "metadata": {},
   "source": [
    "üî• I think, it works great!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3c77d078-0fe0-45f8-9383-a874f1bd2902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 104, 105, 115, 32, 105, 115, 32, 109, 101, 32, 110, 111, 119, 49, 50, 51]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"This is me now123\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "da120c26-e18b-4d93-93d2-e241e50a5fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 104, 355, 549, 344, 101, 435, 397, 49, 486]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is me now123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8781c-c446-4a46-a2f0-8b979bf6f715",
   "metadata": {},
   "source": [
    "üî•üî• Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd562e-b6c8-4972-883f-1d1184f08cbf",
   "metadata": {},
   "source": [
    "# Aaai, think we should be done here...\n",
    "The next steps are regarding comparing the GPT-4 tokenizer and so on. But we can skip that since we have acomplished much of the stuff here.\n",
    "\n",
    "Let's train GPT from scratch with this new tokenizer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
